{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdfa08d-aae0-44af-9723-18f9e0cab026",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "311c2dec-2fe8-4010-bf2f-52e185b18676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: prophet in c:\\users\\ravik\\appdata\\roaming\\python\\python312\\site-packages (1.1.7)\n",
      "Requirement already satisfied: cmdstanpy>=1.0.4 in c:\\users\\ravik\\appdata\\roaming\\python\\python312\\site-packages (from prophet) (1.2.5)\n",
      "Requirement already satisfied: numpy>=1.15.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from prophet) (1.26.4)\n",
      "Requirement already satisfied: matplotlib>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from prophet) (3.9.2)\n",
      "Requirement already satisfied: pandas>=1.0.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from prophet) (2.2.2)\n",
      "Requirement already satisfied: holidays<1,>=0.25 in c:\\users\\ravik\\appdata\\roaming\\python\\python312\\site-packages (from prophet) (0.81)\n",
      "Requirement already satisfied: tqdm>=4.36.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from prophet) (4.66.5)\n",
      "Requirement already satisfied: importlib_resources in c:\\users\\ravik\\appdata\\roaming\\python\\python312\\site-packages (from prophet) (6.5.2)\n",
      "Requirement already satisfied: stanio<2.0.0,>=0.4.0 in c:\\users\\ravik\\appdata\\roaming\\python\\python312\\site-packages (from cmdstanpy>=1.0.4->prophet) (0.5.1)\n",
      "Requirement already satisfied: python-dateutil in c:\\programdata\\anaconda3\\lib\\site-packages (from holidays<1,>=0.25->prophet) (2.9.0.post0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=2.0.0->prophet) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=2.0.0->prophet) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=2.0.0->prophet) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=2.0.0->prophet) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=2.0.0->prophet) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=2.0.0->prophet) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=2.0.0->prophet) (3.1.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=1.0.4->prophet) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=1.0.4->prophet) (2023.3)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.36.1->prophet) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil->holidays<1,>=0.25->prophet) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "decb0eca-57bd-4a85-ade7-8ae801eec6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Processing: capsicum_Bangalore_daily.csv ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:38:31 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:38:40 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for capsicum_Bangalore_daily.csv:\n",
      "  MAE        : 688.81\n",
      "  RMSE       : 974.68\n",
      "  RÂ²         : 0.4256\n",
      "  MAPE(%)    : 26.45\n",
      "  Accuracy(%) : 73.55\n",
      "\n",
      "Saved updated CSV with Date, Actual & Predicted: prophet_output_csv\\capsicum_Bangalore_daily_prophet_updated.csv\n",
      "========== Processing: capsicum_Belgaum_daily.csv ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:38:44 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:38:44 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for capsicum_Belgaum_daily.csv:\n",
      "  MAE        : 442.34\n",
      "  RMSE       : 604.33\n",
      "  RÂ²         : 0.4192\n",
      "  MAPE(%)    : 15.4\n",
      "  Accuracy(%) : 84.6\n",
      "\n",
      "Saved updated CSV with Date, Actual & Predicted: prophet_output_csv\\capsicum_Belgaum_daily_prophet_updated.csv\n",
      "========== Processing: capsicum_Bellary_daily.csv ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:38:46 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:38:46 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for capsicum_Bellary_daily.csv:\n",
      "  MAE        : 248.24\n",
      "  RMSE       : 344.61\n",
      "  RÂ²         : 0.5025\n",
      "  MAPE(%)    : 10.55\n",
      "  Accuracy(%) : 89.45\n",
      "\n",
      "Saved updated CSV with Date, Actual & Predicted: prophet_output_csv\\capsicum_Bellary_daily_prophet_updated.csv\n",
      "========== Processing: capsicum_Chikmagalur_daily.csv ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:38:48 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:38:51 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for capsicum_Chikmagalur_daily.csv:\n",
      "  MAE        : 398.3\n",
      "  RMSE       : 578.86\n",
      "  RÂ²         : -0.2084\n",
      "  MAPE(%)    : 25.61\n",
      "  Accuracy(%) : 74.39\n",
      "\n",
      "Saved updated CSV with Date, Actual & Predicted: prophet_output_csv\\capsicum_Chikmagalur_daily_prophet_updated.csv\n",
      "========== Processing: capsicum_Davangere_daily.csv ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:38:53 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:38:58 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for capsicum_Davangere_daily.csv:\n",
      "  MAE        : 797.45\n",
      "  RMSE       : 1130.93\n",
      "  RÂ²         : 0.5772\n",
      "  MAPE(%)    : 26.2\n",
      "  Accuracy(%) : 73.8\n",
      "\n",
      "Saved updated CSV with Date, Actual & Predicted: prophet_output_csv\\capsicum_Davangere_daily_prophet_updated.csv\n",
      "========== Processing: capsicum_Dharwad_daily.csv ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:39:00 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:39:00 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for capsicum_Dharwad_daily.csv:\n",
      "  MAE        : 557.13\n",
      "  RMSE       : 711.96\n",
      "  RÂ²         : 0.5025\n",
      "  MAPE(%)    : 18.12\n",
      "  Accuracy(%) : 81.88\n",
      "\n",
      "Saved updated CSV with Date, Actual & Predicted: prophet_output_csv\\capsicum_Dharwad_daily_prophet_updated.csv\n",
      "========== Processing: capsicum_Hassan_daily.csv ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:39:02 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:39:07 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for capsicum_Hassan_daily.csv:\n",
      "  MAE        : 382.25\n",
      "  RMSE       : 541.1\n",
      "  RÂ²         : 0.7049\n",
      "  MAPE(%)    : 21.88\n",
      "  Accuracy(%) : 78.12\n",
      "\n",
      "Saved updated CSV with Date, Actual & Predicted: prophet_output_csv\\capsicum_Hassan_daily_prophet_updated.csv\n",
      "========== Processing: capsicum_Haveri_daily.csv ==========\n",
      "âŒ Skipping capsicum_Haveri_daily.csv â€” not enough valid rows for Prophet model.\n",
      "\n",
      "========== Processing: capsicum_Kalburgi_daily.csv ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:39:10 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:39:11 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for capsicum_Kalburgi_daily.csv:\n",
      "  MAE        : 429.57\n",
      "  RMSE       : 610.44\n",
      "  RÂ²         : 0.7662\n",
      "  MAPE(%)    : 27.3\n",
      "  Accuracy(%) : 72.7\n",
      "\n",
      "Saved updated CSV with Date, Actual & Predicted: prophet_output_csv\\capsicum_Kalburgi_daily_prophet_updated.csv\n",
      "========== Processing: capsicum_Kolar_daily.csv ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:39:15 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:39:58 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for capsicum_Kolar_daily.csv:\n",
      "  MAE        : 955.7\n",
      "  RMSE       : 1239.64\n",
      "  RÂ²         : -0.5575\n",
      "  MAPE(%)    : 74.89\n",
      "  Accuracy(%) : 25.11\n",
      "\n",
      "Saved updated CSV with Date, Actual & Predicted: prophet_output_csv\\capsicum_Kolar_daily_prophet_updated.csv\n",
      "========== Processing: capsicum_Mandya_daily.csv ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:40:02 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:40:09 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for capsicum_Mandya_daily.csv:\n",
      "  MAE        : 255.67\n",
      "  RMSE       : 352.71\n",
      "  RÂ²         : 0.8716\n",
      "  MAPE(%)    : 18.63\n",
      "  Accuracy(%) : 81.37\n",
      "\n",
      "Saved updated CSV with Date, Actual & Predicted: prophet_output_csv\\capsicum_Mandya_daily_prophet_updated.csv\n",
      "========== Processing: capsicum_Mysore_daily.csv ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:40:12 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:40:28 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for capsicum_Mysore_daily.csv:\n",
      "  MAE        : 1097.13\n",
      "  RMSE       : 1311.69\n",
      "  RÂ²         : -0.9465\n",
      "  MAPE(%)    : 54.53\n",
      "  Accuracy(%) : 45.47\n",
      "\n",
      "Saved updated CSV with Date, Actual & Predicted: prophet_output_csv\\capsicum_Mysore_daily_prophet_updated.csv\n",
      "========== Processing: capsicum_Shimoga_daily.csv ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:40:32 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:40:32 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for capsicum_Shimoga_daily.csv:\n",
      "  MAE        : 0.01\n",
      "  RMSE       : 0.02\n",
      "  RÂ²         : 1.0\n",
      "  MAPE(%)    : 0.0\n",
      "  Accuracy(%) : 100.0\n",
      "\n",
      "Saved updated CSV with Date, Actual & Predicted: prophet_output_csv\\capsicum_Shimoga_daily_prophet_updated.csv\n",
      "========== Processing: capsicum_Udupi_daily.csv ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:40:34 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:40:41 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for capsicum_Udupi_daily.csv:\n",
      "  MAE        : 496.79\n",
      "  RMSE       : 736.9\n",
      "  RÂ²         : 0.757\n",
      "  MAPE(%)    : 14.62\n",
      "  Accuracy(%) : 85.38\n",
      "\n",
      "Saved updated CSV with Date, Actual & Predicted: prophet_output_csv\\capsicum_Udupi_daily_prophet_updated.csv\n",
      "âœ… Multiple CSVs processed! Metrics saved to prophet_metrics.csv. Models, updated CSVs, and graphs saved in respective folders.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from prophet import Prophet\n",
    "import joblib\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -----------------------------\n",
    "# Paths\n",
    "# -----------------------------\n",
    "input_folder = \"dataset\"\n",
    "output_models = \"prophet_output_models\"\n",
    "output_csv = \"prophet_output_csv\"\n",
    "output_metrics_csv = \"prophet_metrics.csv\"\n",
    "output_graphs = \"prophet_output_graphs\"\n",
    "\n",
    "os.makedirs(output_models, exist_ok=True)\n",
    "os.makedirs(output_csv, exist_ok=True)\n",
    "os.makedirs(output_graphs, exist_ok=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Prepare metrics storage\n",
    "# -----------------------------\n",
    "metrics_list = []\n",
    "\n",
    "# -----------------------------\n",
    "# Function to robustly parse dates\n",
    "# -----------------------------\n",
    "def parse_dates_safe(date_series):\n",
    "    try:\n",
    "        return pd.to_datetime(date_series, dayfirst=True)\n",
    "    except:\n",
    "        return pd.to_datetime(date_series, dayfirst=False)\n",
    "\n",
    "# -----------------------------\n",
    "# Function to calculate MAPE\n",
    "# -----------------------------\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "# -----------------------------\n",
    "# Process each CSV\n",
    "# -----------------------------\n",
    "for file in os.listdir(input_folder):\n",
    "    if file.endswith(\".csv\"):\n",
    "        file_path = os.path.join(input_folder, file)\n",
    "        print(f\"========== Processing: {file} ==========\")\n",
    "\n",
    "        # Load CSV\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Convert date column safely\n",
    "        df['Date'] = parse_dates_safe(df['Date'])\n",
    "        df = df.sort_values('Date')\n",
    "\n",
    "        # Handle missing values\n",
    "        if df['Average Price'].isna().sum() > 0:\n",
    "            mean_value = df['Average Price'].mean()\n",
    "            df['Average Price'].fillna(mean_value, inplace=True)\n",
    "            print(f\"Filled {df['Average Price'].isna().sum()} missing values with mean: {mean_value:.2f}\")\n",
    "\n",
    "        df['Average Price'] = df['Average Price'].round(2)\n",
    "\n",
    "        # Prepare Prophet dataframe\n",
    "        prophet_df = df[['Date', 'Average Price']].rename(columns={'Date': 'ds', 'Average Price': 'y'})\n",
    "\n",
    "        # -----------------------------\n",
    "        # SKIP FILES WITH < 2 VALID ROWS\n",
    "        # -----------------------------\n",
    "        valid_rows = prophet_df[prophet_df['y'].notna()]\n",
    "        if len(valid_rows) < 2:\n",
    "            print(f\"âŒ Skipping {file} â€” not enough valid rows for Prophet model.\\n\")\n",
    "            continue\n",
    "\n",
    "        # Build and fit Prophet model\n",
    "        model = Prophet(daily_seasonality=True)\n",
    "        model.fit(prophet_df)\n",
    "\n",
    "        # Predict\n",
    "        future = model.make_future_dataframe(periods=0)\n",
    "        forecast = model.predict(future)\n",
    "\n",
    "        df['Predicted'] = forecast['yhat']\n",
    "\n",
    "        # Metrics handling\n",
    "        metrics_df = df.dropna(subset=['Average Price', 'Predicted'])\n",
    "        y_true = metrics_df['Average Price'].values\n",
    "        y_pred = metrics_df['Predicted'].values\n",
    "\n",
    "        df['Predicted'] = df['Predicted'].round(2)\n",
    "        df.rename(columns={'Average Price': 'Actual'}, inplace=True)\n",
    "\n",
    "        # -----------------------------\n",
    "        # Calculate metrics\n",
    "        # -----------------------------\n",
    "        mae = round(mean_absolute_error(y_true, y_pred), 2)\n",
    "        rmse = round(np.sqrt(mean_squared_error(y_true, y_pred)), 2)\n",
    "        r2 = round(r2_score(y_true, y_pred), 4)\n",
    "        mape = round(mean_absolute_percentage_error(y_true, y_pred), 2)\n",
    "        accuracy = round(100 - mape, 2)\n",
    "\n",
    "        print(f\"Metrics for {file}:\")\n",
    "        print(f\"  MAE        : {mae}\")\n",
    "        print(f\"  RMSE       : {rmse}\")\n",
    "        print(f\"  RÂ²         : {r2}\")\n",
    "        print(f\"  MAPE(%)    : {mape}\")\n",
    "        print(f\"  Accuracy(%) : {accuracy}\\n\")\n",
    "\n",
    "        metrics_list.append({\n",
    "            'File': file,\n",
    "            'MAE': mae,\n",
    "            'RMSE': rmse,\n",
    "            'R2': r2,\n",
    "            'MAPE(%)': mape,\n",
    "            'Accuracy(%)': accuracy\n",
    "        })\n",
    "\n",
    "        # Save model\n",
    "        model_file = os.path.join(output_models, file.replace(\".csv\", \"_prophet_model.pkl\"))\n",
    "        joblib.dump(model, model_file)\n",
    "\n",
    "        # Save updated CSV\n",
    "        save_df = df[['Date', 'Actual', 'Predicted']]\n",
    "        updated_csv_path = os.path.join(output_csv, file.replace(\".csv\", \"_prophet_updated.csv\"))\n",
    "        save_df.to_csv(updated_csv_path, index=False)\n",
    "        print(f\"Saved updated CSV with Date, Actual & Predicted: {updated_csv_path}\")\n",
    "\n",
    "        # Add moving averages\n",
    "        df['MA7'] = df['Actual'].rolling(7).mean()\n",
    "        df['MA30'] = df['Actual'].rolling(30).mean()\n",
    "\n",
    "        # Plot\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(df['Date'], df['Actual'], label='Actual', color='blue')\n",
    "        plt.plot(df['Date'], df['Predicted'], label='Predicted', color='red')\n",
    "        plt.plot(df['Date'], df['MA7'], label='MA7', color='green', linestyle='--')\n",
    "        plt.plot(df['Date'], df['MA30'], label='MA30', color='orange', linestyle='--')\n",
    "        plt.title(f\"{file} - Actual vs Predicted with Moving Averages\")\n",
    "        plt.xlabel(\"Date\")\n",
    "        plt.ylabel(\"Price\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        graph_path = os.path.join(output_graphs, file.replace(\".csv\", \"_prophet_graph.png\"))\n",
    "        plt.savefig(graph_path, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "# -----------------------------\n",
    "# SAVE ALL METRICS\n",
    "# -----------------------------\n",
    "metrics_df = pd.DataFrame(metrics_list)\n",
    "metrics_df.to_csv(output_metrics_csv, index=False)\n",
    "print(f\"âœ… Multiple CSVs processed! Metrics saved to {output_metrics_csv}. Models, updated CSVs, and graphs saved in respective folders.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab05abc-bc5c-42ac-9099-3ff7e5181ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f875441b-5c2b-432a-99a9-89672ce4f05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Processing: capsicum_Bangalore_daily.csv ==========\n",
      "Metrics for capsicum_Bangalore_daily.csv:\n",
      "  MAE        : 474.49\n",
      "  RMSE       : 731.01\n",
      "  RÂ²         : 0.6583\n",
      "  MAPE(%)    : 14.89\n",
      "  Accuracy(%) : 85.11\n",
      "\n",
      "Saved updated CSV: arima_output_csv\\capsicum_Bangalore_daily_arima_updated.csv\n",
      "========== Processing: capsicum_Belgaum_daily.csv ==========\n",
      "Metrics for capsicum_Belgaum_daily.csv:\n",
      "  MAE        : 82.19\n",
      "  RMSE       : 249.99\n",
      "  RÂ²         : 0.9006\n",
      "  MAPE(%)    : 2.87\n",
      "  Accuracy(%) : 97.13\n",
      "\n",
      "Saved updated CSV: arima_output_csv\\capsicum_Belgaum_daily_arima_updated.csv\n",
      "========== Processing: capsicum_Bellary_daily.csv ==========\n",
      "Metrics for capsicum_Bellary_daily.csv:\n",
      "  MAE        : 164.4\n",
      "  RMSE       : 330.77\n",
      "  RÂ²         : 0.5417\n",
      "  MAPE(%)    : 6.55\n",
      "  Accuracy(%) : 93.45\n",
      "\n",
      "Saved updated CSV: arima_output_csv\\capsicum_Bellary_daily_arima_updated.csv\n",
      "========== Processing: capsicum_Chikmagalur_daily.csv ==========\n",
      "Metrics for capsicum_Chikmagalur_daily.csv:\n",
      "  MAE        : 154.74\n",
      "  RMSE       : 253.07\n",
      "  RÂ²         : 0.8163\n",
      "  MAPE(%)    : 9.36\n",
      "  Accuracy(%) : 90.64\n",
      "\n",
      "Saved updated CSV: arima_output_csv\\capsicum_Chikmagalur_daily_arima_updated.csv\n",
      "========== Processing: capsicum_Davangere_daily.csv ==========\n",
      "Metrics for capsicum_Davangere_daily.csv:\n",
      "  MAE        : 101.34\n",
      "  RMSE       : 249.85\n",
      "  RÂ²         : 0.9794\n",
      "  MAPE(%)    : 2.81\n",
      "  Accuracy(%) : 97.19\n",
      "\n",
      "Saved updated CSV: arima_output_csv\\capsicum_Davangere_daily_arima_updated.csv\n",
      "========== Processing: capsicum_Dharwad_daily.csv ==========\n",
      "Metrics for capsicum_Dharwad_daily.csv:\n",
      "  MAE        : 579.21\n",
      "  RMSE       : 806.59\n",
      "  RÂ²         : 0.5621\n",
      "  MAPE(%)    : 35.78\n",
      "  Accuracy(%) : 64.22\n",
      "\n",
      "Saved updated CSV: arima_output_csv\\capsicum_Dharwad_daily_arima_updated.csv\n",
      "========== Processing: capsicum_Hassan_daily.csv ==========\n",
      "Metrics for capsicum_Hassan_daily.csv:\n",
      "  MAE        : 161.66\n",
      "  RMSE       : 372.19\n",
      "  RÂ²         : 0.8637\n",
      "  MAPE(%)    : 6.19\n",
      "  Accuracy(%) : 93.81\n",
      "\n",
      "Saved updated CSV: arima_output_csv\\capsicum_Hassan_daily_arima_updated.csv\n",
      "========== Processing: capsicum_Haveri_daily.csv ==========\n",
      "âŒ Skipping capsicum_Haveri_daily.csv â€” ARIMA failed to fit on this dataset.\n",
      "\n",
      "========== Processing: capsicum_Kalburgi_daily.csv ==========\n",
      "Metrics for capsicum_Kalburgi_daily.csv:\n",
      "  MAE        : 175.86\n",
      "  RMSE       : 336.36\n",
      "  RÂ²         : 0.929\n",
      "  MAPE(%)    : 7.52\n",
      "  Accuracy(%) : 92.48\n",
      "\n",
      "Saved updated CSV: arima_output_csv\\capsicum_Kalburgi_daily_arima_updated.csv\n",
      "========== Processing: capsicum_Kolar_daily.csv ==========\n",
      "Metrics for capsicum_Kolar_daily.csv:\n",
      "  MAE        : 954.11\n",
      "  RMSE       : 1423.3\n",
      "  RÂ²         : 0.2327\n",
      "  MAPE(%)    : 46.03\n",
      "  Accuracy(%) : 53.97\n",
      "\n",
      "Saved updated CSV: arima_output_csv\\capsicum_Kolar_daily_arima_updated.csv\n",
      "========== Processing: capsicum_Mandya_daily.csv ==========\n",
      "Metrics for capsicum_Mandya_daily.csv:\n",
      "  MAE        : 140.15\n",
      "  RMSE       : 316.06\n",
      "  RÂ²         : 0.9244\n",
      "  MAPE(%)    : 5.82\n",
      "  Accuracy(%) : 94.18\n",
      "\n",
      "Saved updated CSV: arima_output_csv\\capsicum_Mandya_daily_arima_updated.csv\n",
      "========== Processing: capsicum_Mysore_daily.csv ==========\n",
      "Metrics for capsicum_Mysore_daily.csv:\n",
      "  MAE        : 500.92\n",
      "  RMSE       : 770.88\n",
      "  RÂ²         : 0.6228\n",
      "  MAPE(%)    : 19.06\n",
      "  Accuracy(%) : 80.94\n",
      "\n",
      "Saved updated CSV: arima_output_csv\\capsicum_Mysore_daily_arima_updated.csv\n",
      "========== Processing: capsicum_Shimoga_daily.csv ==========\n",
      "Metrics for capsicum_Shimoga_daily.csv:\n",
      "  MAE        : 15.63\n",
      "  RMSE       : 138.81\n",
      "  RÂ²         : 0.4702\n",
      "  MAPE(%)    : 1.12\n",
      "  Accuracy(%) : 98.88\n",
      "\n",
      "Saved updated CSV: arima_output_csv\\capsicum_Shimoga_daily_arima_updated.csv\n",
      "========== Processing: capsicum_Udupi_daily.csv ==========\n",
      "Metrics for capsicum_Udupi_daily.csv:\n",
      "  MAE        : 39.7\n",
      "  RMSE       : 175.11\n",
      "  RÂ²         : 0.9863\n",
      "  MAPE(%)    : 0.93\n",
      "  Accuracy(%) : 99.07\n",
      "\n",
      "Saved updated CSV: arima_output_csv\\capsicum_Udupi_daily_arima_updated.csv\n",
      "âœ… All done! Metrics saved to arima_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# -----------------------------\n",
    "# Paths\n",
    "# -----------------------------\n",
    "input_folder = \"dataset\"\n",
    "output_models = \"arima_output_models\"\n",
    "output_csv = \"arima_output_csv\"\n",
    "output_metrics_csv = \"arima_metrics.csv\"\n",
    "output_graphs = \"arima_output_graphs\"\n",
    "\n",
    "os.makedirs(output_models, exist_ok=True)\n",
    "os.makedirs(output_csv, exist_ok=True)\n",
    "os.makedirs(output_graphs, exist_ok=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Metrics storage\n",
    "# -----------------------------\n",
    "metrics_list = []\n",
    "\n",
    "# -----------------------------\n",
    "# Robust date parsing\n",
    "# -----------------------------\n",
    "def parse_dates_safe(date_series):\n",
    "    try:\n",
    "        return pd.to_datetime(date_series, dayfirst=True)\n",
    "    except:\n",
    "        return pd.to_datetime(date_series, dayfirst=False)\n",
    "\n",
    "# -----------------------------\n",
    "# MAPE\n",
    "# -----------------------------\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "# -----------------------------\n",
    "# Process CSVs\n",
    "# -----------------------------\n",
    "for file in os.listdir(input_folder):\n",
    "    if file.endswith(\".csv\"):\n",
    "        file_path = os.path.join(input_folder, file)\n",
    "        print(f\"========== Processing: {file} ==========\")\n",
    "\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        df['Date'] = parse_dates_safe(df['Date'])\n",
    "        df = df.sort_values('Date')\n",
    "\n",
    "        if df['Average Price'].isna().sum() > 0:\n",
    "            mean_value = df['Average Price'].mean()\n",
    "            df['Average Price'].fillna(mean_value, inplace=True)\n",
    "            print(f\"Filled missing values with mean: {mean_value:.2f}\")\n",
    "\n",
    "        df['Average Price'] = df['Average Price'].round(2)\n",
    "\n",
    "        # --------------- ARIMA FIT + FAILURE HANDLING ----------------\n",
    "        order = (5, 1, 0)\n",
    "        model = ARIMA(df['Average Price'], order=order)\n",
    "\n",
    "        try:\n",
    "            model_fit = model.fit()\n",
    "        except:\n",
    "            print(f\"âŒ Skipping {file} â€” ARIMA failed to fit on this dataset.\\n\")\n",
    "            continue\n",
    "        # -------------------------------------------------------------\n",
    "\n",
    "        df['Predicted'] = model_fit.predict(start=0, end=len(df)-1)\n",
    "        df['Predicted'] = df['Predicted'].round(2)\n",
    "\n",
    "        df.rename(columns={'Average Price': 'Actual'}, inplace=True)\n",
    "\n",
    "        y_true = df['Actual'].values\n",
    "        y_pred = df['Predicted'].values\n",
    "\n",
    "        mae = round(mean_absolute_error(y_true, y_pred), 2)\n",
    "        rmse = round(np.sqrt(mean_squared_error(y_true, y_pred)), 2)\n",
    "        r2 = round(r2_score(y_true, y_pred), 4)\n",
    "        mape = round(mean_absolute_percentage_error(y_true, y_pred), 2)\n",
    "        accuracy = round(100 - mape, 2)\n",
    "\n",
    "        print(f\"Metrics for {file}:\")\n",
    "        print(f\"  MAE        : {mae}\")\n",
    "        print(f\"  RMSE       : {rmse}\")\n",
    "        print(f\"  RÂ²         : {r2}\")\n",
    "        print(f\"  MAPE(%)    : {mape}\")\n",
    "        print(f\"  Accuracy(%) : {accuracy}\\n\")\n",
    "\n",
    "        metrics_list.append({\n",
    "            'File': file,\n",
    "            'MAE': mae,\n",
    "            'RMSE': rmse,\n",
    "            'R2': r2,\n",
    "            'MAPE(%)': mape,\n",
    "            'Accuracy(%)': accuracy\n",
    "        })\n",
    "\n",
    "        # Save model\n",
    "        model_file = os.path.join(output_models, file.replace(\".csv\", \"_arima_model.pkl\"))\n",
    "        joblib.dump(model_fit, model_file)\n",
    "\n",
    "        # Save updated CSV\n",
    "        save_df = df[['Date', 'Actual', 'Predicted']]\n",
    "        updated_csv_path = os.path.join(output_csv, file.replace(\".csv\", \"_arima_updated.csv\"))\n",
    "        save_df.to_csv(updated_csv_path, index=False)\n",
    "        print(f\"Saved updated CSV: {updated_csv_path}\")\n",
    "\n",
    "        # Moving averages\n",
    "        df['MA7'] = df['Actual'].rolling(7).mean()\n",
    "        df['MA30'] = df['Actual'].rolling(30).mean()\n",
    "\n",
    "        # Graph\n",
    "        plt.figure(figsize=(12,6))\n",
    "        plt.plot(df['Date'], df['Actual'], label='Actual', color='blue')\n",
    "        plt.plot(df['Date'], df['Predicted'], label='Predicted', color='red')\n",
    "        plt.plot(df['Date'], df['MA7'], label='MA7', color='green', linestyle='--')\n",
    "        plt.plot(df['Date'], df['MA30'], label='MA30', color='orange', linestyle='--')\n",
    "        plt.title(f\"{file} - Actual vs Predicted (ARIMA)\")\n",
    "        plt.xlabel(\"Date\")\n",
    "        plt.ylabel(\"Price\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        graph_path = os.path.join(output_graphs, file.replace(\".csv\", \"_arima_graph.png\"))\n",
    "        plt.savefig(graph_path, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "# -----------------------------\n",
    "# Save metrics\n",
    "# -----------------------------\n",
    "metrics_df = pd.DataFrame(metrics_list)\n",
    "metrics_df.to_csv(output_metrics_csv, index=False)\n",
    "print(f\"âœ… All done! Metrics saved to {output_metrics_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e338c04-6ba3-441f-acfa-f6110eb5a903",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SARIMAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f841492-73a6-4d89-b909-c25190f827ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Processing: capsicum_Bangalore_daily.csv ==========\n",
      "Metrics for capsicum_Bangalore_daily.csv:\n",
      "  MAE        : 489.67\n",
      "  RMSE       : 746.87\n",
      "  RÂ²         : 0.6433\n",
      "  MAPE(%)    : 15.4\n",
      "  Accuracy(%) : 84.6\n",
      "\n",
      "Saved updated CSV with Date, Actual & Predicted: sarimax_output_csv\\capsicum_Bangalore_daily_sarimax_updated.csv\n",
      "========== Processing: capsicum_Belgaum_daily.csv ==========\n",
      "Metrics for capsicum_Belgaum_daily.csv:\n",
      "  MAE        : 88.75\n",
      "  RMSE       : 257.56\n",
      "  RÂ²         : 0.8945\n",
      "  MAPE(%)    : 3.13\n",
      "  Accuracy(%) : 96.87\n",
      "\n",
      "Saved updated CSV with Date, Actual & Predicted: sarimax_output_csv\\capsicum_Belgaum_daily_sarimax_updated.csv\n",
      "========== Processing: capsicum_Bellary_daily.csv ==========\n",
      "Metrics for capsicum_Bellary_daily.csv:\n",
      "  MAE        : 205.5\n",
      "  RMSE       : 403.02\n",
      "  RÂ²         : 0.3195\n",
      "  MAPE(%)    : 8.36\n",
      "  Accuracy(%) : 91.64\n",
      "\n",
      "Saved updated CSV with Date, Actual & Predicted: sarimax_output_csv\\capsicum_Bellary_daily_sarimax_updated.csv\n",
      "========== Processing: capsicum_Chikmagalur_daily.csv ==========\n",
      "Metrics for capsicum_Chikmagalur_daily.csv:\n",
      "  MAE        : 158.57\n",
      "  RMSE       : 256.63\n",
      "  RÂ²         : 0.811\n",
      "  MAPE(%)    : 9.54\n",
      "  Accuracy(%) : 90.46\n",
      "\n",
      "Saved updated CSV with Date, Actual & Predicted: sarimax_output_csv\\capsicum_Chikmagalur_daily_sarimax_updated.csv\n",
      "========== Processing: capsicum_Davangere_daily.csv ==========\n",
      "Metrics for capsicum_Davangere_daily.csv:\n",
      "  MAE        : 103.92\n",
      "  RMSE       : 250.12\n",
      "  RÂ²         : 0.9793\n",
      "  MAPE(%)    : 2.89\n",
      "  Accuracy(%) : 97.11\n",
      "\n",
      "Saved updated CSV with Date, Actual & Predicted: sarimax_output_csv\\capsicum_Davangere_daily_sarimax_updated.csv\n",
      "========== Processing: capsicum_Dharwad_daily.csv ==========\n",
      "Metrics for capsicum_Dharwad_daily.csv:\n",
      "  MAE        : 606.99\n",
      "  RMSE       : 807.87\n",
      "  RÂ²         : 0.5607\n",
      "  MAPE(%)    : 37.79\n",
      "  Accuracy(%) : 62.21\n",
      "\n",
      "Saved updated CSV with Date, Actual & Predicted: sarimax_output_csv\\capsicum_Dharwad_daily_sarimax_updated.csv\n",
      "========== Processing: capsicum_Hassan_daily.csv ==========\n",
      "Metrics for capsicum_Hassan_daily.csv:\n",
      "  MAE        : 175.47\n",
      "  RMSE       : 381.42\n",
      "  RÂ²         : 0.8569\n",
      "  MAPE(%)    : 6.85\n",
      "  Accuracy(%) : 93.15\n",
      "\n",
      "Saved updated CSV with Date, Actual & Predicted: sarimax_output_csv\\capsicum_Hassan_daily_sarimax_updated.csv\n",
      "========== Processing: capsicum_Haveri_daily.csv ==========\n",
      "Metrics for capsicum_Haveri_daily.csv:\n",
      "  MAE        : 2850.0\n",
      "  RMSE       : 2850.0\n",
      "  RÂ²         : nan\n",
      "  MAPE(%)    : 100.0\n",
      "  Accuracy(%) : 0.0\n",
      "\n",
      "Saved updated CSV with Date, Actual & Predicted: sarimax_output_csv\\capsicum_Haveri_daily_sarimax_updated.csv\n",
      "========== Processing: capsicum_Kalburgi_daily.csv ==========\n",
      "Metrics for capsicum_Kalburgi_daily.csv:\n",
      "  MAE        : 178.41\n",
      "  RMSE       : 331.9\n",
      "  RÂ²         : 0.9309\n",
      "  MAPE(%)    : 7.58\n",
      "  Accuracy(%) : 92.42\n",
      "\n",
      "Saved updated CSV with Date, Actual & Predicted: sarimax_output_csv\\capsicum_Kalburgi_daily_sarimax_updated.csv\n",
      "========== Processing: capsicum_Kolar_daily.csv ==========\n",
      "Metrics for capsicum_Kolar_daily.csv:\n",
      "  MAE        : 923.52\n",
      "  RMSE       : 1348.19\n",
      "  RÂ²         : 0.3116\n",
      "  MAPE(%)    : 44.99\n",
      "  Accuracy(%) : 55.01\n",
      "\n",
      "Saved updated CSV with Date, Actual & Predicted: sarimax_output_csv\\capsicum_Kolar_daily_sarimax_updated.csv\n",
      "========== Processing: capsicum_Mandya_daily.csv ==========\n",
      "Metrics for capsicum_Mandya_daily.csv:\n",
      "  MAE        : 148.71\n",
      "  RMSE       : 328.16\n",
      "  RÂ²         : 0.9185\n",
      "  MAPE(%)    : 6.2\n",
      "  Accuracy(%) : 93.8\n",
      "\n",
      "Saved updated CSV with Date, Actual & Predicted: sarimax_output_csv\\capsicum_Mandya_daily_sarimax_updated.csv\n",
      "========== Processing: capsicum_Mysore_daily.csv ==========\n",
      "Metrics for capsicum_Mysore_daily.csv:\n",
      "  MAE        : 512.65\n",
      "  RMSE       : 775.07\n",
      "  RÂ²         : 0.6187\n",
      "  MAPE(%)    : 19.43\n",
      "  Accuracy(%) : 80.57\n",
      "\n",
      "Saved updated CSV with Date, Actual & Predicted: sarimax_output_csv\\capsicum_Mysore_daily_sarimax_updated.csv\n",
      "========== Processing: capsicum_Shimoga_daily.csv ==========\n",
      "Metrics for capsicum_Shimoga_daily.csv:\n",
      "  MAE        : 26.64\n",
      "  RMSE       : 166.92\n",
      "  RÂ²         : 0.2338\n",
      "  MAPE(%)    : 1.99\n",
      "  Accuracy(%) : 98.01\n",
      "\n",
      "Saved updated CSV with Date, Actual & Predicted: sarimax_output_csv\\capsicum_Shimoga_daily_sarimax_updated.csv\n",
      "========== Processing: capsicum_Udupi_daily.csv ==========\n",
      "Metrics for capsicum_Udupi_daily.csv:\n",
      "  MAE        : 44.62\n",
      "  RMSE       : 176.19\n",
      "  RÂ²         : 0.9861\n",
      "  MAPE(%)    : 1.08\n",
      "  Accuracy(%) : 98.92\n",
      "\n",
      "Saved updated CSV with Date, Actual & Predicted: sarimax_output_csv\\capsicum_Udupi_daily_sarimax_updated.csv\n",
      "âœ… Multiple CSVs processed! Metrics saved to sarimax_metrics.csv. Models, updated CSVs, and graphs saved in respective folders.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import warnings\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")  # Ignore warnings\n",
    "\n",
    "# -----------------------------\n",
    "# Paths\n",
    "# -----------------------------\n",
    "input_folder = \"dataset\"\n",
    "output_models = \"sarimax_output_models\"\n",
    "output_csv = \"sarimax_output_csv\"\n",
    "output_graphs = \"sarimax_output_graphs\"\n",
    "output_metrics_csv = \"sarimax_metrics.csv\"\n",
    "\n",
    "os.makedirs(output_models, exist_ok=True)\n",
    "os.makedirs(output_csv, exist_ok=True)\n",
    "os.makedirs(output_graphs, exist_ok=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Metrics storage\n",
    "# -----------------------------\n",
    "metrics_list = []\n",
    "\n",
    "# -----------------------------\n",
    "# Function to robustly parse dates\n",
    "# -----------------------------\n",
    "def parse_dates_safe(date_series):\n",
    "    try:\n",
    "        # Try DD-MM-YYYY first\n",
    "        return pd.to_datetime(date_series, dayfirst=True)\n",
    "    except:\n",
    "        # Fallback to MM-DD-YYYY\n",
    "        return pd.to_datetime(date_series, dayfirst=False)\n",
    "\n",
    "# -----------------------------\n",
    "# Function to calculate MAPE\n",
    "# -----------------------------\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "# -----------------------------\n",
    "# Process each CSV\n",
    "# -----------------------------\n",
    "for file in os.listdir(input_folder):\n",
    "    if file.endswith(\".csv\"):\n",
    "        file_path = os.path.join(input_folder, file)\n",
    "        print(f\"========== Processing: {file} ==========\")\n",
    "\n",
    "        # Load CSV\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # -----------------------------\n",
    "        # Convert 'Date' column safely\n",
    "        # -----------------------------\n",
    "        df['Date'] = parse_dates_safe(df['Date'])\n",
    "        df = df.sort_values('Date')\n",
    "        df.set_index('Date', inplace=True)\n",
    "\n",
    "        # Handle missing values\n",
    "        if df['Average Price'].isna().sum() > 0:\n",
    "            mean_value = df['Average Price'].mean()\n",
    "            df['Average Price'].fillna(mean_value, inplace=True)\n",
    "            print(f\"Filled {df['Average Price'].isna().sum()} missing values with mean: {mean_value:.2f}\")\n",
    "\n",
    "        # Round actual values\n",
    "        df['Average Price'] = df['Average Price'].round(2)\n",
    "\n",
    "        # Add moving averages\n",
    "        df['MA_7'] = df['Average Price'].rolling(window=7).mean()\n",
    "        df['MA_30'] = df['Average Price'].rolling(window=30).mean()\n",
    "\n",
    "        # SARIMAX order (tune if needed)\n",
    "        order = (1, 1, 1)\n",
    "        seasonal_order = (1, 1, 1, 7)\n",
    "\n",
    "        # Fit SARIMAX model\n",
    "        model = SARIMAX(df['Average Price'],\n",
    "                        order=order,\n",
    "                        seasonal_order=seasonal_order,\n",
    "                        enforce_stationarity=False,\n",
    "                        enforce_invertibility=False)\n",
    "        model_fit = model.fit(disp=False)\n",
    "\n",
    "        # Save model\n",
    "        model_file = os.path.join(output_models, file.replace(\".csv\", \"_sarimax_model.pkl\"))\n",
    "        joblib.dump(model_fit, model_file)\n",
    "\n",
    "        # Predictions\n",
    "        df['Predicted'] = model_fit.predict(start=0, end=len(df)-1)\n",
    "        df['Predicted'] = df['Predicted'].round(2)  # round predictions\n",
    "\n",
    "        # Rename Average Price to Actual\n",
    "        df.rename(columns={'Average Price': 'Actual'}, inplace=True)\n",
    "\n",
    "        # Calculate metrics\n",
    "        y_true = df['Actual'].values\n",
    "        y_pred = df['Predicted'].values\n",
    "        mae = round(mean_absolute_error(y_true, y_pred), 2)\n",
    "        rmse = round(np.sqrt(mean_squared_error(y_true, y_pred)), 2)\n",
    "        r2 = round(r2_score(y_true, y_pred), 4)\n",
    "        mape = round(mean_absolute_percentage_error(y_true, y_pred), 2)\n",
    "        accuracy = round(100 - mape, 2)\n",
    "\n",
    "        print(f\"Metrics for {file}:\")\n",
    "        print(f\"  MAE        : {mae}\")\n",
    "        print(f\"  RMSE       : {rmse}\")\n",
    "        print(f\"  RÂ²         : {r2}\")\n",
    "        print(f\"  MAPE(%)    : {mape}\")\n",
    "        print(f\"  Accuracy(%) : {accuracy}\\n\")\n",
    "\n",
    "        # Save metrics\n",
    "        metrics_list.append({\n",
    "            'File': file,\n",
    "            'MAE': mae,\n",
    "            'RMSE': rmse,\n",
    "            'R2': r2,\n",
    "            'MAPE(%)': mape,\n",
    "            'Accuracy(%)': accuracy\n",
    "        })\n",
    "\n",
    "        # -----------------------------\n",
    "        # Save updated CSV with only Date, Actual, Predicted\n",
    "        # -----------------------------\n",
    "        save_df = df[['Actual', 'Predicted']].copy()\n",
    "        save_df['Date'] = df.index\n",
    "        save_df = save_df[['Date', 'Actual', 'Predicted']]\n",
    "        updated_csv_path = os.path.join(output_csv, file.replace(\".csv\", \"_sarimax_updated.csv\"))\n",
    "        save_df.to_csv(updated_csv_path, index=False)\n",
    "        print(f\"Saved updated CSV with Date, Actual & Predicted: {updated_csv_path}\")\n",
    "\n",
    "        # -----------------------------\n",
    "        # Plot graph\n",
    "        # -----------------------------\n",
    "        plt.figure(figsize=(12,6))\n",
    "        plt.plot(df.index, df['Actual'], label=\"Actual\", color=\"blue\")\n",
    "        plt.plot(df.index, df['Predicted'], label=\"Predicted\", color=\"red\", linestyle=\"dashed\")\n",
    "        plt.plot(df.index, df['MA_7'], label=\"MA 7\", color=\"orange\")\n",
    "        plt.plot(df.index, df['MA_30'], label=\"MA 30\", color=\"green\")\n",
    "        plt.xlabel(\"Date\")\n",
    "        plt.ylabel(\"Average Price\")\n",
    "        plt.title(f\"Price Prediction (SARIMAX) - {file}\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        graph_file = os.path.join(output_graphs, file.replace(\".csv\", \"_sarimax_graph.png\"))\n",
    "        plt.savefig(graph_file, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "# Save all metrics to CSV\n",
    "metrics_df = pd.DataFrame(metrics_list)\n",
    "metrics_df.to_csv(output_metrics_csv, index=False)\n",
    "print(f\"âœ… Multiple CSVs processed! Metrics saved to {output_metrics_csv}. Models, updated CSVs, and graphs saved in respective folders.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1cecd1-bdfd-423a-9e7e-be5a5cbd2de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TAT+MHA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc84b6f6-92ae-4d79-9a3d-b6854863c2f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Processing: capsicum_Bangalore_daily.csv\n",
      "Epoch 1/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 17ms/step - loss: 0.0391 - mae: 0.1520 - val_loss: 0.0220 - val_mae: 0.1024\n",
      "Epoch 2/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 16ms/step - loss: 0.0152 - mae: 0.0890 - val_loss: 0.0226 - val_mae: 0.1044\n",
      "Epoch 3/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 17ms/step - loss: 0.0163 - mae: 0.0912 - val_loss: 0.0224 - val_mae: 0.1039\n",
      "Epoch 4/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - loss: 0.0160 - mae: 0.0906 - val_loss: 0.0231 - val_mae: 0.1061\n",
      "Epoch 5/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 17ms/step - loss: 0.0154 - mae: 0.0889 - val_loss: 0.0219 - val_mae: 0.1023\n",
      "Epoch 6/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - loss: 0.0156 - mae: 0.0897 - val_loss: 0.0226 - val_mae: 0.1045\n",
      "Epoch 7/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 16ms/step - loss: 0.0155 - mae: 0.0893 - val_loss: 0.0243 - val_mae: 0.1100\n",
      "Epoch 8/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 16ms/step - loss: 0.0154 - mae: 0.0897 - val_loss: 0.0236 - val_mae: 0.1078\n",
      "Epoch 9/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 16ms/step - loss: 0.0152 - mae: 0.0893 - val_loss: 0.0212 - val_mae: 0.0999\n",
      "Epoch 10/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - loss: 0.0154 - mae: 0.0890 - val_loss: 0.0216 - val_mae: 0.1010\n",
      "Epoch 11/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 16ms/step - loss: 0.0157 - mae: 0.0905 - val_loss: 0.0226 - val_mae: 0.1044\n",
      "Epoch 12/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 16ms/step - loss: 0.0156 - mae: 0.0891 - val_loss: 0.0223 - val_mae: 0.1034\n",
      "Epoch 13/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 17ms/step - loss: 0.0158 - mae: 0.0898 - val_loss: 0.0242 - val_mae: 0.1100\n",
      "Epoch 14/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 0.0157 - mae: 0.0901 - val_loss: 0.0218 - val_mae: 0.1019\n",
      "Epoch 15/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - loss: 0.0159 - mae: 0.0902 - val_loss: 0.0217 - val_mae: 0.1014\n",
      "Epoch 16/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - loss: 0.0160 - mae: 0.0910 - val_loss: 0.0220 - val_mae: 0.1023\n",
      "Epoch 17/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 17ms/step - loss: 0.0160 - mae: 0.0885 - val_loss: 0.0205 - val_mae: 0.0975\n",
      "Epoch 18/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 0.0158 - mae: 0.0899 - val_loss: 0.0220 - val_mae: 0.1024\n",
      "Epoch 19/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 16ms/step - loss: 0.0160 - mae: 0.0910 - val_loss: 0.0243 - val_mae: 0.1102\n",
      "Epoch 20/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 16ms/step - loss: 0.0157 - mae: 0.0888 - val_loss: 0.0228 - val_mae: 0.1053\n",
      "Epoch 21/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - loss: 0.0159 - mae: 0.0900 - val_loss: 0.0235 - val_mae: 0.1076\n",
      "Epoch 22/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 16ms/step - loss: 0.0161 - mae: 0.0903 - val_loss: 0.0234 - val_mae: 0.1074\n",
      "Epoch 23/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 16ms/step - loss: 0.0161 - mae: 0.0916 - val_loss: 0.0225 - val_mae: 0.1042\n",
      "Epoch 24/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - loss: 0.0160 - mae: 0.0909 - val_loss: 0.0206 - val_mae: 0.0978\n",
      "Epoch 25/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 0.0147 - mae: 0.0882 - val_loss: 0.0217 - val_mae: 0.1013\n",
      "Epoch 26/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - loss: 0.0168 - mae: 0.0923 - val_loss: 0.0234 - val_mae: 0.1072\n",
      "Epoch 27/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - loss: 0.0153 - mae: 0.0884 - val_loss: 0.0209 - val_mae: 0.0987\n",
      "Epoch 28/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - loss: 0.0156 - mae: 0.0894 - val_loss: 0.0223 - val_mae: 0.1036\n",
      "Epoch 29/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 16ms/step - loss: 0.0156 - mae: 0.0897 - val_loss: 0.0241 - val_mae: 0.1095\n",
      "Epoch 30/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - loss: 0.0162 - mae: 0.0911 - val_loss: 0.0225 - val_mae: 0.1041\n",
      "Epoch 31/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 16ms/step - loss: 0.0160 - mae: 0.0912 - val_loss: 0.0235 - val_mae: 0.1077\n",
      "Epoch 32/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 17ms/step - loss: 0.0154 - mae: 0.0892 - val_loss: 0.0213 - val_mae: 0.1002\n",
      "Epoch 33/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 0.0154 - mae: 0.0897 - val_loss: 0.0235 - val_mae: 0.1074\n",
      "Epoch 34/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 0.0158 - mae: 0.0900 - val_loss: 0.0211 - val_mae: 0.0995\n",
      "Epoch 35/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - loss: 0.0159 - mae: 0.0911 - val_loss: 0.0236 - val_mae: 0.1079\n",
      "Epoch 36/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 0.0156 - mae: 0.0900 - val_loss: 0.0218 - val_mae: 0.1019\n",
      "Epoch 37/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 16ms/step - loss: 0.0163 - mae: 0.0911 - val_loss: 0.0213 - val_mae: 0.1001\n",
      "Epoch 38/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 16ms/step - loss: 0.0164 - mae: 0.0915 - val_loss: 0.0217 - val_mae: 0.1015\n",
      "Epoch 39/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - loss: 0.0157 - mae: 0.0894 - val_loss: 0.0230 - val_mae: 0.1059\n",
      "Epoch 40/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 16ms/step - loss: 0.0161 - mae: 0.0900 - val_loss: 0.0233 - val_mae: 0.1067\n",
      "Epoch 41/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 16ms/step - loss: 0.0157 - mae: 0.0895 - val_loss: 0.0215 - val_mae: 0.1007\n",
      "Epoch 42/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 0.0151 - mae: 0.0882 - val_loss: 0.0211 - val_mae: 0.0996\n",
      "Epoch 43/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 16ms/step - loss: 0.0157 - mae: 0.0899 - val_loss: 0.0219 - val_mae: 0.1023\n",
      "Epoch 44/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 16ms/step - loss: 0.0152 - mae: 0.0883 - val_loss: 0.0219 - val_mae: 0.1022\n",
      "Epoch 45/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 16ms/step - loss: 0.0159 - mae: 0.0913 - val_loss: 0.0221 - val_mae: 0.1029\n",
      "Epoch 46/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 16ms/step - loss: 0.0150 - mae: 0.0881 - val_loss: 0.0227 - val_mae: 0.1047\n",
      "Epoch 47/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - loss: 0.0156 - mae: 0.0896 - val_loss: 0.0229 - val_mae: 0.1055\n",
      "Epoch 48/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - loss: 0.0153 - mae: 0.0892 - val_loss: 0.0201 - val_mae: 0.0960\n",
      "Epoch 49/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - loss: 0.0156 - mae: 0.0897 - val_loss: 0.0222 - val_mae: 0.1030\n",
      "Epoch 50/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - loss: 0.0159 - mae: 0.0896 - val_loss: 0.0233 - val_mae: 0.1069\n",
      "Epoch 51/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - loss: 0.0155 - mae: 0.0892 - val_loss: 0.0217 - val_mae: 0.1015\n",
      "Epoch 52/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - loss: 0.0158 - mae: 0.0902 - val_loss: 0.0218 - val_mae: 0.1019\n",
      "Epoch 53/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - loss: 0.0164 - mae: 0.0907 - val_loss: 0.0218 - val_mae: 0.1018\n",
      "Epoch 54/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 18ms/step - loss: 0.0160 - mae: 0.0901 - val_loss: 0.0216 - val_mae: 0.1013\n",
      "Epoch 55/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 18ms/step - loss: 0.0161 - mae: 0.0906 - val_loss: 0.0223 - val_mae: 0.1036\n",
      "Epoch 56/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 18ms/step - loss: 0.0154 - mae: 0.0885 - val_loss: 0.0223 - val_mae: 0.1035\n",
      "Epoch 57/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 18ms/step - loss: 0.0156 - mae: 0.0894 - val_loss: 0.0223 - val_mae: 0.1035\n",
      "Epoch 58/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - loss: 0.0158 - mae: 0.0899 - val_loss: 0.0239 - val_mae: 0.1088\n",
      "Epoch 59/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 18ms/step - loss: 0.0160 - mae: 0.0902 - val_loss: 0.0235 - val_mae: 0.1075\n",
      "Epoch 60/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - loss: 0.0154 - mae: 0.0892 - val_loss: 0.0235 - val_mae: 0.1076\n",
      "Epoch 61/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 18ms/step - loss: 0.0156 - mae: 0.0897 - val_loss: 0.0225 - val_mae: 0.1043\n",
      "Epoch 62/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 17ms/step - loss: 0.0165 - mae: 0.0912 - val_loss: 0.0230 - val_mae: 0.1059\n",
      "Epoch 63/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 18ms/step - loss: 0.0159 - mae: 0.0901 - val_loss: 0.0225 - val_mae: 0.1042\n",
      "Epoch 64/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 17ms/step - loss: 0.0157 - mae: 0.0888 - val_loss: 0.0222 - val_mae: 0.1033\n",
      "Epoch 65/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 17ms/step - loss: 0.0162 - mae: 0.0905 - val_loss: 0.0235 - val_mae: 0.1075\n",
      "Epoch 66/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 18ms/step - loss: 0.0155 - mae: 0.0892 - val_loss: 0.0213 - val_mae: 0.1002\n",
      "Epoch 67/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - loss: 0.0157 - mae: 0.0892 - val_loss: 0.0224 - val_mae: 0.1037\n",
      "Epoch 68/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 17ms/step - loss: 0.0165 - mae: 0.0908 - val_loss: 0.0225 - val_mae: 0.1041\n",
      "Epoch 69/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 18ms/step - loss: 0.0154 - mae: 0.0890 - val_loss: 0.0250 - val_mae: 0.1123\n",
      "Epoch 70/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - loss: 0.0160 - mae: 0.0902 - val_loss: 0.0231 - val_mae: 0.1064\n",
      "Epoch 71/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 22ms/step - loss: 0.0152 - mae: 0.0889 - val_loss: 0.0234 - val_mae: 0.1071\n",
      "Epoch 72/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - loss: 0.0160 - mae: 0.0901 - val_loss: 0.0232 - val_mae: 0.1067\n",
      "Epoch 73/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 18ms/step - loss: 0.0157 - mae: 0.0899 - val_loss: 0.0242 - val_mae: 0.1097\n",
      "Epoch 74/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - loss: 0.0156 - mae: 0.0894 - val_loss: 0.0213 - val_mae: 0.1001\n",
      "Epoch 75/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 18ms/step - loss: 0.0158 - mae: 0.0905 - val_loss: 0.0213 - val_mae: 0.1002\n",
      "Epoch 76/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - loss: 0.0158 - mae: 0.0899 - val_loss: 0.0234 - val_mae: 0.1072\n",
      "Epoch 77/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - loss: 0.0155 - mae: 0.0895 - val_loss: 0.0237 - val_mae: 0.1081\n",
      "Epoch 78/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 18ms/step - loss: 0.0158 - mae: 0.0897 - val_loss: 0.0223 - val_mae: 0.1034\n",
      "Epoch 79/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 18ms/step - loss: 0.0159 - mae: 0.0911 - val_loss: 0.0223 - val_mae: 0.1034\n",
      "Epoch 80/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 18ms/step - loss: 0.0151 - mae: 0.0880 - val_loss: 0.0218 - val_mae: 0.1019\n",
      "Epoch 81/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - loss: 0.0150 - mae: 0.0893 - val_loss: 0.0225 - val_mae: 0.1043\n",
      "Epoch 82/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - loss: 0.0149 - mae: 0.0881 - val_loss: 0.0207 - val_mae: 0.0980\n",
      "Epoch 83/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 18ms/step - loss: 0.0161 - mae: 0.0904 - val_loss: 0.0221 - val_mae: 0.1029\n",
      "Epoch 84/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 18ms/step - loss: 0.0160 - mae: 0.0903 - val_loss: 0.0218 - val_mae: 0.1019\n",
      "Epoch 85/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - loss: 0.0159 - mae: 0.0890 - val_loss: 0.0217 - val_mae: 0.1016\n",
      "Epoch 86/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - loss: 0.0162 - mae: 0.0901 - val_loss: 0.0213 - val_mae: 0.1000\n",
      "Epoch 87/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - loss: 0.0149 - mae: 0.0879 - val_loss: 0.0210 - val_mae: 0.0992\n",
      "Epoch 88/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - loss: 0.0150 - mae: 0.0883 - val_loss: 0.0233 - val_mae: 0.1069\n",
      "Epoch 89/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - loss: 0.0156 - mae: 0.0897 - val_loss: 0.0218 - val_mae: 0.1018\n",
      "Epoch 90/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - loss: 0.0157 - mae: 0.0897 - val_loss: 0.0217 - val_mae: 0.1016\n",
      "Epoch 91/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 18ms/step - loss: 0.0161 - mae: 0.0901 - val_loss: 0.0230 - val_mae: 0.1057\n",
      "Epoch 92/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - loss: 0.0153 - mae: 0.0891 - val_loss: 0.0218 - val_mae: 0.1019\n",
      "Epoch 93/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - loss: 0.0156 - mae: 0.0895 - val_loss: 0.0231 - val_mae: 0.1064\n",
      "Epoch 94/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 18ms/step - loss: 0.0162 - mae: 0.0913 - val_loss: 0.0224 - val_mae: 0.1038\n",
      "Epoch 95/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 18ms/step - loss: 0.0159 - mae: 0.0894 - val_loss: 0.0214 - val_mae: 0.1006\n",
      "Epoch 96/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 18ms/step - loss: 0.0163 - mae: 0.0909 - val_loss: 0.0218 - val_mae: 0.1018\n",
      "Epoch 97/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - loss: 0.0159 - mae: 0.0904 - val_loss: 0.0218 - val_mae: 0.1017\n",
      "Epoch 98/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - loss: 0.0161 - mae: 0.0898 - val_loss: 0.0235 - val_mae: 0.1076\n",
      "Epoch 99/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - loss: 0.0159 - mae: 0.0905 - val_loss: 0.0220 - val_mae: 0.1025\n",
      "Epoch 100/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 18ms/step - loss: 0.0160 - mae: 0.0900 - val_loss: 0.0227 - val_mae: 0.1049\n",
      "\u001b[1m290/290\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Done with capsicum_Bangalore_daily.csv | MAE=895.16, RMSE=1263.72, R2=-0.0217, MAPE=31.73%, ACC=68.27%\n",
      "\n",
      "ğŸš€ Processing: capsicum_Belgaum_daily.csv\n",
      "Epoch 1/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 66ms/step - loss: 0.0909 - mae: 0.2454 - val_loss: 0.0907 - val_mae: 0.2915\n",
      "Epoch 2/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.0691 - mae: 0.2166 - val_loss: 0.0716 - val_mae: 0.2566\n",
      "Epoch 3/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0493 - mae: 0.1745 - val_loss: 0.0558 - val_mae: 0.2237\n",
      "Epoch 4/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0479 - mae: 0.1669 - val_loss: 0.0425 - val_mae: 0.1917\n",
      "Epoch 5/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0381 - mae: 0.1510 - val_loss: 0.0328 - val_mae: 0.1644\n",
      "Epoch 6/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0362 - mae: 0.1483 - val_loss: 0.0253 - val_mae: 0.1402\n",
      "Epoch 7/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0369 - mae: 0.1437 - val_loss: 0.0204 - val_mae: 0.1221\n",
      "Epoch 8/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0353 - mae: 0.1335 - val_loss: 0.0171 - val_mae: 0.1083\n",
      "Epoch 9/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0255 - mae: 0.1234 - val_loss: 0.0145 - val_mae: 0.0961\n",
      "Epoch 10/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0217 - mae: 0.1143 - val_loss: 0.0130 - val_mae: 0.0885\n",
      "Epoch 11/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0286 - mae: 0.1207 - val_loss: 0.0118 - val_mae: 0.0821\n",
      "Epoch 12/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0284 - mae: 0.1204 - val_loss: 0.0115 - val_mae: 0.0806\n",
      "Epoch 13/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0231 - mae: 0.1151 - val_loss: 0.0114 - val_mae: 0.0801\n",
      "Epoch 14/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0296 - mae: 0.1230 - val_loss: 0.0112 - val_mae: 0.0789\n",
      "Epoch 15/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0266 - mae: 0.1216 - val_loss: 0.0110 - val_mae: 0.0778\n",
      "Epoch 16/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0312 - mae: 0.1256 - val_loss: 0.0107 - val_mae: 0.0759\n",
      "Epoch 17/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0255 - mae: 0.1184 - val_loss: 0.0110 - val_mae: 0.0775\n",
      "Epoch 18/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0288 - mae: 0.1226 - val_loss: 0.0108 - val_mae: 0.0766\n",
      "Epoch 19/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0230 - mae: 0.1165 - val_loss: 0.0108 - val_mae: 0.0766\n",
      "Epoch 20/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0303 - mae: 0.1276 - val_loss: 0.0103 - val_mae: 0.0740\n",
      "Epoch 21/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0248 - mae: 0.1123 - val_loss: 0.0105 - val_mae: 0.0750\n",
      "Epoch 22/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0267 - mae: 0.1189 - val_loss: 0.0105 - val_mae: 0.0748\n",
      "Epoch 23/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0252 - mae: 0.1149 - val_loss: 0.0109 - val_mae: 0.0773\n",
      "Epoch 24/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0262 - mae: 0.1245 - val_loss: 0.0111 - val_mae: 0.0780\n",
      "Epoch 25/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0302 - mae: 0.1261 - val_loss: 0.0108 - val_mae: 0.0764\n",
      "Epoch 26/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0286 - mae: 0.1257 - val_loss: 0.0109 - val_mae: 0.0771\n",
      "Epoch 27/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0258 - mae: 0.1212 - val_loss: 0.0110 - val_mae: 0.0775\n",
      "Epoch 28/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0318 - mae: 0.1294 - val_loss: 0.0106 - val_mae: 0.0752\n",
      "Epoch 29/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0242 - mae: 0.1194 - val_loss: 0.0111 - val_mae: 0.0781\n",
      "Epoch 30/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0270 - mae: 0.1226 - val_loss: 0.0104 - val_mae: 0.0741\n",
      "Epoch 31/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0263 - mae: 0.1228 - val_loss: 0.0107 - val_mae: 0.0762\n",
      "Epoch 32/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0278 - mae: 0.1181 - val_loss: 0.0108 - val_mae: 0.0764\n",
      "Epoch 33/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0266 - mae: 0.1222 - val_loss: 0.0108 - val_mae: 0.0766\n",
      "Epoch 34/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0248 - mae: 0.1175 - val_loss: 0.0110 - val_mae: 0.0776\n",
      "Epoch 35/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0193 - mae: 0.1046 - val_loss: 0.0107 - val_mae: 0.0760\n",
      "Epoch 36/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0232 - mae: 0.1143 - val_loss: 0.0108 - val_mae: 0.0768\n",
      "Epoch 37/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0367 - mae: 0.1341 - val_loss: 0.0108 - val_mae: 0.0768\n",
      "Epoch 38/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0294 - mae: 0.1289 - val_loss: 0.0108 - val_mae: 0.0766\n",
      "Epoch 39/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0281 - mae: 0.1248 - val_loss: 0.0112 - val_mae: 0.0789\n",
      "Epoch 40/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0281 - mae: 0.1202 - val_loss: 0.0106 - val_mae: 0.0753\n",
      "Epoch 41/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0301 - mae: 0.1248 - val_loss: 0.0109 - val_mae: 0.0769\n",
      "Epoch 42/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0249 - mae: 0.1191 - val_loss: 0.0104 - val_mae: 0.0741\n",
      "Epoch 43/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0224 - mae: 0.1170 - val_loss: 0.0109 - val_mae: 0.0774\n",
      "Epoch 44/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0263 - mae: 0.1223 - val_loss: 0.0105 - val_mae: 0.0746\n",
      "Epoch 45/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0301 - mae: 0.1261 - val_loss: 0.0105 - val_mae: 0.0746\n",
      "Epoch 46/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0266 - mae: 0.1201 - val_loss: 0.0107 - val_mae: 0.0762\n",
      "Epoch 47/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0237 - mae: 0.1158 - val_loss: 0.0112 - val_mae: 0.0790\n",
      "Epoch 48/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0324 - mae: 0.1336 - val_loss: 0.0107 - val_mae: 0.0758\n",
      "Epoch 49/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0237 - mae: 0.1173 - val_loss: 0.0108 - val_mae: 0.0765\n",
      "Epoch 50/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0265 - mae: 0.1207 - val_loss: 0.0109 - val_mae: 0.0770\n",
      "Epoch 51/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0268 - mae: 0.1245 - val_loss: 0.0109 - val_mae: 0.0771\n",
      "Epoch 52/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0291 - mae: 0.1258 - val_loss: 0.0108 - val_mae: 0.0764\n",
      "Epoch 53/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0283 - mae: 0.1215 - val_loss: 0.0106 - val_mae: 0.0753\n",
      "Epoch 54/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0235 - mae: 0.1154 - val_loss: 0.0107 - val_mae: 0.0762\n",
      "Epoch 55/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0312 - mae: 0.1266 - val_loss: 0.0108 - val_mae: 0.0764\n",
      "Epoch 56/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0246 - mae: 0.1160 - val_loss: 0.0111 - val_mae: 0.0784\n",
      "Epoch 57/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0277 - mae: 0.1195 - val_loss: 0.0109 - val_mae: 0.0769\n",
      "Epoch 58/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 0.0335 - mae: 0.1244 - val_loss: 0.0109 - val_mae: 0.0771\n",
      "Epoch 59/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.0234 - mae: 0.1133 - val_loss: 0.0113 - val_mae: 0.0793\n",
      "Epoch 60/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0261 - mae: 0.1224 - val_loss: 0.0108 - val_mae: 0.0764\n",
      "Epoch 61/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0236 - mae: 0.1174 - val_loss: 0.0108 - val_mae: 0.0765\n",
      "Epoch 62/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0219 - mae: 0.1119 - val_loss: 0.0107 - val_mae: 0.0760\n",
      "Epoch 63/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0296 - mae: 0.1273 - val_loss: 0.0104 - val_mae: 0.0745\n",
      "Epoch 64/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0213 - mae: 0.1078 - val_loss: 0.0116 - val_mae: 0.0808\n",
      "Epoch 65/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0245 - mae: 0.1179 - val_loss: 0.0108 - val_mae: 0.0764\n",
      "Epoch 66/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0298 - mae: 0.1268 - val_loss: 0.0106 - val_mae: 0.0754\n",
      "Epoch 67/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0282 - mae: 0.1265 - val_loss: 0.0107 - val_mae: 0.0763\n",
      "Epoch 68/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.0261 - mae: 0.1196 - val_loss: 0.0109 - val_mae: 0.0769\n",
      "Epoch 69/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0261 - mae: 0.1201 - val_loss: 0.0104 - val_mae: 0.0743\n",
      "Epoch 70/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0220 - mae: 0.1116 - val_loss: 0.0108 - val_mae: 0.0765\n",
      "Epoch 71/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 0.0227 - mae: 0.1132 - val_loss: 0.0111 - val_mae: 0.0785\n",
      "Epoch 72/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0252 - mae: 0.1184 - val_loss: 0.0104 - val_mae: 0.0743\n",
      "Epoch 73/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.0270 - mae: 0.1182 - val_loss: 0.0106 - val_mae: 0.0756\n",
      "Epoch 74/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0257 - mae: 0.1199 - val_loss: 0.0109 - val_mae: 0.0770\n",
      "Epoch 75/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 0.0263 - mae: 0.1199 - val_loss: 0.0108 - val_mae: 0.0765\n",
      "Epoch 76/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 0.0265 - mae: 0.1227 - val_loss: 0.0108 - val_mae: 0.0765\n",
      "Epoch 77/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 0.0244 - mae: 0.1160 - val_loss: 0.0111 - val_mae: 0.0784\n",
      "Epoch 78/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0299 - mae: 0.1265 - val_loss: 0.0104 - val_mae: 0.0741\n",
      "Epoch 79/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.0231 - mae: 0.1155 - val_loss: 0.0106 - val_mae: 0.0756\n",
      "Epoch 80/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 0.0269 - mae: 0.1179 - val_loss: 0.0104 - val_mae: 0.0744\n",
      "Epoch 81/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.0296 - mae: 0.1247 - val_loss: 0.0106 - val_mae: 0.0752\n",
      "Epoch 82/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 0.0266 - mae: 0.1225 - val_loss: 0.0108 - val_mae: 0.0767\n",
      "Epoch 83/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.0256 - mae: 0.1188 - val_loss: 0.0109 - val_mae: 0.0773\n",
      "Epoch 84/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.0283 - mae: 0.1279 - val_loss: 0.0107 - val_mae: 0.0760\n",
      "Epoch 85/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.0277 - mae: 0.1223 - val_loss: 0.0108 - val_mae: 0.0763\n",
      "Epoch 86/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0245 - mae: 0.1184 - val_loss: 0.0108 - val_mae: 0.0765\n",
      "Epoch 87/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0230 - mae: 0.1149 - val_loss: 0.0109 - val_mae: 0.0774\n",
      "Epoch 88/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 0.0282 - mae: 0.1234 - val_loss: 0.0104 - val_mae: 0.0744\n",
      "Epoch 89/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0271 - mae: 0.1182 - val_loss: 0.0106 - val_mae: 0.0752\n",
      "Epoch 90/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0239 - mae: 0.1203 - val_loss: 0.0111 - val_mae: 0.0784\n",
      "Epoch 91/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0216 - mae: 0.1111 - val_loss: 0.0108 - val_mae: 0.0768\n",
      "Epoch 92/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0240 - mae: 0.1172 - val_loss: 0.0103 - val_mae: 0.0737\n",
      "Epoch 93/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 0.0294 - mae: 0.1210 - val_loss: 0.0104 - val_mae: 0.0746\n",
      "Epoch 94/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 0.0252 - mae: 0.1192 - val_loss: 0.0108 - val_mae: 0.0768\n",
      "Epoch 95/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 0.0288 - mae: 0.1242 - val_loss: 0.0109 - val_mae: 0.0774\n",
      "Epoch 96/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 0.0224 - mae: 0.1133 - val_loss: 0.0111 - val_mae: 0.0785\n",
      "Epoch 97/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 0.0260 - mae: 0.1182 - val_loss: 0.0104 - val_mae: 0.0741\n",
      "Epoch 98/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.0273 - mae: 0.1230 - val_loss: 0.0109 - val_mae: 0.0772\n",
      "Epoch 99/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0288 - mae: 0.1268 - val_loss: 0.0108 - val_mae: 0.0764\n",
      "Epoch 100/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.0289 - mae: 0.1240 - val_loss: 0.0104 - val_mae: 0.0744\n",
      "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Done with capsicum_Belgaum_daily.csv | MAE=627.17, RMSE=863.9, R2=-0.0056, MAPE=22.45%, ACC=77.55%\n",
      "\n",
      "ğŸš€ Processing: capsicum_Bellary_daily.csv\n",
      "Epoch 1/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9s/step - loss: 0.0986 - mae: 0.1853 - val_loss: 0.0572 - val_mae: 0.2063\n",
      "Epoch 2/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 273ms/step - loss: 0.0980 - mae: 0.1838 - val_loss: 0.0566 - val_mae: 0.2048\n",
      "Epoch 3/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 275ms/step - loss: 0.0975 - mae: 0.1823 - val_loss: 0.0560 - val_mae: 0.2033\n",
      "Epoch 4/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 266ms/step - loss: 0.0970 - mae: 0.1808 - val_loss: 0.0554 - val_mae: 0.2018\n",
      "Epoch 5/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 276ms/step - loss: 0.0964 - mae: 0.1793 - val_loss: 0.0548 - val_mae: 0.2003\n",
      "Epoch 6/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 267ms/step - loss: 0.0959 - mae: 0.1778 - val_loss: 0.0542 - val_mae: 0.1988\n",
      "Epoch 7/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 270ms/step - loss: 0.0953 - mae: 0.1763 - val_loss: 0.0536 - val_mae: 0.1973\n",
      "Epoch 8/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 275ms/step - loss: 0.0948 - mae: 0.1748 - val_loss: 0.0530 - val_mae: 0.1958\n",
      "Epoch 9/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 264ms/step - loss: 0.0943 - mae: 0.1733 - val_loss: 0.0524 - val_mae: 0.1943\n",
      "Epoch 10/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 261ms/step - loss: 0.0938 - mae: 0.1718 - val_loss: 0.0518 - val_mae: 0.1928\n",
      "Epoch 11/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 267ms/step - loss: 0.0933 - mae: 0.1703 - val_loss: 0.0512 - val_mae: 0.1913\n",
      "Epoch 12/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 275ms/step - loss: 0.0928 - mae: 0.1688 - val_loss: 0.0507 - val_mae: 0.1898\n",
      "Epoch 13/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 272ms/step - loss: 0.0923 - mae: 0.1673 - val_loss: 0.0501 - val_mae: 0.1883\n",
      "Epoch 14/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 262ms/step - loss: 0.0918 - mae: 0.1658 - val_loss: 0.0495 - val_mae: 0.1868\n",
      "Epoch 15/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 267ms/step - loss: 0.0913 - mae: 0.1643 - val_loss: 0.0490 - val_mae: 0.1853\n",
      "Epoch 16/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 276ms/step - loss: 0.0908 - mae: 0.1628 - val_loss: 0.0484 - val_mae: 0.1838\n",
      "Epoch 17/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 266ms/step - loss: 0.0903 - mae: 0.1613 - val_loss: 0.0479 - val_mae: 0.1823\n",
      "Epoch 18/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 281ms/step - loss: 0.0898 - mae: 0.1598 - val_loss: 0.0473 - val_mae: 0.1808\n",
      "Epoch 19/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 280ms/step - loss: 0.0893 - mae: 0.1583 - val_loss: 0.0468 - val_mae: 0.1793\n",
      "Epoch 20/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 273ms/step - loss: 0.0889 - mae: 0.1568 - val_loss: 0.0463 - val_mae: 0.1778\n",
      "Epoch 21/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 265ms/step - loss: 0.0884 - mae: 0.1553 - val_loss: 0.0457 - val_mae: 0.1763\n",
      "Epoch 22/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 266ms/step - loss: 0.0879 - mae: 0.1538 - val_loss: 0.0452 - val_mae: 0.1748\n",
      "Epoch 23/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 275ms/step - loss: 0.0875 - mae: 0.1523 - val_loss: 0.0447 - val_mae: 0.1733\n",
      "Epoch 24/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 267ms/step - loss: 0.0870 - mae: 0.1508 - val_loss: 0.0442 - val_mae: 0.1718\n",
      "Epoch 25/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 271ms/step - loss: 0.0866 - mae: 0.1493 - val_loss: 0.0437 - val_mae: 0.1703\n",
      "Epoch 26/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 267ms/step - loss: 0.0861 - mae: 0.1478 - val_loss: 0.0432 - val_mae: 0.1689\n",
      "Epoch 27/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 365ms/step - loss: 0.0857 - mae: 0.1464 - val_loss: 0.0427 - val_mae: 0.1674\n",
      "Epoch 28/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278ms/step - loss: 0.0853 - mae: 0.1449 - val_loss: 0.0422 - val_mae: 0.1659\n",
      "Epoch 29/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 269ms/step - loss: 0.0848 - mae: 0.1434 - val_loss: 0.0417 - val_mae: 0.1644\n",
      "Epoch 30/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 272ms/step - loss: 0.0844 - mae: 0.1419 - val_loss: 0.0412 - val_mae: 0.1630\n",
      "Epoch 31/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 338ms/step - loss: 0.0840 - mae: 0.1405 - val_loss: 0.0407 - val_mae: 0.1615\n",
      "Epoch 32/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 267ms/step - loss: 0.0836 - mae: 0.1390 - val_loss: 0.0403 - val_mae: 0.1600\n",
      "Epoch 33/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 274ms/step - loss: 0.0832 - mae: 0.1376 - val_loss: 0.0398 - val_mae: 0.1586\n",
      "Epoch 34/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 273ms/step - loss: 0.0828 - mae: 0.1377 - val_loss: 0.0393 - val_mae: 0.1571\n",
      "Epoch 35/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 266ms/step - loss: 0.0824 - mae: 0.1378 - val_loss: 0.0389 - val_mae: 0.1557\n",
      "Epoch 36/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 281ms/step - loss: 0.0820 - mae: 0.1379 - val_loss: 0.0384 - val_mae: 0.1542\n",
      "Epoch 37/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 272ms/step - loss: 0.0816 - mae: 0.1380 - val_loss: 0.0380 - val_mae: 0.1528\n",
      "Epoch 38/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 288ms/step - loss: 0.0812 - mae: 0.1381 - val_loss: 0.0376 - val_mae: 0.1513\n",
      "Epoch 39/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 271ms/step - loss: 0.0809 - mae: 0.1382 - val_loss: 0.0371 - val_mae: 0.1499\n",
      "Epoch 40/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 276ms/step - loss: 0.0805 - mae: 0.1383 - val_loss: 0.0367 - val_mae: 0.1485\n",
      "Epoch 41/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 272ms/step - loss: 0.0801 - mae: 0.1384 - val_loss: 0.0363 - val_mae: 0.1470\n",
      "Epoch 42/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 271ms/step - loss: 0.0798 - mae: 0.1385 - val_loss: 0.0359 - val_mae: 0.1456\n",
      "Epoch 43/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 272ms/step - loss: 0.0794 - mae: 0.1386 - val_loss: 0.0355 - val_mae: 0.1442\n",
      "Epoch 44/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 282ms/step - loss: 0.0791 - mae: 0.1387 - val_loss: 0.0350 - val_mae: 0.1428\n",
      "Epoch 45/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 269ms/step - loss: 0.0787 - mae: 0.1388 - val_loss: 0.0346 - val_mae: 0.1414\n",
      "Epoch 46/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 317ms/step - loss: 0.0784 - mae: 0.1391 - val_loss: 0.0343 - val_mae: 0.1400\n",
      "Epoch 47/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 282ms/step - loss: 0.0781 - mae: 0.1394 - val_loss: 0.0339 - val_mae: 0.1386\n",
      "Epoch 48/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 276ms/step - loss: 0.0777 - mae: 0.1397 - val_loss: 0.0335 - val_mae: 0.1372\n",
      "Epoch 49/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 274ms/step - loss: 0.0774 - mae: 0.1400 - val_loss: 0.0331 - val_mae: 0.1358\n",
      "Epoch 50/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 270ms/step - loss: 0.0771 - mae: 0.1402 - val_loss: 0.0327 - val_mae: 0.1344\n",
      "Epoch 51/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 277ms/step - loss: 0.0768 - mae: 0.1405 - val_loss: 0.0324 - val_mae: 0.1330\n",
      "Epoch 52/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 279ms/step - loss: 0.0765 - mae: 0.1408 - val_loss: 0.0320 - val_mae: 0.1317\n",
      "Epoch 53/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 274ms/step - loss: 0.0762 - mae: 0.1411 - val_loss: 0.0316 - val_mae: 0.1303\n",
      "Epoch 54/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 267ms/step - loss: 0.0759 - mae: 0.1413 - val_loss: 0.0313 - val_mae: 0.1289\n",
      "Epoch 55/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 271ms/step - loss: 0.0756 - mae: 0.1416 - val_loss: 0.0309 - val_mae: 0.1276\n",
      "Epoch 56/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 277ms/step - loss: 0.0753 - mae: 0.1419 - val_loss: 0.0306 - val_mae: 0.1262\n",
      "Epoch 57/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 275ms/step - loss: 0.0750 - mae: 0.1422 - val_loss: 0.0303 - val_mae: 0.1255\n",
      "Epoch 58/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 277ms/step - loss: 0.0748 - mae: 0.1424 - val_loss: 0.0299 - val_mae: 0.1248\n",
      "Epoch 59/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 274ms/step - loss: 0.0745 - mae: 0.1427 - val_loss: 0.0296 - val_mae: 0.1242\n",
      "Epoch 60/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 329ms/step - loss: 0.0742 - mae: 0.1430 - val_loss: 0.0293 - val_mae: 0.1235\n",
      "Epoch 61/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 272ms/step - loss: 0.0740 - mae: 0.1432 - val_loss: 0.0290 - val_mae: 0.1229\n",
      "Epoch 62/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 270ms/step - loss: 0.0737 - mae: 0.1435 - val_loss: 0.0287 - val_mae: 0.1222\n",
      "Epoch 63/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 270ms/step - loss: 0.0735 - mae: 0.1437 - val_loss: 0.0284 - val_mae: 0.1216\n",
      "Epoch 64/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 270ms/step - loss: 0.0732 - mae: 0.1440 - val_loss: 0.0281 - val_mae: 0.1209\n",
      "Epoch 65/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 273ms/step - loss: 0.0730 - mae: 0.1443 - val_loss: 0.0278 - val_mae: 0.1203\n",
      "Epoch 66/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 307ms/step - loss: 0.0727 - mae: 0.1445 - val_loss: 0.0275 - val_mae: 0.1196\n",
      "Epoch 67/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 273ms/step - loss: 0.0725 - mae: 0.1448 - val_loss: 0.0272 - val_mae: 0.1190\n",
      "Epoch 68/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 268ms/step - loss: 0.0723 - mae: 0.1450 - val_loss: 0.0269 - val_mae: 0.1184\n",
      "Epoch 69/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 286ms/step - loss: 0.0720 - mae: 0.1453 - val_loss: 0.0266 - val_mae: 0.1178\n",
      "Epoch 70/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 293ms/step - loss: 0.0718 - mae: 0.1455 - val_loss: 0.0264 - val_mae: 0.1176\n",
      "Epoch 71/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 275ms/step - loss: 0.0716 - mae: 0.1458 - val_loss: 0.0261 - val_mae: 0.1176\n",
      "Epoch 72/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 343ms/step - loss: 0.0714 - mae: 0.1460 - val_loss: 0.0258 - val_mae: 0.1176\n",
      "Epoch 73/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 309ms/step - loss: 0.0712 - mae: 0.1463 - val_loss: 0.0256 - val_mae: 0.1176\n",
      "Epoch 74/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 301ms/step - loss: 0.0710 - mae: 0.1465 - val_loss: 0.0253 - val_mae: 0.1176\n",
      "Epoch 75/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 284ms/step - loss: 0.0708 - mae: 0.1468 - val_loss: 0.0251 - val_mae: 0.1176\n",
      "Epoch 76/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 264ms/step - loss: 0.0706 - mae: 0.1470 - val_loss: 0.0248 - val_mae: 0.1176\n",
      "Epoch 77/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 270ms/step - loss: 0.0704 - mae: 0.1472 - val_loss: 0.0246 - val_mae: 0.1176\n",
      "Epoch 78/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 275ms/step - loss: 0.0702 - mae: 0.1475 - val_loss: 0.0244 - val_mae: 0.1176\n",
      "Epoch 79/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 282ms/step - loss: 0.0700 - mae: 0.1477 - val_loss: 0.0241 - val_mae: 0.1176\n",
      "Epoch 80/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 276ms/step - loss: 0.0699 - mae: 0.1480 - val_loss: 0.0239 - val_mae: 0.1176\n",
      "Epoch 81/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 276ms/step - loss: 0.0697 - mae: 0.1482 - val_loss: 0.0237 - val_mae: 0.1176\n",
      "Epoch 82/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 298ms/step - loss: 0.0695 - mae: 0.1484 - val_loss: 0.0235 - val_mae: 0.1176\n",
      "Epoch 83/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 283ms/step - loss: 0.0694 - mae: 0.1486 - val_loss: 0.0232 - val_mae: 0.1176\n",
      "Epoch 84/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 274ms/step - loss: 0.0692 - mae: 0.1489 - val_loss: 0.0230 - val_mae: 0.1176\n",
      "Epoch 85/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 266ms/step - loss: 0.0690 - mae: 0.1491 - val_loss: 0.0228 - val_mae: 0.1176\n",
      "Epoch 86/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 281ms/step - loss: 0.0689 - mae: 0.1493 - val_loss: 0.0226 - val_mae: 0.1176\n",
      "Epoch 87/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 266ms/step - loss: 0.0687 - mae: 0.1495 - val_loss: 0.0224 - val_mae: 0.1176\n",
      "Epoch 88/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 265ms/step - loss: 0.0686 - mae: 0.1498 - val_loss: 0.0222 - val_mae: 0.1176\n",
      "Epoch 89/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 276ms/step - loss: 0.0685 - mae: 0.1500 - val_loss: 0.0221 - val_mae: 0.1176\n",
      "Epoch 90/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278ms/step - loss: 0.0683 - mae: 0.1502 - val_loss: 0.0219 - val_mae: 0.1176\n",
      "Epoch 91/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 274ms/step - loss: 0.0682 - mae: 0.1504 - val_loss: 0.0217 - val_mae: 0.1176\n",
      "Epoch 92/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 297ms/step - loss: 0.0680 - mae: 0.1506 - val_loss: 0.0215 - val_mae: 0.1176\n",
      "Epoch 93/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 298ms/step - loss: 0.0679 - mae: 0.1508 - val_loss: 0.0214 - val_mae: 0.1176\n",
      "Epoch 94/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318ms/step - loss: 0.0678 - mae: 0.1510 - val_loss: 0.0212 - val_mae: 0.1176\n",
      "Epoch 95/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 300ms/step - loss: 0.0677 - mae: 0.1513 - val_loss: 0.0210 - val_mae: 0.1176\n",
      "Epoch 96/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 354ms/step - loss: 0.0676 - mae: 0.1516 - val_loss: 0.0209 - val_mae: 0.1176\n",
      "Epoch 97/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 330ms/step - loss: 0.0674 - mae: 0.1519 - val_loss: 0.0207 - val_mae: 0.1176\n",
      "Epoch 98/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 361ms/step - loss: 0.0673 - mae: 0.1523 - val_loss: 0.0205 - val_mae: 0.1176\n",
      "Epoch 99/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 312ms/step - loss: 0.0672 - mae: 0.1526 - val_loss: 0.0204 - val_mae: 0.1176\n",
      "Epoch 100/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 382ms/step - loss: 0.0671 - mae: 0.1529 - val_loss: 0.0203 - val_mae: 0.1176\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 456ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Done with capsicum_Bellary_daily.csv | MAE=306.08, RMSE=502.1, R2=-0.0603, MAPE=12.65%, ACC=87.35%\n",
      "\n",
      "ğŸš€ Processing: capsicum_Chikmagalur_daily.csv\n",
      "Epoch 1/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 26ms/step - loss: 0.0989 - mae: 0.2775 - val_loss: 0.1066 - val_mae: 0.2721\n",
      "Epoch 2/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0201 - mae: 0.1052 - val_loss: 0.0779 - val_mae: 0.2382\n",
      "Epoch 3/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0172 - mae: 0.0986 - val_loss: 0.0776 - val_mae: 0.2377\n",
      "Epoch 4/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0170 - mae: 0.0981 - val_loss: 0.0750 - val_mae: 0.2346\n",
      "Epoch 5/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0177 - mae: 0.1006 - val_loss: 0.0768 - val_mae: 0.2368\n",
      "Epoch 6/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - loss: 0.0177 - mae: 0.1021 - val_loss: 0.0777 - val_mae: 0.2379\n",
      "Epoch 7/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0174 - mae: 0.0996 - val_loss: 0.0771 - val_mae: 0.2372\n",
      "Epoch 8/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0174 - mae: 0.0999 - val_loss: 0.0751 - val_mae: 0.2348\n",
      "Epoch 9/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0170 - mae: 0.0998 - val_loss: 0.0770 - val_mae: 0.2370\n",
      "Epoch 10/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0173 - mae: 0.1001 - val_loss: 0.0762 - val_mae: 0.2361\n",
      "Epoch 11/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - loss: 0.0178 - mae: 0.1024 - val_loss: 0.0771 - val_mae: 0.2372\n",
      "Epoch 12/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0174 - mae: 0.1004 - val_loss: 0.0762 - val_mae: 0.2361\n",
      "Epoch 13/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0178 - mae: 0.1014 - val_loss: 0.0786 - val_mae: 0.2390\n",
      "Epoch 14/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 0.0176 - mae: 0.1007 - val_loss: 0.0780 - val_mae: 0.2382\n",
      "Epoch 15/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0179 - mae: 0.1013 - val_loss: 0.0751 - val_mae: 0.2347\n",
      "Epoch 16/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0178 - mae: 0.1021 - val_loss: 0.0751 - val_mae: 0.2348\n",
      "Epoch 17/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0188 - mae: 0.1053 - val_loss: 0.0739 - val_mae: 0.2333\n",
      "Epoch 18/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 0.0184 - mae: 0.1042 - val_loss: 0.0776 - val_mae: 0.2378\n",
      "Epoch 19/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0186 - mae: 0.1045 - val_loss: 0.0776 - val_mae: 0.2377\n",
      "Epoch 20/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0184 - mae: 0.1038 - val_loss: 0.0770 - val_mae: 0.2371\n",
      "Epoch 21/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0174 - mae: 0.1010 - val_loss: 0.0775 - val_mae: 0.2377\n",
      "Epoch 22/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0185 - mae: 0.1029 - val_loss: 0.0800 - val_mae: 0.2407\n",
      "Epoch 23/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - loss: 0.0183 - mae: 0.1033 - val_loss: 0.0784 - val_mae: 0.2388\n",
      "Epoch 24/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0178 - mae: 0.1018 - val_loss: 0.0801 - val_mae: 0.2409\n",
      "Epoch 25/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0175 - mae: 0.1016 - val_loss: 0.0794 - val_mae: 0.2400\n",
      "Epoch 26/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0177 - mae: 0.1001 - val_loss: 0.0753 - val_mae: 0.2349\n",
      "Epoch 27/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0181 - mae: 0.1033 - val_loss: 0.0766 - val_mae: 0.2366\n",
      "Epoch 28/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0174 - mae: 0.0988 - val_loss: 0.0775 - val_mae: 0.2377\n",
      "Epoch 29/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.0182 - mae: 0.1039 - val_loss: 0.0753 - val_mae: 0.2350\n",
      "Epoch 30/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - loss: 0.0179 - mae: 0.1025 - val_loss: 0.0794 - val_mae: 0.2400\n",
      "Epoch 31/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0187 - mae: 0.1039 - val_loss: 0.0786 - val_mae: 0.2390\n",
      "Epoch 32/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0182 - mae: 0.1024 - val_loss: 0.0798 - val_mae: 0.2404\n",
      "Epoch 33/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - loss: 0.0176 - mae: 0.1009 - val_loss: 0.0793 - val_mae: 0.2398\n",
      "Epoch 34/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0180 - mae: 0.1026 - val_loss: 0.0781 - val_mae: 0.2384\n",
      "Epoch 35/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - loss: 0.0170 - mae: 0.0979 - val_loss: 0.0777 - val_mae: 0.2380\n",
      "Epoch 36/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0177 - mae: 0.1012 - val_loss: 0.0788 - val_mae: 0.2393\n",
      "Epoch 37/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0178 - mae: 0.1004 - val_loss: 0.0775 - val_mae: 0.2377\n",
      "Epoch 38/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 0.0170 - mae: 0.0985 - val_loss: 0.0773 - val_mae: 0.2374\n",
      "Epoch 39/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0177 - mae: 0.1011 - val_loss: 0.0792 - val_mae: 0.2398\n",
      "Epoch 40/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - loss: 0.0176 - mae: 0.1001 - val_loss: 0.0783 - val_mae: 0.2387\n",
      "Epoch 41/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0175 - mae: 0.1005 - val_loss: 0.0746 - val_mae: 0.2341\n",
      "Epoch 42/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0171 - mae: 0.0981 - val_loss: 0.0754 - val_mae: 0.2351\n",
      "Epoch 43/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0177 - mae: 0.1009 - val_loss: 0.0774 - val_mae: 0.2375\n",
      "Epoch 44/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0171 - mae: 0.0994 - val_loss: 0.0760 - val_mae: 0.2359\n",
      "Epoch 45/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0178 - mae: 0.1007 - val_loss: 0.0769 - val_mae: 0.2370\n",
      "Epoch 46/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0177 - mae: 0.1015 - val_loss: 0.0752 - val_mae: 0.2349\n",
      "Epoch 47/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0184 - mae: 0.1036 - val_loss: 0.0794 - val_mae: 0.2400\n",
      "Epoch 48/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0169 - mae: 0.0979 - val_loss: 0.0785 - val_mae: 0.2388\n",
      "Epoch 49/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0179 - mae: 0.1011 - val_loss: 0.0782 - val_mae: 0.2385\n",
      "Epoch 50/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0169 - mae: 0.0972 - val_loss: 0.0768 - val_mae: 0.2368\n",
      "Epoch 51/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0173 - mae: 0.1003 - val_loss: 0.0780 - val_mae: 0.2383\n",
      "Epoch 52/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0178 - mae: 0.1021 - val_loss: 0.0784 - val_mae: 0.2387\n",
      "Epoch 53/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0173 - mae: 0.0995 - val_loss: 0.0762 - val_mae: 0.2361\n",
      "Epoch 54/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - loss: 0.0173 - mae: 0.0995 - val_loss: 0.0772 - val_mae: 0.2373\n",
      "Epoch 55/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - loss: 0.0180 - mae: 0.1021 - val_loss: 0.0755 - val_mae: 0.2353\n",
      "Epoch 56/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 0.0178 - mae: 0.1019 - val_loss: 0.0746 - val_mae: 0.2341\n",
      "Epoch 57/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 0.0180 - mae: 0.1027 - val_loss: 0.0764 - val_mae: 0.2363\n",
      "Epoch 58/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0184 - mae: 0.1037 - val_loss: 0.0752 - val_mae: 0.2348\n",
      "Epoch 59/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 0.0175 - mae: 0.0996 - val_loss: 0.0791 - val_mae: 0.2396\n",
      "Epoch 60/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - loss: 0.0178 - mae: 0.1013 - val_loss: 0.0774 - val_mae: 0.2376\n",
      "Epoch 61/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 0.0174 - mae: 0.1011 - val_loss: 0.0792 - val_mae: 0.2397\n",
      "Epoch 62/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 25ms/step - loss: 0.0183 - mae: 0.1017 - val_loss: 0.0788 - val_mae: 0.2392\n",
      "Epoch 63/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 0.0176 - mae: 0.0998 - val_loss: 0.0767 - val_mae: 0.2367\n",
      "Epoch 64/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 0.0169 - mae: 0.0973 - val_loss: 0.0751 - val_mae: 0.2348\n",
      "Epoch 65/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 0.0171 - mae: 0.0996 - val_loss: 0.0782 - val_mae: 0.2385\n",
      "Epoch 66/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0182 - mae: 0.1020 - val_loss: 0.0778 - val_mae: 0.2380\n",
      "Epoch 67/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 0.0180 - mae: 0.1022 - val_loss: 0.0752 - val_mae: 0.2348\n",
      "Epoch 68/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0182 - mae: 0.1030 - val_loss: 0.0748 - val_mae: 0.2344\n",
      "Epoch 69/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - loss: 0.0182 - mae: 0.1032 - val_loss: 0.0746 - val_mae: 0.2342\n",
      "Epoch 70/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 0.0174 - mae: 0.1007 - val_loss: 0.0757 - val_mae: 0.2355\n",
      "Epoch 71/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0185 - mae: 0.1040 - val_loss: 0.0809 - val_mae: 0.2418\n",
      "Epoch 72/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 0.0181 - mae: 0.1020 - val_loss: 0.0745 - val_mae: 0.2340\n",
      "Epoch 73/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - loss: 0.0180 - mae: 0.1042 - val_loss: 0.0798 - val_mae: 0.2405\n",
      "Epoch 74/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0177 - mae: 0.1000 - val_loss: 0.0752 - val_mae: 0.2349\n",
      "Epoch 75/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - loss: 0.0189 - mae: 0.1046 - val_loss: 0.0740 - val_mae: 0.2334\n",
      "Epoch 76/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0173 - mae: 0.0999 - val_loss: 0.0771 - val_mae: 0.2372\n",
      "Epoch 77/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0180 - mae: 0.1033 - val_loss: 0.0768 - val_mae: 0.2368\n",
      "Epoch 78/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0186 - mae: 0.1044 - val_loss: 0.0758 - val_mae: 0.2356\n",
      "Epoch 79/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0179 - mae: 0.1013 - val_loss: 0.0747 - val_mae: 0.2342\n",
      "Epoch 80/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0182 - mae: 0.1035 - val_loss: 0.0781 - val_mae: 0.2384\n",
      "Epoch 81/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0183 - mae: 0.1034 - val_loss: 0.0773 - val_mae: 0.2374\n",
      "Epoch 82/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0183 - mae: 0.1027 - val_loss: 0.0749 - val_mae: 0.2345\n",
      "Epoch 83/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0178 - mae: 0.1029 - val_loss: 0.0757 - val_mae: 0.2355\n",
      "Epoch 84/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0174 - mae: 0.0999 - val_loss: 0.0750 - val_mae: 0.2347\n",
      "Epoch 85/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 0.0168 - mae: 0.0985 - val_loss: 0.0753 - val_mae: 0.2351\n",
      "Epoch 86/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 0.0181 - mae: 0.1023 - val_loss: 0.0770 - val_mae: 0.2371\n",
      "Epoch 87/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 0.0171 - mae: 0.0986 - val_loss: 0.0758 - val_mae: 0.2356\n",
      "Epoch 88/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0187 - mae: 0.1063 - val_loss: 0.0750 - val_mae: 0.2347\n",
      "Epoch 89/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 0.0175 - mae: 0.1012 - val_loss: 0.0780 - val_mae: 0.2383\n",
      "Epoch 90/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 0.0176 - mae: 0.1005 - val_loss: 0.0775 - val_mae: 0.2376\n",
      "Epoch 91/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 0.0169 - mae: 0.0984 - val_loss: 0.0793 - val_mae: 0.2398\n",
      "Epoch 92/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0180 - mae: 0.1015 - val_loss: 0.0755 - val_mae: 0.2353\n",
      "Epoch 93/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 0.0176 - mae: 0.1020 - val_loss: 0.0747 - val_mae: 0.2343\n",
      "Epoch 94/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0169 - mae: 0.0980 - val_loss: 0.0745 - val_mae: 0.2340\n",
      "Epoch 95/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0171 - mae: 0.0993 - val_loss: 0.0747 - val_mae: 0.2343\n",
      "Epoch 96/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 0.0179 - mae: 0.1017 - val_loss: 0.0739 - val_mae: 0.2333\n",
      "Epoch 97/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0193 - mae: 0.1081 - val_loss: 0.0801 - val_mae: 0.2408\n",
      "Epoch 98/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0177 - mae: 0.1007 - val_loss: 0.0772 - val_mae: 0.2373\n",
      "Epoch 99/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0176 - mae: 0.1017 - val_loss: 0.0796 - val_mae: 0.2402\n",
      "Epoch 100/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0176 - mae: 0.0999 - val_loss: 0.0793 - val_mae: 0.2399\n",
      "\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Done with capsicum_Chikmagalur_daily.csv | MAE=447.57, RMSE=606.64, R2=-0.0528, MAPE=24.46%, ACC=75.54%\n",
      "\n",
      "ğŸš€ Processing: capsicum_Davangere_daily.csv\n",
      "Epoch 1/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 29ms/step - loss: 0.0154 - mae: 0.0956 - val_loss: 0.0482 - val_mae: 0.1805\n",
      "Epoch 2/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 0.0067 - mae: 0.0595 - val_loss: 0.0461 - val_mae: 0.1747\n",
      "Epoch 3/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 0.0065 - mae: 0.0595 - val_loss: 0.0463 - val_mae: 0.1752\n",
      "Epoch 4/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 0.0073 - mae: 0.0635 - val_loss: 0.0462 - val_mae: 0.1751\n",
      "Epoch 5/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - loss: 0.0073 - mae: 0.0629 - val_loss: 0.0477 - val_mae: 0.1793\n",
      "Epoch 6/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - loss: 0.0065 - mae: 0.0597 - val_loss: 0.0458 - val_mae: 0.1740\n",
      "Epoch 7/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 0.0075 - mae: 0.0632 - val_loss: 0.0477 - val_mae: 0.1793\n",
      "Epoch 8/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 0.0072 - mae: 0.0636 - val_loss: 0.0469 - val_mae: 0.1770\n",
      "Epoch 9/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0063 - mae: 0.0588 - val_loss: 0.0481 - val_mae: 0.1804\n",
      "Epoch 10/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - loss: 0.0069 - mae: 0.0604 - val_loss: 0.0463 - val_mae: 0.1753\n",
      "Epoch 11/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0073 - mae: 0.0628 - val_loss: 0.0479 - val_mae: 0.1797\n",
      "Epoch 12/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - loss: 0.0063 - mae: 0.0589 - val_loss: 0.0486 - val_mae: 0.1817\n",
      "Epoch 13/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - loss: 0.0061 - mae: 0.0557 - val_loss: 0.0438 - val_mae: 0.1681\n",
      "Epoch 14/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - loss: 0.0067 - mae: 0.0609 - val_loss: 0.0454 - val_mae: 0.1729\n",
      "Epoch 15/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - loss: 0.0074 - mae: 0.0622 - val_loss: 0.0474 - val_mae: 0.1783\n",
      "Epoch 16/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 0.0067 - mae: 0.0599 - val_loss: 0.0456 - val_mae: 0.1732\n",
      "Epoch 17/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 0.0067 - mae: 0.0605 - val_loss: 0.0455 - val_mae: 0.1732\n",
      "Epoch 18/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - loss: 0.0067 - mae: 0.0604 - val_loss: 0.0477 - val_mae: 0.1793\n",
      "Epoch 19/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 0.0067 - mae: 0.0591 - val_loss: 0.0462 - val_mae: 0.1749\n",
      "Epoch 20/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 0.0065 - mae: 0.0601 - val_loss: 0.0447 - val_mae: 0.1707\n",
      "Epoch 21/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - loss: 0.0070 - mae: 0.0622 - val_loss: 0.0481 - val_mae: 0.1801\n",
      "Epoch 22/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - loss: 0.0069 - mae: 0.0615 - val_loss: 0.0481 - val_mae: 0.1802\n",
      "Epoch 23/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - loss: 0.0069 - mae: 0.0611 - val_loss: 0.0457 - val_mae: 0.1737\n",
      "Epoch 24/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - loss: 0.0070 - mae: 0.0604 - val_loss: 0.0485 - val_mae: 0.1815\n",
      "Epoch 25/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - loss: 0.0068 - mae: 0.0602 - val_loss: 0.0461 - val_mae: 0.1748\n",
      "Epoch 26/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 0.0067 - mae: 0.0603 - val_loss: 0.0475 - val_mae: 0.1787\n",
      "Epoch 27/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - loss: 0.0069 - mae: 0.0611 - val_loss: 0.0493 - val_mae: 0.1834\n",
      "Epoch 28/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - loss: 0.0076 - mae: 0.0628 - val_loss: 0.0474 - val_mae: 0.1783\n",
      "Epoch 29/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - loss: 0.0070 - mae: 0.0604 - val_loss: 0.0458 - val_mae: 0.1740\n",
      "Epoch 30/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 0.0066 - mae: 0.0601 - val_loss: 0.0446 - val_mae: 0.1706\n",
      "Epoch 31/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 0.0075 - mae: 0.0638 - val_loss: 0.0465 - val_mae: 0.1760\n",
      "Epoch 32/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 0.0067 - mae: 0.0593 - val_loss: 0.0463 - val_mae: 0.1753\n",
      "Epoch 33/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 0.0063 - mae: 0.0592 - val_loss: 0.0473 - val_mae: 0.1781\n",
      "Epoch 34/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - loss: 0.0067 - mae: 0.0594 - val_loss: 0.0463 - val_mae: 0.1752\n",
      "Epoch 35/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 0.0070 - mae: 0.0620 - val_loss: 0.0460 - val_mae: 0.1744\n",
      "Epoch 36/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 0.0062 - mae: 0.0586 - val_loss: 0.0479 - val_mae: 0.1797\n",
      "Epoch 37/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - loss: 0.0071 - mae: 0.0611 - val_loss: 0.0490 - val_mae: 0.1827\n",
      "Epoch 38/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - loss: 0.0064 - mae: 0.0593 - val_loss: 0.0471 - val_mae: 0.1775\n",
      "Epoch 39/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - loss: 0.0063 - mae: 0.0583 - val_loss: 0.0452 - val_mae: 0.1722\n",
      "Epoch 40/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - loss: 0.0068 - mae: 0.0603 - val_loss: 0.0454 - val_mae: 0.1728\n",
      "Epoch 41/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - loss: 0.0067 - mae: 0.0599 - val_loss: 0.0470 - val_mae: 0.1772\n",
      "Epoch 42/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 0.0062 - mae: 0.0582 - val_loss: 0.0452 - val_mae: 0.1722\n",
      "Epoch 43/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - loss: 0.0069 - mae: 0.0608 - val_loss: 0.0454 - val_mae: 0.1728\n",
      "Epoch 44/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - loss: 0.0066 - mae: 0.0603 - val_loss: 0.0453 - val_mae: 0.1725\n",
      "Epoch 45/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - loss: 0.0068 - mae: 0.0617 - val_loss: 0.0486 - val_mae: 0.1816\n",
      "Epoch 46/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - loss: 0.0069 - mae: 0.0609 - val_loss: 0.0478 - val_mae: 0.1795\n",
      "Epoch 47/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - loss: 0.0070 - mae: 0.0605 - val_loss: 0.0461 - val_mae: 0.1748\n",
      "Epoch 48/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 0.0066 - mae: 0.0601 - val_loss: 0.0458 - val_mae: 0.1739\n",
      "Epoch 49/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - loss: 0.0074 - mae: 0.0643 - val_loss: 0.0475 - val_mae: 0.1786\n",
      "Epoch 50/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - loss: 0.0067 - mae: 0.0598 - val_loss: 0.0470 - val_mae: 0.1773\n",
      "Epoch 51/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - loss: 0.0066 - mae: 0.0596 - val_loss: 0.0471 - val_mae: 0.1776\n",
      "Epoch 52/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - loss: 0.0073 - mae: 0.0626 - val_loss: 0.0470 - val_mae: 0.1773\n",
      "Epoch 53/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - loss: 0.0068 - mae: 0.0603 - val_loss: 0.0474 - val_mae: 0.1783\n",
      "Epoch 54/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 0.0060 - mae: 0.0579 - val_loss: 0.0467 - val_mae: 0.1765\n",
      "Epoch 55/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - loss: 0.0061 - mae: 0.0581 - val_loss: 0.0437 - val_mae: 0.1679\n",
      "Epoch 56/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 0.0069 - mae: 0.0614 - val_loss: 0.0502 - val_mae: 0.1860\n",
      "Epoch 57/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - loss: 0.0066 - mae: 0.0595 - val_loss: 0.0465 - val_mae: 0.1760\n",
      "Epoch 58/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 0.0063 - mae: 0.0592 - val_loss: 0.0469 - val_mae: 0.1770\n",
      "Epoch 59/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - loss: 0.0063 - mae: 0.0589 - val_loss: 0.0467 - val_mae: 0.1764\n",
      "Epoch 60/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 0.0062 - mae: 0.0587 - val_loss: 0.0481 - val_mae: 0.1804\n",
      "Epoch 61/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - loss: 0.0069 - mae: 0.0614 - val_loss: 0.0467 - val_mae: 0.1764\n",
      "Epoch 62/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 0.0066 - mae: 0.0607 - val_loss: 0.0466 - val_mae: 0.1761\n",
      "Epoch 63/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - loss: 0.0068 - mae: 0.0608 - val_loss: 0.0467 - val_mae: 0.1763\n",
      "Epoch 64/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 0.0068 - mae: 0.0607 - val_loss: 0.0485 - val_mae: 0.1813\n",
      "Epoch 65/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - loss: 0.0064 - mae: 0.0601 - val_loss: 0.0486 - val_mae: 0.1817\n",
      "Epoch 66/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - loss: 0.0074 - mae: 0.0622 - val_loss: 0.0481 - val_mae: 0.1803\n",
      "Epoch 67/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - loss: 0.0069 - mae: 0.0623 - val_loss: 0.0478 - val_mae: 0.1794\n",
      "Epoch 68/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 0.0068 - mae: 0.0617 - val_loss: 0.0479 - val_mae: 0.1798\n",
      "Epoch 69/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 0.0065 - mae: 0.0597 - val_loss: 0.0497 - val_mae: 0.1844\n",
      "Epoch 70/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 0.0070 - mae: 0.0610 - val_loss: 0.0464 - val_mae: 0.1757\n",
      "Epoch 71/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - loss: 0.0073 - mae: 0.0632 - val_loss: 0.0484 - val_mae: 0.1810\n",
      "Epoch 72/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - loss: 0.0065 - mae: 0.0586 - val_loss: 0.0466 - val_mae: 0.1760\n",
      "Epoch 73/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - loss: 0.0068 - mae: 0.0601 - val_loss: 0.0481 - val_mae: 0.1803\n",
      "Epoch 74/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 0.0068 - mae: 0.0592 - val_loss: 0.0489 - val_mae: 0.1824\n",
      "Epoch 75/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - loss: 0.0078 - mae: 0.0634 - val_loss: 0.0461 - val_mae: 0.1749\n",
      "Epoch 76/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 0.0065 - mae: 0.0589 - val_loss: 0.0467 - val_mae: 0.1763\n",
      "Epoch 77/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 0.0059 - mae: 0.0575 - val_loss: 0.0469 - val_mae: 0.1771\n",
      "Epoch 78/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - loss: 0.0062 - mae: 0.0585 - val_loss: 0.0462 - val_mae: 0.1750\n",
      "Epoch 79/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - loss: 0.0069 - mae: 0.0607 - val_loss: 0.0462 - val_mae: 0.1750\n",
      "Epoch 80/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - loss: 0.0067 - mae: 0.0618 - val_loss: 0.0487 - val_mae: 0.1818\n",
      "Epoch 81/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - loss: 0.0061 - mae: 0.0578 - val_loss: 0.0455 - val_mae: 0.1732\n",
      "Epoch 82/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - loss: 0.0067 - mae: 0.0614 - val_loss: 0.0470 - val_mae: 0.1772\n",
      "Epoch 83/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 0.0074 - mae: 0.0623 - val_loss: 0.0483 - val_mae: 0.1807\n",
      "Epoch 84/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 0.0070 - mae: 0.0611 - val_loss: 0.0476 - val_mae: 0.1789\n",
      "Epoch 85/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - loss: 0.0063 - mae: 0.0583 - val_loss: 0.0444 - val_mae: 0.1700\n",
      "Epoch 86/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 0.0068 - mae: 0.0604 - val_loss: 0.0461 - val_mae: 0.1747\n",
      "Epoch 87/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 0.0065 - mae: 0.0594 - val_loss: 0.0473 - val_mae: 0.1781\n",
      "Epoch 88/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 0.0067 - mae: 0.0601 - val_loss: 0.0465 - val_mae: 0.1759\n",
      "Epoch 89/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - loss: 0.0065 - mae: 0.0584 - val_loss: 0.0485 - val_mae: 0.1815\n",
      "Epoch 90/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 0.0067 - mae: 0.0610 - val_loss: 0.0485 - val_mae: 0.1814\n",
      "Epoch 91/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 0.0073 - mae: 0.0631 - val_loss: 0.0492 - val_mae: 0.1831\n",
      "Epoch 92/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - loss: 0.0067 - mae: 0.0592 - val_loss: 0.0455 - val_mae: 0.1730\n",
      "Epoch 93/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 0.0065 - mae: 0.0598 - val_loss: 0.0429 - val_mae: 0.1657\n",
      "Epoch 94/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - loss: 0.0065 - mae: 0.0603 - val_loss: 0.0474 - val_mae: 0.1783\n",
      "Epoch 95/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - loss: 0.0070 - mae: 0.0619 - val_loss: 0.0478 - val_mae: 0.1794\n",
      "Epoch 96/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - loss: 0.0062 - mae: 0.0580 - val_loss: 0.0482 - val_mae: 0.1806\n",
      "Epoch 97/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - loss: 0.0062 - mae: 0.0577 - val_loss: 0.0485 - val_mae: 0.1812\n",
      "Epoch 98/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 0.0066 - mae: 0.0591 - val_loss: 0.0461 - val_mae: 0.1748\n",
      "Epoch 99/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - loss: 0.0066 - mae: 0.0612 - val_loss: 0.0494 - val_mae: 0.1838\n",
      "Epoch 100/100\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 0.0066 - mae: 0.0595 - val_loss: 0.0478 - val_mae: 0.1794\n",
      "\u001b[1m55/55\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Done with capsicum_Davangere_daily.csv | MAE=1306.84, RMSE=1907.89, R2=-0.1098, MAPE=37.91%, ACC=62.09%\n",
      "\n",
      "ğŸš€ Processing: capsicum_Dharwad_daily.csv\n",
      "Epoch 1/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 594ms/step - loss: 0.2403 - mae: 0.3789 - val_loss: 0.2685 - val_mae: 0.4021\n",
      "Epoch 2/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 0.2667 - mae: 0.3852 - val_loss: 0.2660 - val_mae: 0.3989\n",
      "Epoch 3/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - loss: 0.2602 - mae: 0.3843 - val_loss: 0.2635 - val_mae: 0.3958\n",
      "Epoch 4/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - loss: 0.2739 - mae: 0.4035 - val_loss: 0.2611 - val_mae: 0.3926\n",
      "Epoch 5/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 0.2512 - mae: 0.3704 - val_loss: 0.2586 - val_mae: 0.3899\n",
      "Epoch 6/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.2082 - mae: 0.3423 - val_loss: 0.2562 - val_mae: 0.3878\n",
      "Epoch 7/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - loss: 0.2309 - mae: 0.3443 - val_loss: 0.2537 - val_mae: 0.3856\n",
      "Epoch 8/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 0.2512 - mae: 0.3709 - val_loss: 0.2513 - val_mae: 0.3833\n",
      "Epoch 9/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 0.2448 - mae: 0.3723 - val_loss: 0.2488 - val_mae: 0.3811\n",
      "Epoch 10/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.2480 - mae: 0.3690 - val_loss: 0.2464 - val_mae: 0.3788\n",
      "Epoch 11/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 116ms/step - loss: 0.2255 - mae: 0.3622 - val_loss: 0.2439 - val_mae: 0.3766\n",
      "Epoch 12/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 0.2416 - mae: 0.3635 - val_loss: 0.2415 - val_mae: 0.3743\n",
      "Epoch 13/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 0.2451 - mae: 0.3718 - val_loss: 0.2390 - val_mae: 0.3720\n",
      "Epoch 14/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - loss: 0.2344 - mae: 0.3548 - val_loss: 0.2366 - val_mae: 0.3696\n",
      "Epoch 15/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 0.2280 - mae: 0.3462 - val_loss: 0.2342 - val_mae: 0.3673\n",
      "Epoch 16/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 0.2273 - mae: 0.3447 - val_loss: 0.2318 - val_mae: 0.3650\n",
      "Epoch 17/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - loss: 0.2182 - mae: 0.3388 - val_loss: 0.2294 - val_mae: 0.3626\n",
      "Epoch 18/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - loss: 0.2350 - mae: 0.3542 - val_loss: 0.2270 - val_mae: 0.3602\n",
      "Epoch 19/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - loss: 0.1990 - mae: 0.3125 - val_loss: 0.2247 - val_mae: 0.3579\n",
      "Epoch 20/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - loss: 0.2565 - mae: 0.3827 - val_loss: 0.2223 - val_mae: 0.3554\n",
      "Epoch 21/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - loss: 0.2258 - mae: 0.3393 - val_loss: 0.2199 - val_mae: 0.3530\n",
      "Epoch 22/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - loss: 0.1872 - mae: 0.3151 - val_loss: 0.2176 - val_mae: 0.3506\n",
      "Epoch 23/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - loss: 0.2058 - mae: 0.3382 - val_loss: 0.2152 - val_mae: 0.3482\n",
      "Epoch 24/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.2303 - mae: 0.3396 - val_loss: 0.2129 - val_mae: 0.3457\n",
      "Epoch 25/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - loss: 0.2225 - mae: 0.3364 - val_loss: 0.2106 - val_mae: 0.3432\n",
      "Epoch 26/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - loss: 0.1975 - mae: 0.3299 - val_loss: 0.2083 - val_mae: 0.3411\n",
      "Epoch 27/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - loss: 0.2101 - mae: 0.3341 - val_loss: 0.2060 - val_mae: 0.3397\n",
      "Epoch 28/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - loss: 0.2235 - mae: 0.3437 - val_loss: 0.2037 - val_mae: 0.3383\n",
      "Epoch 29/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - loss: 0.2152 - mae: 0.3303 - val_loss: 0.2015 - val_mae: 0.3369\n",
      "Epoch 30/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - loss: 0.2311 - mae: 0.3463 - val_loss: 0.1992 - val_mae: 0.3355\n",
      "Epoch 31/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 0.1965 - mae: 0.3172 - val_loss: 0.1970 - val_mae: 0.3340\n",
      "Epoch 32/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - loss: 0.1997 - mae: 0.3174 - val_loss: 0.1948 - val_mae: 0.3326\n",
      "Epoch 33/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - loss: 0.1984 - mae: 0.3135 - val_loss: 0.1926 - val_mae: 0.3312\n",
      "Epoch 34/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - loss: 0.1762 - mae: 0.2882 - val_loss: 0.1905 - val_mae: 0.3298\n",
      "Epoch 35/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - loss: 0.1983 - mae: 0.3152 - val_loss: 0.1883 - val_mae: 0.3284\n",
      "Epoch 36/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 108ms/step - loss: 0.1489 - mae: 0.2641 - val_loss: 0.1863 - val_mae: 0.3269\n",
      "Epoch 37/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - loss: 0.1493 - mae: 0.2660 - val_loss: 0.1842 - val_mae: 0.3255\n",
      "Epoch 38/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - loss: 0.2163 - mae: 0.3418 - val_loss: 0.1821 - val_mae: 0.3240\n",
      "Epoch 39/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - loss: 0.1657 - mae: 0.2889 - val_loss: 0.1800 - val_mae: 0.3226\n",
      "Epoch 40/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - loss: 0.1934 - mae: 0.3163 - val_loss: 0.1780 - val_mae: 0.3211\n",
      "Epoch 41/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - loss: 0.1799 - mae: 0.3033 - val_loss: 0.1759 - val_mae: 0.3196\n",
      "Epoch 42/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - loss: 0.1775 - mae: 0.3009 - val_loss: 0.1740 - val_mae: 0.3182\n",
      "Epoch 43/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - loss: 0.1918 - mae: 0.3145 - val_loss: 0.1720 - val_mae: 0.3167\n",
      "Epoch 44/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - loss: 0.1529 - mae: 0.2676 - val_loss: 0.1701 - val_mae: 0.3153\n",
      "Epoch 45/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - loss: 0.1831 - mae: 0.3063 - val_loss: 0.1682 - val_mae: 0.3138\n",
      "Epoch 46/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - loss: 0.1649 - mae: 0.2847 - val_loss: 0.1663 - val_mae: 0.3123\n",
      "Epoch 47/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - loss: 0.1932 - mae: 0.3173 - val_loss: 0.1644 - val_mae: 0.3109\n",
      "Epoch 48/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - loss: 0.1701 - mae: 0.2937 - val_loss: 0.1626 - val_mae: 0.3094\n",
      "Epoch 49/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - loss: 0.1767 - mae: 0.2997 - val_loss: 0.1608 - val_mae: 0.3079\n",
      "Epoch 50/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 0.1657 - mae: 0.2838 - val_loss: 0.1591 - val_mae: 0.3065\n",
      "Epoch 51/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - loss: 0.1510 - mae: 0.2764 - val_loss: 0.1574 - val_mae: 0.3050\n",
      "Epoch 52/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 0.1507 - mae: 0.2734 - val_loss: 0.1557 - val_mae: 0.3036\n",
      "Epoch 53/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 0.1655 - mae: 0.2939 - val_loss: 0.1540 - val_mae: 0.3021\n",
      "Epoch 54/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - loss: 0.1634 - mae: 0.2944 - val_loss: 0.1524 - val_mae: 0.3006\n",
      "Epoch 55/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - loss: 0.1612 - mae: 0.2871 - val_loss: 0.1508 - val_mae: 0.2992\n",
      "Epoch 56/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - loss: 0.1499 - mae: 0.2784 - val_loss: 0.1492 - val_mae: 0.2977\n",
      "Epoch 57/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - loss: 0.1215 - mae: 0.2509 - val_loss: 0.1477 - val_mae: 0.2963\n",
      "Epoch 58/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - loss: 0.1480 - mae: 0.2807 - val_loss: 0.1462 - val_mae: 0.2949\n",
      "Epoch 59/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - loss: 0.1548 - mae: 0.2865 - val_loss: 0.1448 - val_mae: 0.2935\n",
      "Epoch 60/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - loss: 0.1552 - mae: 0.2865 - val_loss: 0.1433 - val_mae: 0.2920\n",
      "Epoch 61/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - loss: 0.1630 - mae: 0.2975 - val_loss: 0.1419 - val_mae: 0.2906\n",
      "Epoch 62/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 0.1528 - mae: 0.2847 - val_loss: 0.1406 - val_mae: 0.2892\n",
      "Epoch 63/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - loss: 0.1587 - mae: 0.2899 - val_loss: 0.1392 - val_mae: 0.2881\n",
      "Epoch 64/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 0.1279 - mae: 0.2620 - val_loss: 0.1380 - val_mae: 0.2873\n",
      "Epoch 65/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 0.1312 - mae: 0.2684 - val_loss: 0.1367 - val_mae: 0.2865\n",
      "Epoch 66/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - loss: 0.1350 - mae: 0.2694 - val_loss: 0.1355 - val_mae: 0.2857\n",
      "Epoch 67/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - loss: 0.1420 - mae: 0.2763 - val_loss: 0.1343 - val_mae: 0.2849\n",
      "Epoch 68/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - loss: 0.1370 - mae: 0.2738 - val_loss: 0.1332 - val_mae: 0.2840\n",
      "Epoch 69/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - loss: 0.1273 - mae: 0.2596 - val_loss: 0.1321 - val_mae: 0.2833\n",
      "Epoch 70/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - loss: 0.1354 - mae: 0.2746 - val_loss: 0.1310 - val_mae: 0.2824\n",
      "Epoch 71/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - loss: 0.1416 - mae: 0.2821 - val_loss: 0.1299 - val_mae: 0.2816\n",
      "Epoch 72/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 0.1601 - mae: 0.3030 - val_loss: 0.1289 - val_mae: 0.2808\n",
      "Epoch 73/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - loss: 0.1341 - mae: 0.2726 - val_loss: 0.1279 - val_mae: 0.2801\n",
      "Epoch 74/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 0.1136 - mae: 0.2508 - val_loss: 0.1270 - val_mae: 0.2793\n",
      "Epoch 75/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - loss: 0.1315 - mae: 0.2767 - val_loss: 0.1260 - val_mae: 0.2786\n",
      "Epoch 76/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 0.1303 - mae: 0.2687 - val_loss: 0.1251 - val_mae: 0.2778\n",
      "Epoch 77/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - loss: 0.1325 - mae: 0.2686 - val_loss: 0.1243 - val_mae: 0.2771\n",
      "Epoch 78/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - loss: 0.1070 - mae: 0.2443 - val_loss: 0.1235 - val_mae: 0.2763\n",
      "Epoch 79/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - loss: 0.1603 - mae: 0.3162 - val_loss: 0.1226 - val_mae: 0.2756\n",
      "Epoch 80/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - loss: 0.1208 - mae: 0.2615 - val_loss: 0.1219 - val_mae: 0.2749\n",
      "Epoch 81/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.1159 - mae: 0.2591 - val_loss: 0.1212 - val_mae: 0.2742\n",
      "Epoch 82/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 0.0959 - mae: 0.2336 - val_loss: 0.1205 - val_mae: 0.2736\n",
      "Epoch 83/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - loss: 0.1309 - mae: 0.2784 - val_loss: 0.1198 - val_mae: 0.2729\n",
      "Epoch 84/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - loss: 0.1122 - mae: 0.2616 - val_loss: 0.1192 - val_mae: 0.2723\n",
      "Epoch 85/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - loss: 0.1138 - mae: 0.2561 - val_loss: 0.1186 - val_mae: 0.2716\n",
      "Epoch 86/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - loss: 0.1126 - mae: 0.2596 - val_loss: 0.1180 - val_mae: 0.2710\n",
      "Epoch 87/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.1096 - mae: 0.2549 - val_loss: 0.1174 - val_mae: 0.2703\n",
      "Epoch 88/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - loss: 0.1200 - mae: 0.2667 - val_loss: 0.1169 - val_mae: 0.2697\n",
      "Epoch 89/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - loss: 0.1085 - mae: 0.2528 - val_loss: 0.1163 - val_mae: 0.2691\n",
      "Epoch 90/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - loss: 0.1263 - mae: 0.2761 - val_loss: 0.1158 - val_mae: 0.2684\n",
      "Epoch 91/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - loss: 0.1258 - mae: 0.2800 - val_loss: 0.1153 - val_mae: 0.2678\n",
      "Epoch 92/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.1101 - mae: 0.2578 - val_loss: 0.1149 - val_mae: 0.2673\n",
      "Epoch 93/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - loss: 0.1020 - mae: 0.2485 - val_loss: 0.1145 - val_mae: 0.2667\n",
      "Epoch 94/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 0.1179 - mae: 0.2702 - val_loss: 0.1140 - val_mae: 0.2662\n",
      "Epoch 95/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 0.1255 - mae: 0.2803 - val_loss: 0.1136 - val_mae: 0.2656\n",
      "Epoch 96/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - loss: 0.1062 - mae: 0.2541 - val_loss: 0.1133 - val_mae: 0.2651\n",
      "Epoch 97/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - loss: 0.1064 - mae: 0.2512 - val_loss: 0.1129 - val_mae: 0.2645\n",
      "Epoch 98/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 0.1197 - mae: 0.2737 - val_loss: 0.1125 - val_mae: 0.2642\n",
      "Epoch 99/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 0.1147 - mae: 0.2628 - val_loss: 0.1122 - val_mae: 0.2640\n",
      "Epoch 100/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.0982 - mae: 0.2421 - val_loss: 0.1119 - val_mae: 0.2639\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 434ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Done with capsicum_Dharwad_daily.csv | MAE=933.04, RMSE=1185.75, R2=-0.0341, MAPE=51.77%, ACC=48.23%\n",
      "\n",
      "ğŸš€ Processing: capsicum_Hassan_daily.csv\n",
      "Epoch 1/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 25ms/step - loss: 0.0431 - mae: 0.1685 - val_loss: 0.0896 - val_mae: 0.2395\n",
      "Epoch 2/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0170 - mae: 0.1022 - val_loss: 0.0864 - val_mae: 0.2329\n",
      "Epoch 3/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0169 - mae: 0.1016 - val_loss: 0.0882 - val_mae: 0.2366\n",
      "Epoch 4/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 0.0171 - mae: 0.1045 - val_loss: 0.0853 - val_mae: 0.2305\n",
      "Epoch 5/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - loss: 0.0167 - mae: 0.0992 - val_loss: 0.0847 - val_mae: 0.2295\n",
      "Epoch 6/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 0.0167 - mae: 0.0994 - val_loss: 0.0894 - val_mae: 0.2391\n",
      "Epoch 7/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 0.0172 - mae: 0.1029 - val_loss: 0.0861 - val_mae: 0.2323\n",
      "Epoch 8/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0178 - mae: 0.1044 - val_loss: 0.0872 - val_mae: 0.2345\n",
      "Epoch 9/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 0.0178 - mae: 0.1052 - val_loss: 0.0853 - val_mae: 0.2307\n",
      "Epoch 10/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0171 - mae: 0.1011 - val_loss: 0.0892 - val_mae: 0.2386\n",
      "Epoch 11/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0171 - mae: 0.1022 - val_loss: 0.0854 - val_mae: 0.2308\n",
      "Epoch 12/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0167 - mae: 0.1020 - val_loss: 0.0888 - val_mae: 0.2379\n",
      "Epoch 13/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0182 - mae: 0.1068 - val_loss: 0.0843 - val_mae: 0.2285\n",
      "Epoch 14/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0171 - mae: 0.1015 - val_loss: 0.0865 - val_mae: 0.2332\n",
      "Epoch 15/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0168 - mae: 0.1009 - val_loss: 0.0861 - val_mae: 0.2322\n",
      "Epoch 16/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0165 - mae: 0.1005 - val_loss: 0.0860 - val_mae: 0.2321\n",
      "Epoch 17/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0160 - mae: 0.1003 - val_loss: 0.0853 - val_mae: 0.2305\n",
      "Epoch 18/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 0.0177 - mae: 0.1034 - val_loss: 0.0875 - val_mae: 0.2352\n",
      "Epoch 19/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0174 - mae: 0.1033 - val_loss: 0.0856 - val_mae: 0.2313\n",
      "Epoch 20/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0169 - mae: 0.1012 - val_loss: 0.0837 - val_mae: 0.2273\n",
      "Epoch 21/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0179 - mae: 0.1061 - val_loss: 0.0859 - val_mae: 0.2319\n",
      "Epoch 22/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0174 - mae: 0.1042 - val_loss: 0.0884 - val_mae: 0.2370\n",
      "Epoch 23/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0170 - mae: 0.1018 - val_loss: 0.0874 - val_mae: 0.2349\n",
      "Epoch 24/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0183 - mae: 0.1062 - val_loss: 0.0859 - val_mae: 0.2319\n",
      "Epoch 25/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 0.0166 - mae: 0.1018 - val_loss: 0.0808 - val_mae: 0.2211\n",
      "Epoch 26/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 0.0176 - mae: 0.1024 - val_loss: 0.0876 - val_mae: 0.2354\n",
      "Epoch 27/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0164 - mae: 0.1012 - val_loss: 0.0839 - val_mae: 0.2277\n",
      "Epoch 28/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - loss: 0.0167 - mae: 0.1021 - val_loss: 0.0858 - val_mae: 0.2316\n",
      "Epoch 29/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - loss: 0.0166 - mae: 0.1015 - val_loss: 0.0872 - val_mae: 0.2346\n",
      "Epoch 30/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0178 - mae: 0.1043 - val_loss: 0.0872 - val_mae: 0.2345\n",
      "Epoch 31/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 0.0168 - mae: 0.1028 - val_loss: 0.0847 - val_mae: 0.2294\n",
      "Epoch 32/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 0.0178 - mae: 0.1036 - val_loss: 0.0844 - val_mae: 0.2287\n",
      "Epoch 33/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0183 - mae: 0.1067 - val_loss: 0.0889 - val_mae: 0.2380\n",
      "Epoch 34/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 0.0172 - mae: 0.1039 - val_loss: 0.0845 - val_mae: 0.2290\n",
      "Epoch 35/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0175 - mae: 0.1045 - val_loss: 0.0874 - val_mae: 0.2349\n",
      "Epoch 36/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 27ms/step - loss: 0.0172 - mae: 0.1035 - val_loss: 0.0864 - val_mae: 0.2329\n",
      "Epoch 37/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 29ms/step - loss: 0.0161 - mae: 0.0975 - val_loss: 0.0912 - val_mae: 0.2427\n",
      "Epoch 38/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0171 - mae: 0.1034 - val_loss: 0.0865 - val_mae: 0.2332\n",
      "Epoch 39/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0173 - mae: 0.1032 - val_loss: 0.0886 - val_mae: 0.2375\n",
      "Epoch 40/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0177 - mae: 0.1049 - val_loss: 0.0856 - val_mae: 0.2312\n",
      "Epoch 41/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0175 - mae: 0.1028 - val_loss: 0.0863 - val_mae: 0.2327\n",
      "Epoch 42/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0173 - mae: 0.1039 - val_loss: 0.0849 - val_mae: 0.2297\n",
      "Epoch 43/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - loss: 0.0173 - mae: 0.1027 - val_loss: 0.0847 - val_mae: 0.2294\n",
      "Epoch 44/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - loss: 0.0166 - mae: 0.1015 - val_loss: 0.0850 - val_mae: 0.2299\n",
      "Epoch 45/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 0.0178 - mae: 0.1037 - val_loss: 0.0830 - val_mae: 0.2258\n",
      "Epoch 46/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 0.0173 - mae: 0.1021 - val_loss: 0.0886 - val_mae: 0.2375\n",
      "Epoch 47/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0170 - mae: 0.1013 - val_loss: 0.0921 - val_mae: 0.2445\n",
      "Epoch 48/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0179 - mae: 0.1035 - val_loss: 0.0915 - val_mae: 0.2432\n",
      "Epoch 49/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 0.0173 - mae: 0.1033 - val_loss: 0.0890 - val_mae: 0.2383\n",
      "Epoch 50/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - loss: 0.0170 - mae: 0.1014 - val_loss: 0.0912 - val_mae: 0.2428\n",
      "Epoch 51/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.0178 - mae: 0.1062 - val_loss: 0.0850 - val_mae: 0.2299\n",
      "Epoch 52/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0174 - mae: 0.1032 - val_loss: 0.0852 - val_mae: 0.2305\n",
      "Epoch 53/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0171 - mae: 0.1007 - val_loss: 0.0910 - val_mae: 0.2424\n",
      "Epoch 54/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0176 - mae: 0.1042 - val_loss: 0.0891 - val_mae: 0.2385\n",
      "Epoch 55/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0177 - mae: 0.1038 - val_loss: 0.0874 - val_mae: 0.2351\n",
      "Epoch 56/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0172 - mae: 0.1026 - val_loss: 0.0852 - val_mae: 0.2305\n",
      "Epoch 57/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0166 - mae: 0.1010 - val_loss: 0.0859 - val_mae: 0.2320\n",
      "Epoch 58/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0168 - mae: 0.1012 - val_loss: 0.0881 - val_mae: 0.2364\n",
      "Epoch 59/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0177 - mae: 0.1035 - val_loss: 0.0820 - val_mae: 0.2235\n",
      "Epoch 60/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0184 - mae: 0.1052 - val_loss: 0.0839 - val_mae: 0.2277\n",
      "Epoch 61/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0177 - mae: 0.1027 - val_loss: 0.0903 - val_mae: 0.2408\n",
      "Epoch 62/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0167 - mae: 0.1038 - val_loss: 0.0879 - val_mae: 0.2361\n",
      "Epoch 63/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0173 - mae: 0.1028 - val_loss: 0.0850 - val_mae: 0.2301\n",
      "Epoch 64/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0168 - mae: 0.1025 - val_loss: 0.0838 - val_mae: 0.2274\n",
      "Epoch 65/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0177 - mae: 0.1051 - val_loss: 0.0896 - val_mae: 0.2395\n",
      "Epoch 66/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0178 - mae: 0.1032 - val_loss: 0.0854 - val_mae: 0.2308\n",
      "Epoch 67/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0170 - mae: 0.1001 - val_loss: 0.0858 - val_mae: 0.2316\n",
      "Epoch 68/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0163 - mae: 0.0989 - val_loss: 0.0868 - val_mae: 0.2338\n",
      "Epoch 69/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0175 - mae: 0.1033 - val_loss: 0.0847 - val_mae: 0.2294\n",
      "Epoch 70/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0161 - mae: 0.0988 - val_loss: 0.0912 - val_mae: 0.2428\n",
      "Epoch 71/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0177 - mae: 0.1050 - val_loss: 0.0859 - val_mae: 0.2319\n",
      "Epoch 72/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0175 - mae: 0.1035 - val_loss: 0.0855 - val_mae: 0.2310\n",
      "Epoch 73/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0172 - mae: 0.1028 - val_loss: 0.0856 - val_mae: 0.2312\n",
      "Epoch 74/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0179 - mae: 0.1054 - val_loss: 0.0827 - val_mae: 0.2251\n",
      "Epoch 75/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0177 - mae: 0.1057 - val_loss: 0.0842 - val_mae: 0.2283\n",
      "Epoch 76/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0164 - mae: 0.1013 - val_loss: 0.0847 - val_mae: 0.2293\n",
      "Epoch 77/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0174 - mae: 0.1035 - val_loss: 0.0832 - val_mae: 0.2262\n",
      "Epoch 78/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0176 - mae: 0.1028 - val_loss: 0.0880 - val_mae: 0.2362\n",
      "Epoch 79/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0182 - mae: 0.1067 - val_loss: 0.0846 - val_mae: 0.2292\n",
      "Epoch 80/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0170 - mae: 0.1022 - val_loss: 0.0858 - val_mae: 0.2317\n",
      "Epoch 81/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0174 - mae: 0.1036 - val_loss: 0.0916 - val_mae: 0.2434\n",
      "Epoch 82/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0171 - mae: 0.1036 - val_loss: 0.0883 - val_mae: 0.2368\n",
      "Epoch 83/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0171 - mae: 0.1026 - val_loss: 0.0890 - val_mae: 0.2383\n",
      "Epoch 84/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0177 - mae: 0.1053 - val_loss: 0.0855 - val_mae: 0.2310\n",
      "Epoch 85/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0168 - mae: 0.1018 - val_loss: 0.0870 - val_mae: 0.2341\n",
      "Epoch 86/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0174 - mae: 0.1023 - val_loss: 0.0887 - val_mae: 0.2376\n",
      "Epoch 87/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0172 - mae: 0.1025 - val_loss: 0.0903 - val_mae: 0.2408\n",
      "Epoch 88/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0184 - mae: 0.1064 - val_loss: 0.0895 - val_mae: 0.2392\n",
      "Epoch 89/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0168 - mae: 0.1030 - val_loss: 0.0841 - val_mae: 0.2281\n",
      "Epoch 90/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0176 - mae: 0.1024 - val_loss: 0.0892 - val_mae: 0.2387\n",
      "Epoch 91/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0169 - mae: 0.1025 - val_loss: 0.0859 - val_mae: 0.2319\n",
      "Epoch 92/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0172 - mae: 0.1040 - val_loss: 0.0886 - val_mae: 0.2375\n",
      "Epoch 93/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0168 - mae: 0.1019 - val_loss: 0.0887 - val_mae: 0.2377\n",
      "Epoch 94/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0169 - mae: 0.1023 - val_loss: 0.0831 - val_mae: 0.2260\n",
      "Epoch 95/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0164 - mae: 0.1007 - val_loss: 0.0861 - val_mae: 0.2324\n",
      "Epoch 96/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0171 - mae: 0.1015 - val_loss: 0.0915 - val_mae: 0.2434\n",
      "Epoch 97/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0167 - mae: 0.1018 - val_loss: 0.0920 - val_mae: 0.2443\n",
      "Epoch 98/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0167 - mae: 0.1030 - val_loss: 0.0864 - val_mae: 0.2330\n",
      "Epoch 99/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0172 - mae: 0.1021 - val_loss: 0.0877 - val_mae: 0.2355\n",
      "Epoch 100/100\n",
      "\u001b[1m136/136\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0167 - mae: 0.1020 - val_loss: 0.0836 - val_mae: 0.2270\n",
      "WARNING:tensorflow:5 out of the last 58 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001B07F559C60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 58 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001B07F559C60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Done with capsicum_Hassan_daily.csv | MAE=759.98, RMSE=1047.41, R2=-0.0556, MAPE=44.05%, ACC=55.95%\n",
      "\n",
      "ğŸš€ Processing: capsicum_Haveri_daily.csv\n",
      "ğŸ“Œ Padded 30 rows with avg=2850.00 for capsicum_Haveri_daily.csv to reach 31 rows.\n",
      "âš ï¸ Small dataset detected for capsicum_Haveri_daily.csv â†’ training WITHOUT validation_split.\n",
      "Epoch 1/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 2/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 3/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 4/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 5/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 6/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 7/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 8/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 9/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 10/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 11/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 12/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 13/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 14/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 15/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 16/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 17/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 18/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 19/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 20/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 21/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 22/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 23/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 24/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 25/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 26/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 27/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 28/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 29/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 30/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 31/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 32/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 33/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 34/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 35/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 36/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 37/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 38/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 39/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 40/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 41/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 42/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 43/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 44/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 45/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 46/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 47/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 48/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 49/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 50/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 51/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 52/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 53/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 54/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 55/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 56/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 57/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 58/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 59/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 60/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 61/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 62/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 63/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 64/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 65/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 66/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 67/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 68/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 69/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 70/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 71/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 72/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 73/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 74/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 75/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 76/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 77/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 78/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 79/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 80/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 81/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 82/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 83/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 84/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 85/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 86/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 87/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 88/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 89/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 90/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 91/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 92/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 93/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 94/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 95/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 96/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 97/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 98/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 99/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "Epoch 100/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0000e+00 - mae: 0.0000e+00\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 305ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Done with capsicum_Haveri_daily.csv | MAE=0.0, RMSE=0.0, R2=nan, MAPE=0.0%, ACC=100.0%\n",
      "\n",
      "ğŸš€ Processing: capsicum_Kalburgi_daily.csv\n",
      "Epoch 1/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 31ms/step - loss: 0.2356 - mae: 0.4441 - val_loss: 0.3976 - val_mae: 0.6216\n",
      "Epoch 2/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2157 - mae: 0.4181 - val_loss: 0.3646 - val_mae: 0.5945\n",
      "Epoch 3/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1791 - mae: 0.3868 - val_loss: 0.3319 - val_mae: 0.5663\n",
      "Epoch 4/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.1685 - mae: 0.3679 - val_loss: 0.3003 - val_mae: 0.5377\n",
      "Epoch 5/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.1504 - mae: 0.3428 - val_loss: 0.2698 - val_mae: 0.5086\n",
      "Epoch 6/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1325 - mae: 0.3185 - val_loss: 0.2404 - val_mae: 0.4788\n",
      "Epoch 7/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1069 - mae: 0.2811 - val_loss: 0.2129 - val_mae: 0.4491\n",
      "Epoch 8/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0950 - mae: 0.2614 - val_loss: 0.1872 - val_mae: 0.4196\n",
      "Epoch 9/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0786 - mae: 0.2313 - val_loss: 0.1643 - val_mae: 0.3913\n",
      "Epoch 10/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0671 - mae: 0.2092 - val_loss: 0.1427 - val_mae: 0.3627\n",
      "Epoch 11/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0610 - mae: 0.1986 - val_loss: 0.1250 - val_mae: 0.3375\n",
      "Epoch 12/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0520 - mae: 0.1803 - val_loss: 0.1094 - val_mae: 0.3134\n",
      "Epoch 13/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0469 - mae: 0.1747 - val_loss: 0.0966 - val_mae: 0.2924\n",
      "Epoch 14/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0468 - mae: 0.1660 - val_loss: 0.0857 - val_mae: 0.2731\n",
      "Epoch 15/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0405 - mae: 0.1592 - val_loss: 0.0774 - val_mae: 0.2573\n",
      "Epoch 16/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0381 - mae: 0.1490 - val_loss: 0.0700 - val_mae: 0.2427\n",
      "Epoch 17/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0400 - mae: 0.1550 - val_loss: 0.0649 - val_mae: 0.2318\n",
      "Epoch 18/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0389 - mae: 0.1538 - val_loss: 0.0616 - val_mae: 0.2247\n",
      "Epoch 19/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0374 - mae: 0.1512 - val_loss: 0.0579 - val_mae: 0.2161\n",
      "Epoch 20/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0370 - mae: 0.1570 - val_loss: 0.0556 - val_mae: 0.2107\n",
      "Epoch 21/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0405 - mae: 0.1573 - val_loss: 0.0539 - val_mae: 0.2066\n",
      "Epoch 22/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0344 - mae: 0.1482 - val_loss: 0.0533 - val_mae: 0.2054\n",
      "Epoch 23/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0396 - mae: 0.1550 - val_loss: 0.0526 - val_mae: 0.2036\n",
      "Epoch 24/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0385 - mae: 0.1573 - val_loss: 0.0520 - val_mae: 0.2022\n",
      "Epoch 25/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0346 - mae: 0.1455 - val_loss: 0.0516 - val_mae: 0.2010\n",
      "Epoch 26/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0368 - mae: 0.1494 - val_loss: 0.0506 - val_mae: 0.1985\n",
      "Epoch 27/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0403 - mae: 0.1637 - val_loss: 0.0510 - val_mae: 0.1996\n",
      "Epoch 28/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0373 - mae: 0.1523 - val_loss: 0.0505 - val_mae: 0.1983\n",
      "Epoch 29/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0340 - mae: 0.1462 - val_loss: 0.0505 - val_mae: 0.1984\n",
      "Epoch 30/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0389 - mae: 0.1582 - val_loss: 0.0498 - val_mae: 0.1966\n",
      "Epoch 31/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0360 - mae: 0.1496 - val_loss: 0.0502 - val_mae: 0.1975\n",
      "Epoch 32/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0358 - mae: 0.1533 - val_loss: 0.0502 - val_mae: 0.1977\n",
      "Epoch 33/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0382 - mae: 0.1554 - val_loss: 0.0505 - val_mae: 0.1984\n",
      "Epoch 34/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0340 - mae: 0.1472 - val_loss: 0.0507 - val_mae: 0.1990\n",
      "Epoch 35/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0364 - mae: 0.1529 - val_loss: 0.0509 - val_mae: 0.1993\n",
      "Epoch 36/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0362 - mae: 0.1516 - val_loss: 0.0509 - val_mae: 0.1993\n",
      "Epoch 37/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0380 - mae: 0.1554 - val_loss: 0.0511 - val_mae: 0.2000\n",
      "Epoch 38/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0327 - mae: 0.1466 - val_loss: 0.0505 - val_mae: 0.1983\n",
      "Epoch 39/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0374 - mae: 0.1573 - val_loss: 0.0506 - val_mae: 0.1985\n",
      "Epoch 40/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0338 - mae: 0.1447 - val_loss: 0.0502 - val_mae: 0.1976\n",
      "Epoch 41/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0378 - mae: 0.1540 - val_loss: 0.0498 - val_mae: 0.1965\n",
      "Epoch 42/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0366 - mae: 0.1511 - val_loss: 0.0499 - val_mae: 0.1968\n",
      "Epoch 43/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0409 - mae: 0.1557 - val_loss: 0.0502 - val_mae: 0.1976\n",
      "Epoch 44/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0359 - mae: 0.1518 - val_loss: 0.0507 - val_mae: 0.1989\n",
      "Epoch 45/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0365 - mae: 0.1501 - val_loss: 0.0506 - val_mae: 0.1986\n",
      "Epoch 46/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0361 - mae: 0.1501 - val_loss: 0.0496 - val_mae: 0.1961\n",
      "Epoch 47/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0387 - mae: 0.1545 - val_loss: 0.0507 - val_mae: 0.1988\n",
      "Epoch 48/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0370 - mae: 0.1531 - val_loss: 0.0502 - val_mae: 0.1976\n",
      "Epoch 49/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0377 - mae: 0.1562 - val_loss: 0.0507 - val_mae: 0.1988\n",
      "Epoch 50/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0377 - mae: 0.1546 - val_loss: 0.0490 - val_mae: 0.1946\n",
      "Epoch 51/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0392 - mae: 0.1574 - val_loss: 0.0497 - val_mae: 0.1964\n",
      "Epoch 52/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0392 - mae: 0.1591 - val_loss: 0.0508 - val_mae: 0.1991\n",
      "Epoch 53/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0364 - mae: 0.1522 - val_loss: 0.0503 - val_mae: 0.1977\n",
      "Epoch 54/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0358 - mae: 0.1497 - val_loss: 0.0507 - val_mae: 0.1989\n",
      "Epoch 55/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0379 - mae: 0.1537 - val_loss: 0.0504 - val_mae: 0.1982\n",
      "Epoch 56/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0400 - mae: 0.1586 - val_loss: 0.0504 - val_mae: 0.1982\n",
      "Epoch 57/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0357 - mae: 0.1517 - val_loss: 0.0512 - val_mae: 0.2000\n",
      "Epoch 58/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0321 - mae: 0.1421 - val_loss: 0.0509 - val_mae: 0.1994\n",
      "Epoch 59/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0349 - mae: 0.1461 - val_loss: 0.0505 - val_mae: 0.1983\n",
      "Epoch 60/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0342 - mae: 0.1495 - val_loss: 0.0497 - val_mae: 0.1962\n",
      "Epoch 61/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0361 - mae: 0.1511 - val_loss: 0.0507 - val_mae: 0.1988\n",
      "Epoch 62/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0333 - mae: 0.1475 - val_loss: 0.0500 - val_mae: 0.1970\n",
      "Epoch 63/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0353 - mae: 0.1496 - val_loss: 0.0492 - val_mae: 0.1950\n",
      "Epoch 64/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0383 - mae: 0.1561 - val_loss: 0.0496 - val_mae: 0.1960\n",
      "Epoch 65/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0364 - mae: 0.1508 - val_loss: 0.0501 - val_mae: 0.1974\n",
      "Epoch 66/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0355 - mae: 0.1512 - val_loss: 0.0517 - val_mae: 0.2013\n",
      "Epoch 67/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0392 - mae: 0.1574 - val_loss: 0.0506 - val_mae: 0.1985\n",
      "Epoch 68/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0341 - mae: 0.1471 - val_loss: 0.0507 - val_mae: 0.1988\n",
      "Epoch 69/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0374 - mae: 0.1539 - val_loss: 0.0523 - val_mae: 0.2030\n",
      "Epoch 70/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0374 - mae: 0.1549 - val_loss: 0.0516 - val_mae: 0.2012\n",
      "Epoch 71/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0377 - mae: 0.1567 - val_loss: 0.0499 - val_mae: 0.1968\n",
      "Epoch 72/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0351 - mae: 0.1464 - val_loss: 0.0504 - val_mae: 0.1980\n",
      "Epoch 73/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0409 - mae: 0.1626 - val_loss: 0.0502 - val_mae: 0.1975\n",
      "Epoch 74/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0387 - mae: 0.1563 - val_loss: 0.0510 - val_mae: 0.1997\n",
      "Epoch 75/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0361 - mae: 0.1516 - val_loss: 0.0504 - val_mae: 0.1980\n",
      "Epoch 76/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0348 - mae: 0.1481 - val_loss: 0.0502 - val_mae: 0.1977\n",
      "Epoch 77/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0369 - mae: 0.1508 - val_loss: 0.0517 - val_mae: 0.2014\n",
      "Epoch 78/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0340 - mae: 0.1465 - val_loss: 0.0517 - val_mae: 0.2014\n",
      "Epoch 79/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0332 - mae: 0.1440 - val_loss: 0.0507 - val_mae: 0.1990\n",
      "Epoch 80/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0409 - mae: 0.1564 - val_loss: 0.0508 - val_mae: 0.1992\n",
      "Epoch 81/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0354 - mae: 0.1516 - val_loss: 0.0499 - val_mae: 0.1969\n",
      "Epoch 82/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0351 - mae: 0.1464 - val_loss: 0.0505 - val_mae: 0.1984\n",
      "Epoch 83/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0356 - mae: 0.1513 - val_loss: 0.0498 - val_mae: 0.1965\n",
      "Epoch 84/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0349 - mae: 0.1481 - val_loss: 0.0498 - val_mae: 0.1966\n",
      "Epoch 85/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0373 - mae: 0.1550 - val_loss: 0.0503 - val_mae: 0.1978\n",
      "Epoch 86/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0365 - mae: 0.1492 - val_loss: 0.0501 - val_mae: 0.1974\n",
      "Epoch 87/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0390 - mae: 0.1535 - val_loss: 0.0515 - val_mae: 0.2008\n",
      "Epoch 88/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0364 - mae: 0.1495 - val_loss: 0.0517 - val_mae: 0.2013\n",
      "Epoch 89/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0365 - mae: 0.1528 - val_loss: 0.0506 - val_mae: 0.1985\n",
      "Epoch 90/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0376 - mae: 0.1538 - val_loss: 0.0507 - val_mae: 0.1989\n",
      "Epoch 91/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0360 - mae: 0.1528 - val_loss: 0.0508 - val_mae: 0.1991\n",
      "Epoch 92/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0357 - mae: 0.1498 - val_loss: 0.0509 - val_mae: 0.1993\n",
      "Epoch 93/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0346 - mae: 0.1507 - val_loss: 0.0506 - val_mae: 0.1987\n",
      "Epoch 94/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0374 - mae: 0.1546 - val_loss: 0.0506 - val_mae: 0.1986\n",
      "Epoch 95/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0381 - mae: 0.1577 - val_loss: 0.0515 - val_mae: 0.2008\n",
      "Epoch 96/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0406 - mae: 0.1604 - val_loss: 0.0517 - val_mae: 0.2013\n",
      "Epoch 97/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0349 - mae: 0.1483 - val_loss: 0.0492 - val_mae: 0.1950\n",
      "Epoch 98/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0391 - mae: 0.1571 - val_loss: 0.0500 - val_mae: 0.1970\n",
      "Epoch 99/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0336 - mae: 0.1479 - val_loss: 0.0494 - val_mae: 0.1955\n",
      "Epoch 100/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0341 - mae: 0.1479 - val_loss: 0.0485 - val_mae: 0.1933\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Done with capsicum_Kalburgi_daily.csv | MAE=872.23, RMSE=1071.61, R2=-0.0326, MAPE=68.51%, ACC=31.49%\n",
      "\n",
      "ğŸš€ Processing: capsicum_Kolar_daily.csv\n",
      "Epoch 1/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 0.0656 - mae: 0.1820 - val_loss: 0.0110 - val_mae: 0.0796\n",
      "Epoch 2/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0388 - mae: 0.1436 - val_loss: 0.0115 - val_mae: 0.0803\n",
      "Epoch 3/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0380 - mae: 0.1415 - val_loss: 0.0120 - val_mae: 0.0813\n",
      "Epoch 4/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0376 - mae: 0.1398 - val_loss: 0.0110 - val_mae: 0.0795\n",
      "Epoch 5/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0387 - mae: 0.1438 - val_loss: 0.0111 - val_mae: 0.0797\n",
      "Epoch 6/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0378 - mae: 0.1409 - val_loss: 0.0114 - val_mae: 0.0801\n",
      "Epoch 7/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0371 - mae: 0.1406 - val_loss: 0.0116 - val_mae: 0.0804\n",
      "Epoch 8/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0378 - mae: 0.1416 - val_loss: 0.0118 - val_mae: 0.0809\n",
      "Epoch 9/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0373 - mae: 0.1393 - val_loss: 0.0112 - val_mae: 0.0798\n",
      "Epoch 10/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0365 - mae: 0.1373 - val_loss: 0.0110 - val_mae: 0.0796\n",
      "Epoch 11/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0359 - mae: 0.1361 - val_loss: 0.0109 - val_mae: 0.0796\n",
      "Epoch 12/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0373 - mae: 0.1408 - val_loss: 0.0120 - val_mae: 0.0813\n",
      "Epoch 13/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0381 - mae: 0.1413 - val_loss: 0.0113 - val_mae: 0.0801\n",
      "Epoch 14/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0396 - mae: 0.1450 - val_loss: 0.0118 - val_mae: 0.0809\n",
      "Epoch 15/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0381 - mae: 0.1412 - val_loss: 0.0114 - val_mae: 0.0802\n",
      "Epoch 16/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0393 - mae: 0.1438 - val_loss: 0.0118 - val_mae: 0.0810\n",
      "Epoch 17/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0390 - mae: 0.1429 - val_loss: 0.0113 - val_mae: 0.0801\n",
      "Epoch 18/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0383 - mae: 0.1416 - val_loss: 0.0114 - val_mae: 0.0801\n",
      "Epoch 19/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0395 - mae: 0.1441 - val_loss: 0.0111 - val_mae: 0.0797\n",
      "Epoch 20/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0378 - mae: 0.1402 - val_loss: 0.0109 - val_mae: 0.0796\n",
      "Epoch 21/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0382 - mae: 0.1421 - val_loss: 0.0116 - val_mae: 0.0805\n",
      "Epoch 22/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0363 - mae: 0.1388 - val_loss: 0.0111 - val_mae: 0.0796\n",
      "Epoch 23/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0374 - mae: 0.1401 - val_loss: 0.0111 - val_mae: 0.0797\n",
      "Epoch 24/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0380 - mae: 0.1422 - val_loss: 0.0114 - val_mae: 0.0802\n",
      "Epoch 25/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0367 - mae: 0.1387 - val_loss: 0.0110 - val_mae: 0.0796\n",
      "Epoch 26/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0384 - mae: 0.1415 - val_loss: 0.0114 - val_mae: 0.0802\n",
      "Epoch 27/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0380 - mae: 0.1418 - val_loss: 0.0114 - val_mae: 0.0801\n",
      "Epoch 28/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0374 - mae: 0.1401 - val_loss: 0.0116 - val_mae: 0.0805\n",
      "Epoch 29/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0388 - mae: 0.1427 - val_loss: 0.0121 - val_mae: 0.0815\n",
      "Epoch 30/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0398 - mae: 0.1427 - val_loss: 0.0112 - val_mae: 0.0798\n",
      "Epoch 31/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0382 - mae: 0.1418 - val_loss: 0.0117 - val_mae: 0.0807\n",
      "Epoch 32/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0368 - mae: 0.1387 - val_loss: 0.0115 - val_mae: 0.0804\n",
      "Epoch 33/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0391 - mae: 0.1423 - val_loss: 0.0113 - val_mae: 0.0800\n",
      "Epoch 34/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0370 - mae: 0.1396 - val_loss: 0.0117 - val_mae: 0.0807\n",
      "Epoch 35/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0378 - mae: 0.1412 - val_loss: 0.0109 - val_mae: 0.0799\n",
      "Epoch 36/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0369 - mae: 0.1405 - val_loss: 0.0113 - val_mae: 0.0801\n",
      "Epoch 37/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0387 - mae: 0.1439 - val_loss: 0.0120 - val_mae: 0.0813\n",
      "Epoch 38/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0376 - mae: 0.1397 - val_loss: 0.0115 - val_mae: 0.0803\n",
      "Epoch 39/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0382 - mae: 0.1410 - val_loss: 0.0111 - val_mae: 0.0797\n",
      "Epoch 40/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0385 - mae: 0.1420 - val_loss: 0.0125 - val_mae: 0.0829\n",
      "Epoch 41/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0395 - mae: 0.1432 - val_loss: 0.0110 - val_mae: 0.0796\n",
      "Epoch 42/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0391 - mae: 0.1443 - val_loss: 0.0121 - val_mae: 0.0815\n",
      "Epoch 43/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - loss: 0.0383 - mae: 0.1417 - val_loss: 0.0114 - val_mae: 0.0802\n",
      "Epoch 44/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0389 - mae: 0.1434 - val_loss: 0.0119 - val_mae: 0.0812\n",
      "Epoch 45/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0389 - mae: 0.1422 - val_loss: 0.0118 - val_mae: 0.0809\n",
      "Epoch 46/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0377 - mae: 0.1400 - val_loss: 0.0111 - val_mae: 0.0797\n",
      "Epoch 47/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0390 - mae: 0.1440 - val_loss: 0.0116 - val_mae: 0.0805\n",
      "Epoch 48/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0362 - mae: 0.1379 - val_loss: 0.0109 - val_mae: 0.0797\n",
      "Epoch 49/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0386 - mae: 0.1427 - val_loss: 0.0113 - val_mae: 0.0800\n",
      "Epoch 50/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0399 - mae: 0.1448 - val_loss: 0.0114 - val_mae: 0.0802\n",
      "Epoch 51/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0393 - mae: 0.1436 - val_loss: 0.0111 - val_mae: 0.0797\n",
      "Epoch 52/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0377 - mae: 0.1414 - val_loss: 0.0111 - val_mae: 0.0797\n",
      "Epoch 53/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0383 - mae: 0.1426 - val_loss: 0.0114 - val_mae: 0.0802\n",
      "Epoch 54/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0374 - mae: 0.1401 - val_loss: 0.0111 - val_mae: 0.0797\n",
      "Epoch 55/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - loss: 0.0401 - mae: 0.1460 - val_loss: 0.0116 - val_mae: 0.0806\n",
      "Epoch 56/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - loss: 0.0395 - mae: 0.1439 - val_loss: 0.0117 - val_mae: 0.0808\n",
      "Epoch 57/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - loss: 0.0386 - mae: 0.1418 - val_loss: 0.0115 - val_mae: 0.0804\n",
      "Epoch 58/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - loss: 0.0394 - mae: 0.1443 - val_loss: 0.0113 - val_mae: 0.0799\n",
      "Epoch 59/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - loss: 0.0393 - mae: 0.1446 - val_loss: 0.0112 - val_mae: 0.0799\n",
      "Epoch 60/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0380 - mae: 0.1419 - val_loss: 0.0112 - val_mae: 0.0799\n",
      "Epoch 61/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0361 - mae: 0.1377 - val_loss: 0.0109 - val_mae: 0.0798\n",
      "Epoch 62/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - loss: 0.0378 - mae: 0.1423 - val_loss: 0.0113 - val_mae: 0.0799\n",
      "Epoch 63/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - loss: 0.0382 - mae: 0.1412 - val_loss: 0.0118 - val_mae: 0.0809\n",
      "Epoch 64/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - loss: 0.0373 - mae: 0.1385 - val_loss: 0.0109 - val_mae: 0.0800\n",
      "Epoch 65/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - loss: 0.0390 - mae: 0.1448 - val_loss: 0.0114 - val_mae: 0.0802\n",
      "Epoch 66/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0380 - mae: 0.1421 - val_loss: 0.0114 - val_mae: 0.0801\n",
      "Epoch 67/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - loss: 0.0367 - mae: 0.1388 - val_loss: 0.0114 - val_mae: 0.0801\n",
      "Epoch 68/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - loss: 0.0382 - mae: 0.1432 - val_loss: 0.0114 - val_mae: 0.0802\n",
      "Epoch 69/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - loss: 0.0394 - mae: 0.1441 - val_loss: 0.0121 - val_mae: 0.0815\n",
      "Epoch 70/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - loss: 0.0380 - mae: 0.1404 - val_loss: 0.0112 - val_mae: 0.0799\n",
      "Epoch 71/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0385 - mae: 0.1434 - val_loss: 0.0117 - val_mae: 0.0808\n",
      "Epoch 72/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0383 - mae: 0.1421 - val_loss: 0.0109 - val_mae: 0.0798\n",
      "Epoch 73/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0370 - mae: 0.1396 - val_loss: 0.0113 - val_mae: 0.0800\n",
      "Epoch 74/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0395 - mae: 0.1449 - val_loss: 0.0119 - val_mae: 0.0811\n",
      "Epoch 75/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - loss: 0.0373 - mae: 0.1405 - val_loss: 0.0114 - val_mae: 0.0802\n",
      "Epoch 76/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - loss: 0.0391 - mae: 0.1441 - val_loss: 0.0113 - val_mae: 0.0800\n",
      "Epoch 77/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - loss: 0.0398 - mae: 0.1445 - val_loss: 0.0116 - val_mae: 0.0805\n",
      "Epoch 78/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0375 - mae: 0.1390 - val_loss: 0.0116 - val_mae: 0.0805\n",
      "Epoch 79/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0397 - mae: 0.1452 - val_loss: 0.0115 - val_mae: 0.0804\n",
      "Epoch 80/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0392 - mae: 0.1437 - val_loss: 0.0119 - val_mae: 0.0811\n",
      "Epoch 81/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0356 - mae: 0.1370 - val_loss: 0.0110 - val_mae: 0.0795\n",
      "Epoch 82/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0386 - mae: 0.1431 - val_loss: 0.0115 - val_mae: 0.0804\n",
      "Epoch 83/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0364 - mae: 0.1387 - val_loss: 0.0116 - val_mae: 0.0805\n",
      "Epoch 84/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0377 - mae: 0.1397 - val_loss: 0.0116 - val_mae: 0.0805\n",
      "Epoch 85/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0384 - mae: 0.1408 - val_loss: 0.0110 - val_mae: 0.0796\n",
      "Epoch 86/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0380 - mae: 0.1420 - val_loss: 0.0115 - val_mae: 0.0804\n",
      "Epoch 87/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0387 - mae: 0.1422 - val_loss: 0.0112 - val_mae: 0.0797\n",
      "Epoch 88/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0386 - mae: 0.1432 - val_loss: 0.0117 - val_mae: 0.0807\n",
      "Epoch 89/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0389 - mae: 0.1419 - val_loss: 0.0112 - val_mae: 0.0799\n",
      "Epoch 90/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0378 - mae: 0.1417 - val_loss: 0.0118 - val_mae: 0.0809\n",
      "Epoch 91/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0402 - mae: 0.1447 - val_loss: 0.0121 - val_mae: 0.0815\n",
      "Epoch 92/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0405 - mae: 0.1452 - val_loss: 0.0117 - val_mae: 0.0807\n",
      "Epoch 93/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0354 - mae: 0.1366 - val_loss: 0.0116 - val_mae: 0.0806\n",
      "Epoch 94/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0388 - mae: 0.1422 - val_loss: 0.0113 - val_mae: 0.0801\n",
      "Epoch 95/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0393 - mae: 0.1453 - val_loss: 0.0111 - val_mae: 0.0797\n",
      "Epoch 96/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0385 - mae: 0.1426 - val_loss: 0.0113 - val_mae: 0.0800\n",
      "Epoch 97/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0388 - mae: 0.1426 - val_loss: 0.0113 - val_mae: 0.0800\n",
      "Epoch 98/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0393 - mae: 0.1440 - val_loss: 0.0117 - val_mae: 0.0807\n",
      "Epoch 99/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0370 - mae: 0.1394 - val_loss: 0.0110 - val_mae: 0.0796\n",
      "Epoch 100/100\n",
      "\u001b[1m423/423\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0394 - mae: 0.1446 - val_loss: 0.0118 - val_mae: 0.0809\n",
      "\u001b[1m265/265\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Done with capsicum_Kolar_daily.csv | MAE=1151.18, RMSE=1631.45, R2=-0.0038, MAPE=62.57%, ACC=37.43%\n",
      "\n",
      "ğŸš€ Processing: capsicum_Mandya_daily.csv\n",
      "Epoch 1/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11ms/step - loss: 0.0477 - mae: 0.1543 - val_loss: 0.1270 - val_mae: 0.3387\n",
      "Epoch 2/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0268 - mae: 0.1180 - val_loss: 0.1222 - val_mae: 0.3315\n",
      "Epoch 3/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0256 - mae: 0.1165 - val_loss: 0.1217 - val_mae: 0.3308\n",
      "Epoch 4/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.0264 - mae: 0.1191 - val_loss: 0.1189 - val_mae: 0.3267\n",
      "Epoch 5/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0269 - mae: 0.1217 - val_loss: 0.1189 - val_mae: 0.3268\n",
      "Epoch 6/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0257 - mae: 0.1171 - val_loss: 0.1203 - val_mae: 0.3287\n",
      "Epoch 7/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0263 - mae: 0.1175 - val_loss: 0.1166 - val_mae: 0.3232\n",
      "Epoch 8/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0263 - mae: 0.1189 - val_loss: 0.1227 - val_mae: 0.3324\n",
      "Epoch 9/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0280 - mae: 0.1234 - val_loss: 0.1208 - val_mae: 0.3296\n",
      "Epoch 10/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0263 - mae: 0.1201 - val_loss: 0.1181 - val_mae: 0.3254\n",
      "Epoch 11/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.0272 - mae: 0.1211 - val_loss: 0.1242 - val_mae: 0.3346\n",
      "Epoch 12/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0252 - mae: 0.1170 - val_loss: 0.1170 - val_mae: 0.3238\n",
      "Epoch 13/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0273 - mae: 0.1220 - val_loss: 0.1202 - val_mae: 0.3286\n",
      "Epoch 14/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0280 - mae: 0.1243 - val_loss: 0.1223 - val_mae: 0.3318\n",
      "Epoch 15/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0257 - mae: 0.1169 - val_loss: 0.1167 - val_mae: 0.3233\n",
      "Epoch 16/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.0261 - mae: 0.1188 - val_loss: 0.1177 - val_mae: 0.3249\n",
      "Epoch 17/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0258 - mae: 0.1178 - val_loss: 0.1211 - val_mae: 0.3300\n",
      "Epoch 18/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0273 - mae: 0.1216 - val_loss: 0.1253 - val_mae: 0.3361\n",
      "Epoch 19/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0270 - mae: 0.1201 - val_loss: 0.1241 - val_mae: 0.3345\n",
      "Epoch 20/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0277 - mae: 0.1219 - val_loss: 0.1219 - val_mae: 0.3312\n",
      "Epoch 21/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0273 - mae: 0.1218 - val_loss: 0.1302 - val_mae: 0.3433\n",
      "Epoch 22/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0271 - mae: 0.1176 - val_loss: 0.1193 - val_mae: 0.3273\n",
      "Epoch 23/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0273 - mae: 0.1223 - val_loss: 0.1188 - val_mae: 0.3266\n",
      "Epoch 24/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0257 - mae: 0.1192 - val_loss: 0.1228 - val_mae: 0.3325\n",
      "Epoch 25/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0261 - mae: 0.1193 - val_loss: 0.1217 - val_mae: 0.3309\n",
      "Epoch 26/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0275 - mae: 0.1217 - val_loss: 0.1227 - val_mae: 0.3323\n",
      "Epoch 27/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0265 - mae: 0.1196 - val_loss: 0.1207 - val_mae: 0.3294\n",
      "Epoch 28/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0268 - mae: 0.1202 - val_loss: 0.1206 - val_mae: 0.3292\n",
      "Epoch 29/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0266 - mae: 0.1197 - val_loss: 0.1196 - val_mae: 0.3277\n",
      "Epoch 30/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0272 - mae: 0.1204 - val_loss: 0.1233 - val_mae: 0.3333\n",
      "Epoch 31/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0274 - mae: 0.1215 - val_loss: 0.1252 - val_mae: 0.3360\n",
      "Epoch 32/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0272 - mae: 0.1206 - val_loss: 0.1235 - val_mae: 0.3335\n",
      "Epoch 33/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0259 - mae: 0.1168 - val_loss: 0.1231 - val_mae: 0.3330\n",
      "Epoch 34/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0277 - mae: 0.1210 - val_loss: 0.1192 - val_mae: 0.3271\n",
      "Epoch 35/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0272 - mae: 0.1213 - val_loss: 0.1191 - val_mae: 0.3269\n",
      "Epoch 36/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0270 - mae: 0.1210 - val_loss: 0.1199 - val_mae: 0.3282\n",
      "Epoch 37/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0274 - mae: 0.1202 - val_loss: 0.1227 - val_mae: 0.3324\n",
      "Epoch 38/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0285 - mae: 0.1221 - val_loss: 0.1198 - val_mae: 0.3281\n",
      "Epoch 39/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0260 - mae: 0.1186 - val_loss: 0.1195 - val_mae: 0.3277\n",
      "Epoch 40/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0264 - mae: 0.1193 - val_loss: 0.1216 - val_mae: 0.3307\n",
      "Epoch 41/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0282 - mae: 0.1242 - val_loss: 0.1203 - val_mae: 0.3288\n",
      "Epoch 42/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0270 - mae: 0.1210 - val_loss: 0.1195 - val_mae: 0.3276\n",
      "Epoch 43/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0272 - mae: 0.1202 - val_loss: 0.1209 - val_mae: 0.3297\n",
      "Epoch 44/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.0273 - mae: 0.1212 - val_loss: 0.1195 - val_mae: 0.3276\n",
      "Epoch 45/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0267 - mae: 0.1200 - val_loss: 0.1217 - val_mae: 0.3308\n",
      "Epoch 46/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0279 - mae: 0.1234 - val_loss: 0.1272 - val_mae: 0.3390\n",
      "Epoch 47/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0274 - mae: 0.1209 - val_loss: 0.1227 - val_mae: 0.3323\n",
      "Epoch 48/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0252 - mae: 0.1167 - val_loss: 0.1178 - val_mae: 0.3250\n",
      "Epoch 49/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0275 - mae: 0.1226 - val_loss: 0.1246 - val_mae: 0.3351\n",
      "Epoch 50/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0268 - mae: 0.1188 - val_loss: 0.1213 - val_mae: 0.3304\n",
      "Epoch 51/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0253 - mae: 0.1182 - val_loss: 0.1212 - val_mae: 0.3301\n",
      "Epoch 52/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0277 - mae: 0.1220 - val_loss: 0.1249 - val_mae: 0.3356\n",
      "Epoch 53/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0275 - mae: 0.1215 - val_loss: 0.1228 - val_mae: 0.3324\n",
      "Epoch 54/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0273 - mae: 0.1229 - val_loss: 0.1236 - val_mae: 0.3336\n",
      "Epoch 55/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0279 - mae: 0.1231 - val_loss: 0.1218 - val_mae: 0.3310\n",
      "Epoch 56/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0267 - mae: 0.1211 - val_loss: 0.1244 - val_mae: 0.3349\n",
      "Epoch 57/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0273 - mae: 0.1211 - val_loss: 0.1247 - val_mae: 0.3354\n",
      "Epoch 58/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0286 - mae: 0.1225 - val_loss: 0.1239 - val_mae: 0.3342\n",
      "Epoch 59/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0268 - mae: 0.1201 - val_loss: 0.1155 - val_mae: 0.3215\n",
      "Epoch 60/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0282 - mae: 0.1233 - val_loss: 0.1227 - val_mae: 0.3324\n",
      "Epoch 61/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0264 - mae: 0.1188 - val_loss: 0.1184 - val_mae: 0.3259\n",
      "Epoch 62/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0277 - mae: 0.1215 - val_loss: 0.1217 - val_mae: 0.3309\n",
      "Epoch 63/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0257 - mae: 0.1170 - val_loss: 0.1178 - val_mae: 0.3250\n",
      "Epoch 64/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.0276 - mae: 0.1221 - val_loss: 0.1182 - val_mae: 0.3256\n",
      "Epoch 65/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0285 - mae: 0.1254 - val_loss: 0.1263 - val_mae: 0.3376\n",
      "Epoch 66/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.0293 - mae: 0.1250 - val_loss: 0.1222 - val_mae: 0.3316\n",
      "Epoch 67/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.0277 - mae: 0.1227 - val_loss: 0.1247 - val_mae: 0.3352\n",
      "Epoch 68/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0262 - mae: 0.1169 - val_loss: 0.1166 - val_mae: 0.3232\n",
      "Epoch 69/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0277 - mae: 0.1225 - val_loss: 0.1242 - val_mae: 0.3346\n",
      "Epoch 70/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0271 - mae: 0.1203 - val_loss: 0.1234 - val_mae: 0.3333\n",
      "Epoch 71/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0288 - mae: 0.1219 - val_loss: 0.1200 - val_mae: 0.3283\n",
      "Epoch 72/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0289 - mae: 0.1256 - val_loss: 0.1194 - val_mae: 0.3274\n",
      "Epoch 73/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0270 - mae: 0.1211 - val_loss: 0.1162 - val_mae: 0.3225\n",
      "Epoch 74/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0278 - mae: 0.1227 - val_loss: 0.1226 - val_mae: 0.3322\n",
      "Epoch 75/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0250 - mae: 0.1165 - val_loss: 0.1201 - val_mae: 0.3284\n",
      "Epoch 76/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0285 - mae: 0.1229 - val_loss: 0.1229 - val_mae: 0.3327\n",
      "Epoch 77/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0277 - mae: 0.1222 - val_loss: 0.1221 - val_mae: 0.3315\n",
      "Epoch 78/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0265 - mae: 0.1188 - val_loss: 0.1205 - val_mae: 0.3292\n",
      "Epoch 79/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0261 - mae: 0.1204 - val_loss: 0.1214 - val_mae: 0.3305\n",
      "Epoch 80/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0277 - mae: 0.1212 - val_loss: 0.1189 - val_mae: 0.3267\n",
      "Epoch 81/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0270 - mae: 0.1211 - val_loss: 0.1212 - val_mae: 0.3302\n",
      "Epoch 82/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0279 - mae: 0.1226 - val_loss: 0.1253 - val_mae: 0.3362\n",
      "Epoch 83/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.0280 - mae: 0.1221 - val_loss: 0.1226 - val_mae: 0.3323\n",
      "Epoch 84/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0270 - mae: 0.1191 - val_loss: 0.1174 - val_mae: 0.3245\n",
      "Epoch 85/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0276 - mae: 0.1222 - val_loss: 0.1229 - val_mae: 0.3326\n",
      "Epoch 86/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0273 - mae: 0.1188 - val_loss: 0.1247 - val_mae: 0.3353\n",
      "Epoch 87/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0274 - mae: 0.1196 - val_loss: 0.1189 - val_mae: 0.3267\n",
      "Epoch 88/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0279 - mae: 0.1227 - val_loss: 0.1237 - val_mae: 0.3338\n",
      "Epoch 89/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0264 - mae: 0.1196 - val_loss: 0.1179 - val_mae: 0.3252\n",
      "Epoch 90/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0279 - mae: 0.1229 - val_loss: 0.1226 - val_mae: 0.3322\n",
      "Epoch 91/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0270 - mae: 0.1216 - val_loss: 0.1223 - val_mae: 0.3318\n",
      "Epoch 92/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0268 - mae: 0.1212 - val_loss: 0.1190 - val_mae: 0.3269\n",
      "Epoch 93/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0266 - mae: 0.1204 - val_loss: 0.1186 - val_mae: 0.3263\n",
      "Epoch 94/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0265 - mae: 0.1216 - val_loss: 0.1232 - val_mae: 0.3331\n",
      "Epoch 95/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0276 - mae: 0.1214 - val_loss: 0.1208 - val_mae: 0.3295\n",
      "Epoch 96/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0271 - mae: 0.1223 - val_loss: 0.1218 - val_mae: 0.3310\n",
      "Epoch 97/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0265 - mae: 0.1191 - val_loss: 0.1239 - val_mae: 0.3342\n",
      "Epoch 98/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0264 - mae: 0.1172 - val_loss: 0.1238 - val_mae: 0.3340\n",
      "Epoch 99/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0271 - mae: 0.1209 - val_loss: 0.1197 - val_mae: 0.3279\n",
      "Epoch 100/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0257 - mae: 0.1168 - val_loss: 0.1205 - val_mae: 0.3291\n",
      "\u001b[1m178/178\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Done with capsicum_Mandya_daily.csv | MAE=918.38, RMSE=1207.16, R2=-0.0998, MAPE=58.21%, ACC=41.79%\n",
      "\n",
      "ğŸš€ Processing: capsicum_Mysore_daily.csv\n",
      "Epoch 1/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 0.0035 - mae: 0.0443 - val_loss: 0.0042 - val_mae: 0.0445\n",
      "Epoch 2/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 11ms/step - loss: 0.0025 - mae: 0.0386 - val_loss: 0.0043 - val_mae: 0.0457\n",
      "Epoch 3/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 11ms/step - loss: 0.0024 - mae: 0.0378 - val_loss: 0.0037 - val_mae: 0.0412\n",
      "Epoch 4/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - loss: 0.0025 - mae: 0.0389 - val_loss: 0.0044 - val_mae: 0.0460\n",
      "Epoch 5/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0024 - mae: 0.0380 - val_loss: 0.0051 - val_mae: 0.0519\n",
      "Epoch 6/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0025 - mae: 0.0385 - val_loss: 0.0042 - val_mae: 0.0443\n",
      "Epoch 7/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0025 - mae: 0.0385 - val_loss: 0.0044 - val_mae: 0.0459\n",
      "Epoch 8/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - loss: 0.0024 - mae: 0.0383 - val_loss: 0.0045 - val_mae: 0.0466\n",
      "Epoch 9/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0025 - mae: 0.0389 - val_loss: 0.0046 - val_mae: 0.0480\n",
      "Epoch 10/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0025 - mae: 0.0385 - val_loss: 0.0046 - val_mae: 0.0477\n",
      "Epoch 11/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0025 - mae: 0.0384 - val_loss: 0.0040 - val_mae: 0.0433\n",
      "Epoch 12/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0024 - mae: 0.0381 - val_loss: 0.0046 - val_mae: 0.0475\n",
      "Epoch 13/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0024 - mae: 0.0379 - val_loss: 0.0044 - val_mae: 0.0460\n",
      "Epoch 14/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0025 - mae: 0.0388 - val_loss: 0.0047 - val_mae: 0.0486\n",
      "Epoch 15/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0025 - mae: 0.0384 - val_loss: 0.0046 - val_mae: 0.0482\n",
      "Epoch 16/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 13ms/step - loss: 0.0025 - mae: 0.0386 - val_loss: 0.0044 - val_mae: 0.0466\n",
      "Epoch 17/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 12ms/step - loss: 0.0025 - mae: 0.0383 - val_loss: 0.0042 - val_mae: 0.0444\n",
      "Epoch 18/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0025 - mae: 0.0387 - val_loss: 0.0043 - val_mae: 0.0456\n",
      "Epoch 19/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0025 - mae: 0.0383 - val_loss: 0.0044 - val_mae: 0.0464\n",
      "Epoch 20/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0025 - mae: 0.0390 - val_loss: 0.0050 - val_mae: 0.0514\n",
      "Epoch 21/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0024 - mae: 0.0381 - val_loss: 0.0049 - val_mae: 0.0502\n",
      "Epoch 22/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0024 - mae: 0.0381 - val_loss: 0.0040 - val_mae: 0.0433\n",
      "Epoch 23/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0025 - mae: 0.0389 - val_loss: 0.0047 - val_mae: 0.0484\n",
      "Epoch 24/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0024 - mae: 0.0381 - val_loss: 0.0043 - val_mae: 0.0453\n",
      "Epoch 25/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0025 - mae: 0.0382 - val_loss: 0.0047 - val_mae: 0.0489\n",
      "Epoch 26/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0026 - mae: 0.0389 - val_loss: 0.0042 - val_mae: 0.0446\n",
      "Epoch 27/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0025 - mae: 0.0382 - val_loss: 0.0044 - val_mae: 0.0466\n",
      "Epoch 28/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0024 - mae: 0.0381 - val_loss: 0.0045 - val_mae: 0.0467\n",
      "Epoch 29/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0025 - mae: 0.0381 - val_loss: 0.0043 - val_mae: 0.0457\n",
      "Epoch 30/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0024 - mae: 0.0384 - val_loss: 0.0045 - val_mae: 0.0467\n",
      "Epoch 31/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0026 - mae: 0.0389 - val_loss: 0.0047 - val_mae: 0.0483\n",
      "Epoch 32/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0024 - mae: 0.0384 - val_loss: 0.0047 - val_mae: 0.0487\n",
      "Epoch 33/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0025 - mae: 0.0386 - val_loss: 0.0046 - val_mae: 0.0482\n",
      "Epoch 34/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0024 - mae: 0.0382 - val_loss: 0.0042 - val_mae: 0.0447\n",
      "Epoch 35/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0024 - mae: 0.0383 - val_loss: 0.0047 - val_mae: 0.0483\n",
      "Epoch 36/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0025 - mae: 0.0387 - val_loss: 0.0044 - val_mae: 0.0461\n",
      "Epoch 37/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0026 - mae: 0.0394 - val_loss: 0.0052 - val_mae: 0.0528\n",
      "Epoch 38/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0025 - mae: 0.0386 - val_loss: 0.0042 - val_mae: 0.0449\n",
      "Epoch 39/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0024 - mae: 0.0382 - val_loss: 0.0044 - val_mae: 0.0460\n",
      "Epoch 40/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0025 - mae: 0.0387 - val_loss: 0.0046 - val_mae: 0.0475\n",
      "Epoch 41/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0024 - mae: 0.0382 - val_loss: 0.0045 - val_mae: 0.0470\n",
      "Epoch 42/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0025 - mae: 0.0386 - val_loss: 0.0046 - val_mae: 0.0479\n",
      "Epoch 43/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0025 - mae: 0.0387 - val_loss: 0.0045 - val_mae: 0.0471\n",
      "Epoch 44/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0025 - mae: 0.0382 - val_loss: 0.0041 - val_mae: 0.0439\n",
      "Epoch 45/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0024 - mae: 0.0377 - val_loss: 0.0045 - val_mae: 0.0471\n",
      "Epoch 46/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0026 - mae: 0.0390 - val_loss: 0.0043 - val_mae: 0.0451\n",
      "Epoch 47/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0024 - mae: 0.0378 - val_loss: 0.0041 - val_mae: 0.0442\n",
      "Epoch 48/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0025 - mae: 0.0385 - val_loss: 0.0044 - val_mae: 0.0465\n",
      "Epoch 49/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0025 - mae: 0.0385 - val_loss: 0.0048 - val_mae: 0.0499\n",
      "Epoch 50/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0025 - mae: 0.0387 - val_loss: 0.0042 - val_mae: 0.0445\n",
      "Epoch 51/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0025 - mae: 0.0387 - val_loss: 0.0046 - val_mae: 0.0480\n",
      "Epoch 52/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0026 - mae: 0.0389 - val_loss: 0.0043 - val_mae: 0.0454\n",
      "Epoch 53/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0025 - mae: 0.0388 - val_loss: 0.0044 - val_mae: 0.0461\n",
      "Epoch 54/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0025 - mae: 0.0391 - val_loss: 0.0050 - val_mae: 0.0511\n",
      "Epoch 55/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0025 - mae: 0.0382 - val_loss: 0.0052 - val_mae: 0.0527\n",
      "Epoch 56/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0026 - mae: 0.0390 - val_loss: 0.0046 - val_mae: 0.0476\n",
      "Epoch 57/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0025 - mae: 0.0384 - val_loss: 0.0044 - val_mae: 0.0459\n",
      "Epoch 58/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0025 - mae: 0.0386 - val_loss: 0.0039 - val_mae: 0.0423\n",
      "Epoch 59/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0026 - mae: 0.0389 - val_loss: 0.0043 - val_mae: 0.0454\n",
      "Epoch 60/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0025 - mae: 0.0385 - val_loss: 0.0042 - val_mae: 0.0443\n",
      "Epoch 61/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0024 - mae: 0.0380 - val_loss: 0.0041 - val_mae: 0.0436\n",
      "Epoch 62/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0025 - mae: 0.0382 - val_loss: 0.0045 - val_mae: 0.0474\n",
      "Epoch 63/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0025 - mae: 0.0387 - val_loss: 0.0049 - val_mae: 0.0500\n",
      "Epoch 64/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0025 - mae: 0.0385 - val_loss: 0.0041 - val_mae: 0.0437\n",
      "Epoch 65/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0024 - mae: 0.0379 - val_loss: 0.0047 - val_mae: 0.0485\n",
      "Epoch 66/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0025 - mae: 0.0383 - val_loss: 0.0042 - val_mae: 0.0449\n",
      "Epoch 67/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0025 - mae: 0.0388 - val_loss: 0.0042 - val_mae: 0.0445\n",
      "Epoch 68/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0024 - mae: 0.0380 - val_loss: 0.0043 - val_mae: 0.0457\n",
      "Epoch 69/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0024 - mae: 0.0380 - val_loss: 0.0048 - val_mae: 0.0493\n",
      "Epoch 70/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0024 - mae: 0.0376 - val_loss: 0.0047 - val_mae: 0.0483\n",
      "Epoch 71/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0025 - mae: 0.0384 - val_loss: 0.0045 - val_mae: 0.0473\n",
      "Epoch 72/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0025 - mae: 0.0386 - val_loss: 0.0047 - val_mae: 0.0485\n",
      "Epoch 73/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0024 - mae: 0.0381 - val_loss: 0.0040 - val_mae: 0.0433\n",
      "Epoch 74/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0025 - mae: 0.0383 - val_loss: 0.0045 - val_mae: 0.0467\n",
      "Epoch 75/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0024 - mae: 0.0382 - val_loss: 0.0044 - val_mae: 0.0465\n",
      "Epoch 76/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0024 - mae: 0.0377 - val_loss: 0.0044 - val_mae: 0.0458\n",
      "Epoch 77/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0024 - mae: 0.0379 - val_loss: 0.0041 - val_mae: 0.0440\n",
      "Epoch 78/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0025 - mae: 0.0383 - val_loss: 0.0040 - val_mae: 0.0434\n",
      "Epoch 79/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0025 - mae: 0.0382 - val_loss: 0.0042 - val_mae: 0.0446\n",
      "Epoch 80/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0024 - mae: 0.0378 - val_loss: 0.0040 - val_mae: 0.0434\n",
      "Epoch 81/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0025 - mae: 0.0387 - val_loss: 0.0048 - val_mae: 0.0496\n",
      "Epoch 82/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0025 - mae: 0.0389 - val_loss: 0.0045 - val_mae: 0.0473\n",
      "Epoch 83/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0025 - mae: 0.0384 - val_loss: 0.0047 - val_mae: 0.0484\n",
      "Epoch 84/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0025 - mae: 0.0387 - val_loss: 0.0048 - val_mae: 0.0498\n",
      "Epoch 85/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0025 - mae: 0.0385 - val_loss: 0.0043 - val_mae: 0.0456\n",
      "Epoch 86/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0026 - mae: 0.0390 - val_loss: 0.0049 - val_mae: 0.0501\n",
      "Epoch 87/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0025 - mae: 0.0380 - val_loss: 0.0042 - val_mae: 0.0445\n",
      "Epoch 88/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0025 - mae: 0.0385 - val_loss: 0.0042 - val_mae: 0.0448\n",
      "Epoch 89/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0025 - mae: 0.0388 - val_loss: 0.0043 - val_mae: 0.0450\n",
      "Epoch 90/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0024 - mae: 0.0383 - val_loss: 0.0046 - val_mae: 0.0475\n",
      "Epoch 91/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0025 - mae: 0.0383 - val_loss: 0.0047 - val_mae: 0.0487\n",
      "Epoch 92/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0025 - mae: 0.0385 - val_loss: 0.0044 - val_mae: 0.0460\n",
      "Epoch 93/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0025 - mae: 0.0386 - val_loss: 0.0041 - val_mae: 0.0436\n",
      "Epoch 94/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0025 - mae: 0.0384 - val_loss: 0.0039 - val_mae: 0.0425\n",
      "Epoch 95/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0025 - mae: 0.0386 - val_loss: 0.0050 - val_mae: 0.0510\n",
      "Epoch 96/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0025 - mae: 0.0388 - val_loss: 0.0044 - val_mae: 0.0464\n",
      "Epoch 97/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0025 - mae: 0.0385 - val_loss: 0.0046 - val_mae: 0.0481\n",
      "Epoch 98/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0024 - mae: 0.0379 - val_loss: 0.0042 - val_mae: 0.0447\n",
      "Epoch 99/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0024 - mae: 0.0379 - val_loss: 0.0047 - val_mae: 0.0487\n",
      "Epoch 100/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0025 - mae: 0.0386 - val_loss: 0.0046 - val_mae: 0.0478\n",
      "\u001b[1m319/319\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Done with capsicum_Mysore_daily.csv | MAE=950.66, RMSE=1280.25, R2=-0.0431, MAPE=37.81%, ACC=62.19%\n",
      "\n",
      "ğŸš€ Processing: capsicum_Shimoga_daily.csv\n",
      "Epoch 1/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step - loss: 0.6588 - mae: 0.7959 - val_loss: 0.7209 - val_mae: 0.8342\n",
      "Epoch 2/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - loss: 0.6548 - mae: 0.7934 - val_loss: 0.7167 - val_mae: 0.8317\n",
      "Epoch 3/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - loss: 0.6508 - mae: 0.7909 - val_loss: 0.7125 - val_mae: 0.8291\n",
      "Epoch 4/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - loss: 0.6468 - mae: 0.7883 - val_loss: 0.7083 - val_mae: 0.8266\n",
      "Epoch 5/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - loss: 0.6428 - mae: 0.7858 - val_loss: 0.7041 - val_mae: 0.8241\n",
      "Epoch 6/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - loss: 0.6388 - mae: 0.7832 - val_loss: 0.7000 - val_mae: 0.8215\n",
      "Epoch 7/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.6348 - mae: 0.7807 - val_loss: 0.6958 - val_mae: 0.8190\n",
      "Epoch 8/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - loss: 0.6309 - mae: 0.7782 - val_loss: 0.6916 - val_mae: 0.8165\n",
      "Epoch 9/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 0.6269 - mae: 0.7756 - val_loss: 0.6875 - val_mae: 0.8139\n",
      "Epoch 10/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - loss: 0.6230 - mae: 0.7731 - val_loss: 0.6834 - val_mae: 0.8114\n",
      "Epoch 11/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - loss: 0.6191 - mae: 0.7705 - val_loss: 0.6792 - val_mae: 0.8088\n",
      "Epoch 12/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 0.6151 - mae: 0.7680 - val_loss: 0.6751 - val_mae: 0.8063\n",
      "Epoch 13/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - loss: 0.6112 - mae: 0.7655 - val_loss: 0.6710 - val_mae: 0.8037\n",
      "Epoch 14/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - loss: 0.6074 - mae: 0.7629 - val_loss: 0.6670 - val_mae: 0.8012\n",
      "Epoch 15/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - loss: 0.6035 - mae: 0.7604 - val_loss: 0.6629 - val_mae: 0.7986\n",
      "Epoch 16/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - loss: 0.5996 - mae: 0.7578 - val_loss: 0.6588 - val_mae: 0.7961\n",
      "Epoch 17/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 0.5958 - mae: 0.7553 - val_loss: 0.6548 - val_mae: 0.7936\n",
      "Epoch 18/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - loss: 0.5919 - mae: 0.7527 - val_loss: 0.6507 - val_mae: 0.7910\n",
      "Epoch 19/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.5881 - mae: 0.7502 - val_loss: 0.6467 - val_mae: 0.7885\n",
      "Epoch 20/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 0.5843 - mae: 0.7476 - val_loss: 0.6427 - val_mae: 0.7859\n",
      "Epoch 21/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - loss: 0.5805 - mae: 0.7451 - val_loss: 0.6387 - val_mae: 0.7834\n",
      "Epoch 22/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.5767 - mae: 0.7425 - val_loss: 0.6347 - val_mae: 0.7808\n",
      "Epoch 23/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - loss: 0.5729 - mae: 0.7400 - val_loss: 0.6307 - val_mae: 0.7783\n",
      "Epoch 24/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - loss: 0.5691 - mae: 0.7374 - val_loss: 0.6268 - val_mae: 0.7757\n",
      "Epoch 25/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.5654 - mae: 0.7349 - val_loss: 0.6228 - val_mae: 0.7732\n",
      "Epoch 26/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.5617 - mae: 0.7324 - val_loss: 0.6189 - val_mae: 0.7706\n",
      "Epoch 27/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.5579 - mae: 0.7298 - val_loss: 0.6150 - val_mae: 0.7681\n",
      "Epoch 28/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 0.5542 - mae: 0.7273 - val_loss: 0.6111 - val_mae: 0.7655\n",
      "Epoch 29/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.5505 - mae: 0.7247 - val_loss: 0.6072 - val_mae: 0.7630\n",
      "Epoch 30/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - loss: 0.5468 - mae: 0.7222 - val_loss: 0.6033 - val_mae: 0.7604\n",
      "Epoch 31/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - loss: 0.5432 - mae: 0.7196 - val_loss: 0.5994 - val_mae: 0.7579\n",
      "Epoch 32/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - loss: 0.5395 - mae: 0.7171 - val_loss: 0.5956 - val_mae: 0.7553\n",
      "Epoch 33/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - loss: 0.5359 - mae: 0.7145 - val_loss: 0.5917 - val_mae: 0.7528\n",
      "Epoch 34/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 0.5322 - mae: 0.7120 - val_loss: 0.5879 - val_mae: 0.7503\n",
      "Epoch 35/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 0.5286 - mae: 0.7094 - val_loss: 0.5841 - val_mae: 0.7477\n",
      "Epoch 36/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.5250 - mae: 0.7069 - val_loss: 0.5803 - val_mae: 0.7452\n",
      "Epoch 37/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - loss: 0.5214 - mae: 0.7043 - val_loss: 0.5765 - val_mae: 0.7426\n",
      "Epoch 38/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - loss: 0.5178 - mae: 0.7018 - val_loss: 0.5728 - val_mae: 0.7401\n",
      "Epoch 39/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - loss: 0.5143 - mae: 0.6993 - val_loss: 0.5690 - val_mae: 0.7375\n",
      "Epoch 40/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - loss: 0.5107 - mae: 0.6967 - val_loss: 0.5653 - val_mae: 0.7350\n",
      "Epoch 41/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - loss: 0.5072 - mae: 0.6942 - val_loss: 0.5615 - val_mae: 0.7325\n",
      "Epoch 42/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.5037 - mae: 0.6916 - val_loss: 0.5578 - val_mae: 0.7299\n",
      "Epoch 43/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step - loss: 0.5002 - mae: 0.6891 - val_loss: 0.5541 - val_mae: 0.7274\n",
      "Epoch 44/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - loss: 0.4967 - mae: 0.6866 - val_loss: 0.5504 - val_mae: 0.7248\n",
      "Epoch 45/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - loss: 0.4932 - mae: 0.6840 - val_loss: 0.5468 - val_mae: 0.7223\n",
      "Epoch 46/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.4897 - mae: 0.6815 - val_loss: 0.5431 - val_mae: 0.7198\n",
      "Epoch 47/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.4863 - mae: 0.6789 - val_loss: 0.5395 - val_mae: 0.7172\n",
      "Epoch 48/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.4828 - mae: 0.6764 - val_loss: 0.5358 - val_mae: 0.7147\n",
      "Epoch 49/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 0.4794 - mae: 0.6739 - val_loss: 0.5322 - val_mae: 0.7122\n",
      "Epoch 50/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - loss: 0.4760 - mae: 0.6713 - val_loss: 0.5286 - val_mae: 0.7096\n",
      "Epoch 51/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 0.4726 - mae: 0.6688 - val_loss: 0.5250 - val_mae: 0.7071\n",
      "Epoch 52/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - loss: 0.4692 - mae: 0.6663 - val_loss: 0.5215 - val_mae: 0.7046\n",
      "Epoch 53/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 0.4659 - mae: 0.6637 - val_loss: 0.5179 - val_mae: 0.7020\n",
      "Epoch 54/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - loss: 0.4625 - mae: 0.6612 - val_loss: 0.5144 - val_mae: 0.6995\n",
      "Epoch 55/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 0.4592 - mae: 0.6587 - val_loss: 0.5108 - val_mae: 0.6970\n",
      "Epoch 56/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - loss: 0.4559 - mae: 0.6562 - val_loss: 0.5073 - val_mae: 0.6945\n",
      "Epoch 57/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - loss: 0.4526 - mae: 0.6536 - val_loss: 0.5038 - val_mae: 0.6919\n",
      "Epoch 58/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.4493 - mae: 0.6511 - val_loss: 0.5003 - val_mae: 0.6894\n",
      "Epoch 59/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - loss: 0.4460 - mae: 0.6486 - val_loss: 0.4969 - val_mae: 0.6869\n",
      "Epoch 60/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - loss: 0.4427 - mae: 0.6461 - val_loss: 0.4934 - val_mae: 0.6844\n",
      "Epoch 61/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - loss: 0.4395 - mae: 0.6436 - val_loss: 0.4900 - val_mae: 0.6819\n",
      "Epoch 62/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 0.4362 - mae: 0.6410 - val_loss: 0.4865 - val_mae: 0.6793\n",
      "Epoch 63/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 0.4330 - mae: 0.6385 - val_loss: 0.4831 - val_mae: 0.6768\n",
      "Epoch 64/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 0.4298 - mae: 0.6360 - val_loss: 0.4797 - val_mae: 0.6743\n",
      "Epoch 65/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - loss: 0.4266 - mae: 0.6335 - val_loss: 0.4764 - val_mae: 0.6718\n",
      "Epoch 66/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.4235 - mae: 0.6310 - val_loss: 0.4730 - val_mae: 0.6693\n",
      "Epoch 67/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.4203 - mae: 0.6285 - val_loss: 0.4696 - val_mae: 0.6668\n",
      "Epoch 68/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.4171 - mae: 0.6260 - val_loss: 0.4663 - val_mae: 0.6643\n",
      "Epoch 69/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - loss: 0.4140 - mae: 0.6235 - val_loss: 0.4630 - val_mae: 0.6618\n",
      "Epoch 70/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - loss: 0.4109 - mae: 0.6209 - val_loss: 0.4597 - val_mae: 0.6593\n",
      "Epoch 71/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.4078 - mae: 0.6184 - val_loss: 0.4564 - val_mae: 0.6568\n",
      "Epoch 72/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - loss: 0.4047 - mae: 0.6159 - val_loss: 0.4531 - val_mae: 0.6543\n",
      "Epoch 73/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.4016 - mae: 0.6134 - val_loss: 0.4498 - val_mae: 0.6518\n",
      "Epoch 74/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - loss: 0.3986 - mae: 0.6109 - val_loss: 0.4466 - val_mae: 0.6493\n",
      "Epoch 75/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - loss: 0.3955 - mae: 0.6084 - val_loss: 0.4433 - val_mae: 0.6468\n",
      "Epoch 76/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - loss: 0.3925 - mae: 0.6059 - val_loss: 0.4401 - val_mae: 0.6443\n",
      "Epoch 77/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - loss: 0.3895 - mae: 0.6035 - val_loss: 0.4369 - val_mae: 0.6418\n",
      "Epoch 78/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - loss: 0.3865 - mae: 0.6010 - val_loss: 0.4337 - val_mae: 0.6393\n",
      "Epoch 79/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.3835 - mae: 0.5985 - val_loss: 0.4306 - val_mae: 0.6368\n",
      "Epoch 80/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 0.3805 - mae: 0.5960 - val_loss: 0.4274 - val_mae: 0.6343\n",
      "Epoch 81/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.3776 - mae: 0.5935 - val_loss: 0.4243 - val_mae: 0.6318\n",
      "Epoch 82/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - loss: 0.3746 - mae: 0.5910 - val_loss: 0.4211 - val_mae: 0.6294\n",
      "Epoch 83/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - loss: 0.3717 - mae: 0.5885 - val_loss: 0.4180 - val_mae: 0.6269\n",
      "Epoch 84/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - loss: 0.3688 - mae: 0.5861 - val_loss: 0.4149 - val_mae: 0.6244\n",
      "Epoch 85/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 0.3659 - mae: 0.5836 - val_loss: 0.4118 - val_mae: 0.6219\n",
      "Epoch 86/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 0.3630 - mae: 0.5811 - val_loss: 0.4088 - val_mae: 0.6194\n",
      "Epoch 87/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - loss: 0.3601 - mae: 0.5786 - val_loss: 0.4057 - val_mae: 0.6170\n",
      "Epoch 88/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 0.3573 - mae: 0.5762 - val_loss: 0.4027 - val_mae: 0.6145\n",
      "Epoch 89/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.3544 - mae: 0.5737 - val_loss: 0.3996 - val_mae: 0.6120\n",
      "Epoch 90/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 0.3516 - mae: 0.5712 - val_loss: 0.3966 - val_mae: 0.6096\n",
      "Epoch 91/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.3488 - mae: 0.5688 - val_loss: 0.3936 - val_mae: 0.6071\n",
      "Epoch 92/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - loss: 0.3460 - mae: 0.5663 - val_loss: 0.3906 - val_mae: 0.6047\n",
      "Epoch 93/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.3432 - mae: 0.5638 - val_loss: 0.3877 - val_mae: 0.6022\n",
      "Epoch 94/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - loss: 0.3405 - mae: 0.5614 - val_loss: 0.3847 - val_mae: 0.5997\n",
      "Epoch 95/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.3377 - mae: 0.5589 - val_loss: 0.3818 - val_mae: 0.5973\n",
      "Epoch 96/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 0.3350 - mae: 0.5565 - val_loss: 0.3789 - val_mae: 0.5948\n",
      "Epoch 97/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 0.3323 - mae: 0.5540 - val_loss: 0.3760 - val_mae: 0.5924\n",
      "Epoch 98/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - loss: 0.3296 - mae: 0.5516 - val_loss: 0.3731 - val_mae: 0.5899\n",
      "Epoch 99/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 0.3269 - mae: 0.5491 - val_loss: 0.3702 - val_mae: 0.5875\n",
      "Epoch 100/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 0.3242 - mae: 0.5467 - val_loss: 0.3673 - val_mae: 0.5851\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Done with capsicum_Shimoga_daily.csv | MAE=360.39, RMSE=375.08, R2=-12.0262, MAPE=20.04%, ACC=79.96%\n",
      "\n",
      "ğŸš€ Processing: capsicum_Udupi_daily.csv\n",
      "Epoch 1/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 12ms/step - loss: 0.0379 - mae: 0.1490 - val_loss: 0.0656 - val_mae: 0.2152\n",
      "Epoch 2/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0225 - mae: 0.1254 - val_loss: 0.0640 - val_mae: 0.2118\n",
      "Epoch 3/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0227 - mae: 0.1263 - val_loss: 0.0641 - val_mae: 0.2120\n",
      "Epoch 4/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0228 - mae: 0.1261 - val_loss: 0.0622 - val_mae: 0.2077\n",
      "Epoch 5/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0220 - mae: 0.1242 - val_loss: 0.0567 - val_mae: 0.1950\n",
      "Epoch 6/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0223 - mae: 0.1265 - val_loss: 0.0595 - val_mae: 0.2016\n",
      "Epoch 7/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0222 - mae: 0.1246 - val_loss: 0.0695 - val_mae: 0.2236\n",
      "Epoch 8/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0224 - mae: 0.1248 - val_loss: 0.0583 - val_mae: 0.1987\n",
      "Epoch 9/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0216 - mae: 0.1233 - val_loss: 0.0603 - val_mae: 0.2035\n",
      "Epoch 10/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0226 - mae: 0.1261 - val_loss: 0.0619 - val_mae: 0.2070\n",
      "Epoch 11/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0226 - mae: 0.1270 - val_loss: 0.0599 - val_mae: 0.2025\n",
      "Epoch 12/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0224 - mae: 0.1259 - val_loss: 0.0613 - val_mae: 0.2057\n",
      "Epoch 13/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0227 - mae: 0.1267 - val_loss: 0.0602 - val_mae: 0.2031\n",
      "Epoch 14/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0225 - mae: 0.1262 - val_loss: 0.0633 - val_mae: 0.2102\n",
      "Epoch 15/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0223 - mae: 0.1259 - val_loss: 0.0637 - val_mae: 0.2110\n",
      "Epoch 16/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0224 - mae: 0.1262 - val_loss: 0.0649 - val_mae: 0.2138\n",
      "Epoch 17/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0236 - mae: 0.1280 - val_loss: 0.0657 - val_mae: 0.2154\n",
      "Epoch 18/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0219 - mae: 0.1239 - val_loss: 0.0671 - val_mae: 0.2185\n",
      "Epoch 19/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0234 - mae: 0.1288 - val_loss: 0.0669 - val_mae: 0.2182\n",
      "Epoch 20/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0223 - mae: 0.1262 - val_loss: 0.0624 - val_mae: 0.2081\n",
      "Epoch 21/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0220 - mae: 0.1245 - val_loss: 0.0641 - val_mae: 0.2119\n",
      "Epoch 22/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0222 - mae: 0.1260 - val_loss: 0.0612 - val_mae: 0.2056\n",
      "Epoch 23/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0236 - mae: 0.1282 - val_loss: 0.0686 - val_mae: 0.2217\n",
      "Epoch 24/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0223 - mae: 0.1248 - val_loss: 0.0623 - val_mae: 0.2079\n",
      "Epoch 25/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0228 - mae: 0.1282 - val_loss: 0.0562 - val_mae: 0.1938\n",
      "Epoch 26/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0226 - mae: 0.1267 - val_loss: 0.0640 - val_mae: 0.2118\n",
      "Epoch 27/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0225 - mae: 0.1259 - val_loss: 0.0629 - val_mae: 0.2092\n",
      "Epoch 28/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0228 - mae: 0.1261 - val_loss: 0.0626 - val_mae: 0.2087\n",
      "Epoch 29/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0225 - mae: 0.1248 - val_loss: 0.0574 - val_mae: 0.1966\n",
      "Epoch 30/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0223 - mae: 0.1256 - val_loss: 0.0604 - val_mae: 0.2036\n",
      "Epoch 31/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0232 - mae: 0.1275 - val_loss: 0.0716 - val_mae: 0.2280\n",
      "Epoch 32/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0225 - mae: 0.1248 - val_loss: 0.0645 - val_mae: 0.2129\n",
      "Epoch 33/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0222 - mae: 0.1243 - val_loss: 0.0639 - val_mae: 0.2114\n",
      "Epoch 34/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0225 - mae: 0.1252 - val_loss: 0.0638 - val_mae: 0.2114\n",
      "Epoch 35/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0225 - mae: 0.1259 - val_loss: 0.0646 - val_mae: 0.2132\n",
      "Epoch 36/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0227 - mae: 0.1266 - val_loss: 0.0581 - val_mae: 0.1983\n",
      "Epoch 37/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0225 - mae: 0.1265 - val_loss: 0.0613 - val_mae: 0.2057\n",
      "Epoch 38/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0223 - mae: 0.1265 - val_loss: 0.0696 - val_mae: 0.2237\n",
      "Epoch 39/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0216 - mae: 0.1223 - val_loss: 0.0638 - val_mae: 0.2114\n",
      "Epoch 40/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0217 - mae: 0.1243 - val_loss: 0.0592 - val_mae: 0.2008\n",
      "Epoch 41/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0219 - mae: 0.1247 - val_loss: 0.0635 - val_mae: 0.2107\n",
      "Epoch 42/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0229 - mae: 0.1261 - val_loss: 0.0682 - val_mae: 0.2208\n",
      "Epoch 43/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0233 - mae: 0.1275 - val_loss: 0.0659 - val_mae: 0.2159\n",
      "Epoch 44/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0216 - mae: 0.1237 - val_loss: 0.0620 - val_mae: 0.2072\n",
      "Epoch 45/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0226 - mae: 0.1265 - val_loss: 0.0587 - val_mae: 0.1997\n",
      "Epoch 46/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0227 - mae: 0.1272 - val_loss: 0.0618 - val_mae: 0.2068\n",
      "Epoch 47/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0225 - mae: 0.1261 - val_loss: 0.0603 - val_mae: 0.2033\n",
      "Epoch 48/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0229 - mae: 0.1270 - val_loss: 0.0618 - val_mae: 0.2068\n",
      "Epoch 49/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0224 - mae: 0.1260 - val_loss: 0.0646 - val_mae: 0.2130\n",
      "Epoch 50/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0226 - mae: 0.1269 - val_loss: 0.0650 - val_mae: 0.2139\n",
      "Epoch 51/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0225 - mae: 0.1259 - val_loss: 0.0618 - val_mae: 0.2069\n",
      "Epoch 52/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0238 - mae: 0.1288 - val_loss: 0.0638 - val_mae: 0.2114\n",
      "Epoch 53/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0227 - mae: 0.1261 - val_loss: 0.0614 - val_mae: 0.2059\n",
      "Epoch 54/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0231 - mae: 0.1274 - val_loss: 0.0555 - val_mae: 0.1921\n",
      "Epoch 55/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0229 - mae: 0.1284 - val_loss: 0.0632 - val_mae: 0.2100\n",
      "Epoch 56/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0228 - mae: 0.1257 - val_loss: 0.0626 - val_mae: 0.2085\n",
      "Epoch 57/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0237 - mae: 0.1298 - val_loss: 0.0684 - val_mae: 0.2212\n",
      "Epoch 58/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0221 - mae: 0.1245 - val_loss: 0.0609 - val_mae: 0.2048\n",
      "Epoch 59/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0227 - mae: 0.1265 - val_loss: 0.0612 - val_mae: 0.2054\n",
      "Epoch 60/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0222 - mae: 0.1255 - val_loss: 0.0621 - val_mae: 0.2075\n",
      "Epoch 61/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0229 - mae: 0.1262 - val_loss: 0.0580 - val_mae: 0.1979\n",
      "Epoch 62/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0225 - mae: 0.1262 - val_loss: 0.0618 - val_mae: 0.2068\n",
      "Epoch 63/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0221 - mae: 0.1263 - val_loss: 0.0661 - val_mae: 0.2164\n",
      "Epoch 64/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0229 - mae: 0.1269 - val_loss: 0.0675 - val_mae: 0.2194\n",
      "Epoch 65/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0227 - mae: 0.1258 - val_loss: 0.0599 - val_mae: 0.2024\n",
      "Epoch 66/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0219 - mae: 0.1243 - val_loss: 0.0628 - val_mae: 0.2091\n",
      "Epoch 67/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0233 - mae: 0.1280 - val_loss: 0.0663 - val_mae: 0.2168\n",
      "Epoch 68/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0228 - mae: 0.1256 - val_loss: 0.0676 - val_mae: 0.2196\n",
      "Epoch 69/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0223 - mae: 0.1253 - val_loss: 0.0620 - val_mae: 0.2072\n",
      "Epoch 70/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0226 - mae: 0.1263 - val_loss: 0.0625 - val_mae: 0.2083\n",
      "Epoch 71/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0225 - mae: 0.1250 - val_loss: 0.0599 - val_mae: 0.2026\n",
      "Epoch 72/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0229 - mae: 0.1254 - val_loss: 0.0629 - val_mae: 0.2093\n",
      "Epoch 73/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0227 - mae: 0.1273 - val_loss: 0.0615 - val_mae: 0.2061\n",
      "Epoch 74/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0227 - mae: 0.1274 - val_loss: 0.0649 - val_mae: 0.2137\n",
      "Epoch 75/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0223 - mae: 0.1250 - val_loss: 0.0609 - val_mae: 0.2048\n",
      "Epoch 76/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0229 - mae: 0.1270 - val_loss: 0.0654 - val_mae: 0.2149\n",
      "Epoch 77/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0228 - mae: 0.1271 - val_loss: 0.0597 - val_mae: 0.2021\n",
      "Epoch 78/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0222 - mae: 0.1252 - val_loss: 0.0636 - val_mae: 0.2109\n",
      "Epoch 79/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0219 - mae: 0.1242 - val_loss: 0.0610 - val_mae: 0.2049\n",
      "Epoch 80/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0221 - mae: 0.1249 - val_loss: 0.0620 - val_mae: 0.2072\n",
      "Epoch 81/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0218 - mae: 0.1240 - val_loss: 0.0602 - val_mae: 0.2031\n",
      "Epoch 82/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0241 - mae: 0.1309 - val_loss: 0.0648 - val_mae: 0.2135\n",
      "Epoch 83/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0218 - mae: 0.1244 - val_loss: 0.0621 - val_mae: 0.2075\n",
      "Epoch 84/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0236 - mae: 0.1295 - val_loss: 0.0676 - val_mae: 0.2196\n",
      "Epoch 85/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0226 - mae: 0.1255 - val_loss: 0.0643 - val_mae: 0.2125\n",
      "Epoch 86/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0223 - mae: 0.1251 - val_loss: 0.0662 - val_mae: 0.2166\n",
      "Epoch 87/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0226 - mae: 0.1265 - val_loss: 0.0654 - val_mae: 0.2147\n",
      "Epoch 88/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0232 - mae: 0.1269 - val_loss: 0.0589 - val_mae: 0.2002\n",
      "Epoch 89/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0218 - mae: 0.1238 - val_loss: 0.0619 - val_mae: 0.2071\n",
      "Epoch 90/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0228 - mae: 0.1281 - val_loss: 0.0626 - val_mae: 0.2086\n",
      "Epoch 91/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0219 - mae: 0.1252 - val_loss: 0.0659 - val_mae: 0.2160\n",
      "Epoch 92/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0232 - mae: 0.1277 - val_loss: 0.0651 - val_mae: 0.2142\n",
      "Epoch 93/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0227 - mae: 0.1250 - val_loss: 0.0608 - val_mae: 0.2045\n",
      "Epoch 94/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0226 - mae: 0.1267 - val_loss: 0.0692 - val_mae: 0.2230\n",
      "Epoch 95/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0221 - mae: 0.1243 - val_loss: 0.0589 - val_mae: 0.2002\n",
      "Epoch 96/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0229 - mae: 0.1281 - val_loss: 0.0672 - val_mae: 0.2187\n",
      "Epoch 97/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0222 - mae: 0.1238 - val_loss: 0.0606 - val_mae: 0.2041\n",
      "Epoch 98/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0226 - mae: 0.1256 - val_loss: 0.0587 - val_mae: 0.1997\n",
      "Epoch 99/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0224 - mae: 0.1268 - val_loss: 0.0668 - val_mae: 0.2178\n",
      "Epoch 100/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0224 - mae: 0.1263 - val_loss: 0.0653 - val_mae: 0.2145\n",
      "\u001b[1m155/155\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Done with capsicum_Udupi_daily.csv | MAE=1257.8, RMSE=1551.64, R2=-0.0763, MAPE=39.24%, ACC=60.76%\n",
      "\n",
      "ğŸ“Š Metrics saved to tat_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# -----------------------------\n",
    "# Output directories\n",
    "# -----------------------------\n",
    "input_folder = \"dataset\"\n",
    "output_models = \"tat_output_models\"\n",
    "output_csv = \"tat_output_csv\"\n",
    "output_graphs = \"tat_output_graphs\"\n",
    "output_logs = \"tat_output_logs\"\n",
    "metrics_file = \"tat_metrics.csv\"\n",
    "\n",
    "os.makedirs(output_models, exist_ok=True)\n",
    "os.makedirs(output_csv, exist_ok=True)\n",
    "os.makedirs(output_graphs, exist_ok=True)\n",
    "os.makedirs(output_logs, exist_ok=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Create dataset sequences\n",
    "# -----------------------------\n",
    "def create_dataset(data, look_back=30):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - look_back):\n",
    "        X.append(data[i:i + look_back, 0])\n",
    "        y.append(data[i + look_back, 0])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# -----------------------------\n",
    "# Build TAT Model (MHA + Transformer)\n",
    "# -----------------------------\n",
    "def build_tat_model(input_shape, d_model=64, num_heads=4, ff_dim=128, dropout_rate=0.2):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Multi-Head Self Attention\n",
    "    x = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)(inputs, inputs)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x + inputs)\n",
    "\n",
    "    # Feed Forward Network\n",
    "    ff = layers.Dense(ff_dim, activation='relu')(x)\n",
    "    ff = layers.Dense(input_shape[1])(ff)\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x + ff)\n",
    "\n",
    "    # Global pooling + output\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    outputs = layers.Dense(1)(x)\n",
    "\n",
    "    model = models.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "                  loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# -----------------------------\n",
    "# Safe date parser\n",
    "# -----------------------------\n",
    "def parse_dates_safe(date_series):\n",
    "    return pd.to_datetime(date_series, dayfirst=True, errors='coerce')\n",
    "\n",
    "# -----------------------------\n",
    "# Metrics list\n",
    "# -----------------------------\n",
    "metrics_list = []\n",
    "\n",
    "# -----------------------------\n",
    "# Process ALL CSV Files\n",
    "# -----------------------------\n",
    "look_back = 30\n",
    "\n",
    "for file in os.listdir(input_folder):\n",
    "    if file.endswith(\".csv\"):\n",
    "\n",
    "        print(f\"ğŸš€ Processing: {file}\")\n",
    "        file_path = os.path.join(input_folder, file)\n",
    "\n",
    "        # Load and clean\n",
    "        df = pd.read_csv(file_path)\n",
    "        df['Date'] = parse_dates_safe(df['Date'])\n",
    "        df = df.dropna(subset=['Date'])\n",
    "        df = df.sort_values('Date')\n",
    "\n",
    "        # Fill and round prices\n",
    "        df['Average Price'].fillna(df['Average Price'].mean(), inplace=True)\n",
    "        df['Average Price'] = df['Average Price'].round(2)\n",
    "\n",
    "        # Moving averages\n",
    "        df['MA_7'] = df['Average Price'].rolling(7).mean()\n",
    "        df['MA_30'] = df['Average Price'].rolling(30).mean()\n",
    "        df['MA_7'].fillna(df['MA_7'].mean(), inplace=True)\n",
    "        df['MA_30'].fillna(df['MA_30'].mean(), inplace=True)\n",
    "\n",
    "        # -----------------------------\n",
    "        # Padding with AVERAGE for short series\n",
    "        # Ensure at least (look_back + 1) rows\n",
    "        # -----------------------------\n",
    "        if len(df) < look_back + 1:\n",
    "            missing = (look_back + 1) - len(df)\n",
    "            avg_price = df['Average Price'].mean()\n",
    "            min_date = df['Date'].min()\n",
    "\n",
    "            pad_df = pd.DataFrame({\n",
    "                'Date': [min_date] * missing,\n",
    "                'Average Price': [avg_price] * missing,\n",
    "                'MA_7': [avg_price] * missing,\n",
    "                'MA_30': [avg_price] * missing\n",
    "            })\n",
    "\n",
    "            df = pd.concat([pad_df, df], ignore_index=True)\n",
    "            print(f\"ğŸ“Œ Padded {missing} rows with avg={avg_price:.2f} for {file} to reach {look_back + 1} rows.\")\n",
    "\n",
    "        # Scale values\n",
    "        values = df[['Average Price']].values.astype('float32')\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        scaled_values = scaler.fit_transform(values)\n",
    "\n",
    "        # Create sequences\n",
    "        X, y = create_dataset(scaled_values, look_back)\n",
    "\n",
    "        # Just in case: if still 0 samples, create 1 dummy sample\n",
    "        if len(X) == 0:\n",
    "            print(f\"âš ï¸ Still not enough sequences for {file}, creating one dummy sample.\")\n",
    "            pad = np.repeat(scaled_values.mean(), look_back + 1).reshape(-1, 1)\n",
    "            X = np.array([pad[:look_back, 0]])\n",
    "            y = np.array([pad[look_back, 0]])\n",
    "\n",
    "        # Reshape for LSTM-style input\n",
    "        X = np.reshape(X, (X.shape[0], X.shape[1], 1))\n",
    "\n",
    "        # Build model\n",
    "        model = build_tat_model(input_shape=(look_back, 1))\n",
    "\n",
    "        # -----------------------------\n",
    "        # FIX: Small dataset â†’ no validation split\n",
    "        # -----------------------------\n",
    "        if len(X) < 5:\n",
    "            val_split = 0\n",
    "            print(f\"âš ï¸ Small dataset detected for {file} â†’ training WITHOUT validation_split.\")\n",
    "        else:\n",
    "            val_split = 0.2\n",
    "\n",
    "        # Train\n",
    "        history = model.fit(\n",
    "            X, y,\n",
    "            epochs=100,\n",
    "            batch_size=16,\n",
    "            validation_split=val_split,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # Save training logs\n",
    "        log_file = os.path.join(output_logs, file.replace(\".csv\", \"_tat_training.txt\"))\n",
    "        with open(log_file, \"w\") as f:\n",
    "            f.write(\"Training Loss per Epoch:\\n\")\n",
    "            for i in range(len(history.history['loss'])):\n",
    "                if val_split == 0:\n",
    "                    f.write(f\"Epoch {i+1}: Loss={history.history['loss'][i]}\\n\")\n",
    "                else:\n",
    "                    f.write(\n",
    "                        f\"Epoch {i+1}: Loss={history.history['loss'][i]}, \"\n",
    "                        f\"Val_Loss={history.history['val_loss'][i]}\\n\"\n",
    "                    )\n",
    "\n",
    "        # Predict\n",
    "        predictions = model.predict(X)\n",
    "        predictions_rescaled = scaler.inverse_transform(predictions)\n",
    "\n",
    "        # Add predictions to df\n",
    "        df['Predicted'] = [np.nan] * look_back + list(predictions_rescaled.flatten())\n",
    "        df['Predicted'] = df['Predicted'].round(2)\n",
    "        df['Actual'] = df['Average Price']\n",
    "\n",
    "        # Metrics (ignore first look_back rows with NaN preds)\n",
    "        y_true = df['Actual'].values[look_back:]\n",
    "        y_pred = predictions_rescaled.flatten()\n",
    "\n",
    "        mae = round(mean_absolute_error(y_true, y_pred), 2)\n",
    "        rmse = round(np.sqrt(mean_squared_error(y_true, y_pred)), 2)\n",
    "        r2 = round(r2_score(y_true, y_pred), 4)\n",
    "        mape = round(np.mean(np.abs((y_true - y_pred) / y_true)) * 100, 2)\n",
    "        accuracy = round(100 - mape, 2)\n",
    "\n",
    "        metrics_list.append([file.replace(\".csv\", \"\"), mae, rmse, r2, mape, accuracy])\n",
    "\n",
    "        # Save model\n",
    "        model_file = os.path.join(output_models, file.replace(\".csv\", \"_tat_model.h5\"))\n",
    "        model.save(model_file)\n",
    "\n",
    "        # Save updated CSV\n",
    "        updated_csv_path = os.path.join(output_csv, file.replace(\".csv\", \"_tat_updated.csv\"))\n",
    "        df[['Date', 'Actual', 'Predicted']].to_csv(updated_csv_path, index=False)\n",
    "\n",
    "        # Save graph\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(df['Date'], df['Actual'], label='Actual', color='blue')\n",
    "        plt.plot(df['Date'], df['Predicted'], label='Predicted', color='red', linestyle='dashed')\n",
    "        plt.plot(df['Date'], df['MA_7'], label='MA 7', color='orange')\n",
    "        plt.plot(df['Date'], df['MA_30'], label='MA 30', color='green')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Average Price')\n",
    "        plt.title(f'Price Prediction (TAT) - {file}')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        graph_path = os.path.join(output_graphs, file.replace(\".csv\", \"_tat_graph.png\"))\n",
    "        plt.savefig(graph_path, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        print(f\"âœ… Done with {file} | MAE={mae}, RMSE={rmse}, R2={r2}, MAPE={mape}%, ACC={accuracy}%\\n\")\n",
    "\n",
    "# Save metrics CSV\n",
    "metrics_df = pd.DataFrame(\n",
    "    metrics_list,\n",
    "    columns=['District', 'MAE', 'RMSE', 'R2', 'MAPE(%)', 'Accuracy(%)']\n",
    ")\n",
    "metrics_df.to_csv(metrics_file, index=False)\n",
    "\n",
    "print(f\"ğŸ“Š Metrics saved to {metrics_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e19029-f72e-40c9-8ef4-9605c54b94ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TAT+MQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbdfa4bd-01af-4e46-9b7b-b6f8ad83fbfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Processing: capsicum_Bangalore_daily.csv\n",
      "WARNING:tensorflow:From C:\\Users\\ravik\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:219: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                  </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape              </span>â”ƒ<span style=\"font-weight: bold\">         Param # </span>â”ƒ<span style=\"font-weight: bold\"> Connected to               </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)             â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                          â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ multi_query_attention         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">17,216</span> â”‚ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiQueryAttention</span>)         â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ multi_query_attention[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">â€¦</span> â”‚\n",
       "â”‚                               â”‚                           â”‚                 â”‚ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> â”‚ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]                  â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)          â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> â”‚ layer_normalization[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)             â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> â”‚ dense_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ add_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ layer_normalization[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], â”‚\n",
       "â”‚                               â”‚                           â”‚                 â”‚ dense_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization_1         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> â”‚ add_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]                â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)          â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_average_pooling1d      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ layer_normalization_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">â€¦</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)      â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                 â”‚              <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> â”‚ global_average_pooling1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">â€¦</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m1\u001b[0m)             â”‚               \u001b[38;5;34m0\u001b[0m â”‚ -                          â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ multi_query_attention         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)            â”‚          \u001b[38;5;34m17,216\u001b[0m â”‚ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          â”‚\n",
       "â”‚ (\u001b[38;5;33mMultiQueryAttention\u001b[0m)         â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ add (\u001b[38;5;33mAdd\u001b[0m)                     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)            â”‚               \u001b[38;5;34m0\u001b[0m â”‚ multi_query_attention[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34mâ€¦\u001b[0m â”‚\n",
       "â”‚                               â”‚                           â”‚                 â”‚ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)            â”‚             \u001b[38;5;34m128\u001b[0m â”‚ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]                  â”‚\n",
       "â”‚ (\u001b[38;5;33mLayerNormalization\u001b[0m)          â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_7 (\u001b[38;5;33mDense\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)           â”‚           \u001b[38;5;34m8,320\u001b[0m â”‚ layer_normalization[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_8 (\u001b[38;5;33mDense\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m1\u001b[0m)             â”‚             \u001b[38;5;34m129\u001b[0m â”‚ dense_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ add_1 (\u001b[38;5;33mAdd\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)            â”‚               \u001b[38;5;34m0\u001b[0m â”‚ layer_normalization[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], â”‚\n",
       "â”‚                               â”‚                           â”‚                 â”‚ dense_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization_1         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)            â”‚             \u001b[38;5;34m128\u001b[0m â”‚ add_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]                â”‚\n",
       "â”‚ (\u001b[38;5;33mLayerNormalization\u001b[0m)          â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_average_pooling1d      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                â”‚               \u001b[38;5;34m0\u001b[0m â”‚ layer_normalization_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34mâ€¦\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)      â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_9 (\u001b[38;5;33mDense\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                 â”‚              \u001b[38;5;34m65\u001b[0m â”‚ global_average_pooling1d[\u001b[38;5;34mâ€¦\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,986</span> (101.51 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m25,986\u001b[0m (101.51 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,986</span> (101.51 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m25,986\u001b[0m (101.51 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - loss: 0.1178 - mae: 0.1855 - val_loss: 0.0135 - val_mae: 0.0943\n",
      "Epoch 2/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0102 - mae: 0.0757 - val_loss: 0.0127 - val_mae: 0.0884\n",
      "Epoch 3/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0080 - mae: 0.0655 - val_loss: 0.0126 - val_mae: 0.0870\n",
      "Epoch 4/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0069 - mae: 0.0604 - val_loss: 0.0156 - val_mae: 0.1006\n",
      "Epoch 5/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0067 - mae: 0.0588 - val_loss: 0.0151 - val_mae: 0.0975\n",
      "Epoch 6/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0067 - mae: 0.0583 - val_loss: 0.0134 - val_mae: 0.0906\n",
      "Epoch 7/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0062 - mae: 0.0559 - val_loss: 0.0153 - val_mae: 0.0990\n",
      "Epoch 8/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0063 - mae: 0.0564 - val_loss: 0.0124 - val_mae: 0.0863\n",
      "Epoch 9/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0064 - mae: 0.0569 - val_loss: 0.0126 - val_mae: 0.0872\n",
      "Epoch 10/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0064 - mae: 0.0566 - val_loss: 0.0147 - val_mae: 0.0980\n",
      "Epoch 11/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0061 - mae: 0.0553 - val_loss: 0.0122 - val_mae: 0.0855\n",
      "Epoch 12/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0061 - mae: 0.0547 - val_loss: 0.0129 - val_mae: 0.0899\n",
      "Epoch 13/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0059 - mae: 0.0540 - val_loss: 0.0141 - val_mae: 0.0952\n",
      "Epoch 14/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0060 - mae: 0.0541 - val_loss: 0.0129 - val_mae: 0.0891\n",
      "Epoch 15/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0062 - mae: 0.0550 - val_loss: 0.0125 - val_mae: 0.0871\n",
      "Epoch 16/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0059 - mae: 0.0533 - val_loss: 0.0127 - val_mae: 0.0886\n",
      "Epoch 17/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0058 - mae: 0.0527 - val_loss: 0.0124 - val_mae: 0.0864\n",
      "Epoch 18/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0061 - mae: 0.0543 - val_loss: 0.0124 - val_mae: 0.0866\n",
      "Epoch 19/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0057 - mae: 0.0521 - val_loss: 0.0121 - val_mae: 0.0854\n",
      "Epoch 20/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0060 - mae: 0.0535 - val_loss: 0.0142 - val_mae: 0.0952\n",
      "Epoch 21/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0061 - mae: 0.0532 - val_loss: 0.0166 - val_mae: 0.1039\n",
      "Epoch 22/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0061 - mae: 0.0540 - val_loss: 0.0143 - val_mae: 0.0954\n",
      "Epoch 23/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0059 - mae: 0.0530 - val_loss: 0.0124 - val_mae: 0.0862\n",
      "Epoch 24/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0060 - mae: 0.0531 - val_loss: 0.0120 - val_mae: 0.0849\n",
      "Epoch 25/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0058 - mae: 0.0527 - val_loss: 0.0124 - val_mae: 0.0858\n",
      "Epoch 26/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0057 - mae: 0.0518 - val_loss: 0.0120 - val_mae: 0.0843\n",
      "Epoch 27/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0056 - mae: 0.0520 - val_loss: 0.0123 - val_mae: 0.0866\n",
      "Epoch 28/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0062 - mae: 0.0532 - val_loss: 0.0146 - val_mae: 0.0959\n",
      "Epoch 29/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0062 - mae: 0.0536 - val_loss: 0.0125 - val_mae: 0.0878\n",
      "Epoch 30/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0056 - mae: 0.0520 - val_loss: 0.0145 - val_mae: 0.0965\n",
      "Epoch 31/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0057 - mae: 0.0522 - val_loss: 0.0126 - val_mae: 0.0879\n",
      "Epoch 32/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0055 - mae: 0.0511 - val_loss: 0.0116 - val_mae: 0.0803\n",
      "Epoch 33/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0059 - mae: 0.0530 - val_loss: 0.0133 - val_mae: 0.0906\n",
      "Epoch 34/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0057 - mae: 0.0517 - val_loss: 0.0134 - val_mae: 0.0922\n",
      "Epoch 35/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0057 - mae: 0.0514 - val_loss: 0.0116 - val_mae: 0.0801\n",
      "Epoch 36/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0060 - mae: 0.0519 - val_loss: 0.0143 - val_mae: 0.0946\n",
      "Epoch 37/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0059 - mae: 0.0520 - val_loss: 0.0146 - val_mae: 0.0951\n",
      "Epoch 38/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0058 - mae: 0.0521 - val_loss: 0.0121 - val_mae: 0.0855\n",
      "Epoch 39/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0055 - mae: 0.0504 - val_loss: 0.0120 - val_mae: 0.0846\n",
      "Epoch 40/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0061 - mae: 0.0539 - val_loss: 0.0117 - val_mae: 0.0827\n",
      "Epoch 41/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0058 - mae: 0.0519 - val_loss: 0.0142 - val_mae: 0.0947\n",
      "Epoch 42/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0056 - mae: 0.0510 - val_loss: 0.0130 - val_mae: 0.0891\n",
      "Epoch 43/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0060 - mae: 0.0530 - val_loss: 0.0134 - val_mae: 0.0919\n",
      "Epoch 44/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0058 - mae: 0.0517 - val_loss: 0.0133 - val_mae: 0.0917\n",
      "Epoch 45/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0054 - mae: 0.0504 - val_loss: 0.0126 - val_mae: 0.0871\n",
      "Epoch 46/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0056 - mae: 0.0512 - val_loss: 0.0131 - val_mae: 0.0898\n",
      "Epoch 47/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0056 - mae: 0.0509 - val_loss: 0.0146 - val_mae: 0.0962\n",
      "Epoch 48/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0058 - mae: 0.0520 - val_loss: 0.0116 - val_mae: 0.0814\n",
      "Epoch 49/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0054 - mae: 0.0510 - val_loss: 0.0127 - val_mae: 0.0889\n",
      "Epoch 50/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0059 - mae: 0.0526 - val_loss: 0.0127 - val_mae: 0.0871\n",
      "Epoch 51/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0059 - mae: 0.0528 - val_loss: 0.0153 - val_mae: 0.0995\n",
      "Epoch 52/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0060 - mae: 0.0525 - val_loss: 0.0122 - val_mae: 0.0858\n",
      "Epoch 53/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0055 - mae: 0.0519 - val_loss: 0.0136 - val_mae: 0.0919\n",
      "Epoch 54/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0054 - mae: 0.0501 - val_loss: 0.0132 - val_mae: 0.0899\n",
      "Epoch 55/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0061 - mae: 0.0524 - val_loss: 0.0126 - val_mae: 0.0877\n",
      "Epoch 56/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0055 - mae: 0.0516 - val_loss: 0.0174 - val_mae: 0.1094\n",
      "Epoch 57/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0056 - mae: 0.0523 - val_loss: 0.0137 - val_mae: 0.0929\n",
      "Epoch 58/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0057 - mae: 0.0514 - val_loss: 0.0122 - val_mae: 0.0863\n",
      "Epoch 59/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0057 - mae: 0.0516 - val_loss: 0.0133 - val_mae: 0.0894\n",
      "Epoch 60/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0058 - mae: 0.0517 - val_loss: 0.0125 - val_mae: 0.0880\n",
      "Epoch 61/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0060 - mae: 0.0522 - val_loss: 0.0165 - val_mae: 0.1023\n",
      "Epoch 62/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0058 - mae: 0.0523 - val_loss: 0.0121 - val_mae: 0.0842\n",
      "Epoch 63/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0056 - mae: 0.0514 - val_loss: 0.0125 - val_mae: 0.0876\n",
      "Epoch 64/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0056 - mae: 0.0510 - val_loss: 0.0131 - val_mae: 0.0893\n",
      "Epoch 65/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0055 - mae: 0.0506 - val_loss: 0.0130 - val_mae: 0.0893\n",
      "Epoch 66/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0059 - mae: 0.0519 - val_loss: 0.0145 - val_mae: 0.0956\n",
      "Epoch 67/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0056 - mae: 0.0516 - val_loss: 0.0123 - val_mae: 0.0867\n",
      "Epoch 68/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0054 - mae: 0.0512 - val_loss: 0.0126 - val_mae: 0.0876\n",
      "Epoch 69/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0054 - mae: 0.0506 - val_loss: 0.0137 - val_mae: 0.0928\n",
      "Epoch 70/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0054 - mae: 0.0499 - val_loss: 0.0125 - val_mae: 0.0870\n",
      "Epoch 71/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0056 - mae: 0.0510 - val_loss: 0.0126 - val_mae: 0.0873\n",
      "Epoch 72/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0057 - mae: 0.0514 - val_loss: 0.0129 - val_mae: 0.0901\n",
      "Epoch 73/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0057 - mae: 0.0515 - val_loss: 0.0125 - val_mae: 0.0874\n",
      "Epoch 74/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0058 - mae: 0.0514 - val_loss: 0.0121 - val_mae: 0.0844\n",
      "Epoch 75/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0055 - mae: 0.0515 - val_loss: 0.0143 - val_mae: 0.0952\n",
      "Epoch 76/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0055 - mae: 0.0511 - val_loss: 0.0118 - val_mae: 0.0808\n",
      "Epoch 77/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0057 - mae: 0.0516 - val_loss: 0.0137 - val_mae: 0.0924\n",
      "Epoch 78/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0056 - mae: 0.0507 - val_loss: 0.0128 - val_mae: 0.0886\n",
      "Epoch 79/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0054 - mae: 0.0497 - val_loss: 0.0119 - val_mae: 0.0819\n",
      "Epoch 80/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0059 - mae: 0.0522 - val_loss: 0.0120 - val_mae: 0.0838\n",
      "Epoch 81/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0059 - mae: 0.0515 - val_loss: 0.0140 - val_mae: 0.0937\n",
      "Epoch 82/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0055 - mae: 0.0502 - val_loss: 0.0122 - val_mae: 0.0860\n",
      "Epoch 83/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0053 - mae: 0.0503 - val_loss: 0.0118 - val_mae: 0.0817\n",
      "Epoch 84/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0057 - mae: 0.0516 - val_loss: 0.0132 - val_mae: 0.0905\n",
      "Epoch 85/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0055 - mae: 0.0507 - val_loss: 0.0129 - val_mae: 0.0881\n",
      "Epoch 86/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0060 - mae: 0.0524 - val_loss: 0.0139 - val_mae: 0.0935\n",
      "Epoch 87/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0055 - mae: 0.0510 - val_loss: 0.0122 - val_mae: 0.0849\n",
      "Epoch 88/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0057 - mae: 0.0513 - val_loss: 0.0129 - val_mae: 0.0885\n",
      "Epoch 89/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0056 - mae: 0.0509 - val_loss: 0.0130 - val_mae: 0.0889\n",
      "Epoch 90/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0058 - mae: 0.0515 - val_loss: 0.0124 - val_mae: 0.0862\n",
      "Epoch 91/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0057 - mae: 0.0515 - val_loss: 0.0125 - val_mae: 0.0869\n",
      "Epoch 92/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0057 - mae: 0.0511 - val_loss: 0.0148 - val_mae: 0.0948\n",
      "Epoch 93/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0055 - mae: 0.0513 - val_loss: 0.0120 - val_mae: 0.0845\n",
      "Epoch 94/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0056 - mae: 0.0511 - val_loss: 0.0128 - val_mae: 0.0871\n",
      "Epoch 95/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0056 - mae: 0.0507 - val_loss: 0.0124 - val_mae: 0.0853\n",
      "Epoch 96/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0054 - mae: 0.0504 - val_loss: 0.0122 - val_mae: 0.0856\n",
      "Epoch 97/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0055 - mae: 0.0513 - val_loss: 0.0121 - val_mae: 0.0850\n",
      "Epoch 98/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0057 - mae: 0.0510 - val_loss: 0.0124 - val_mae: 0.0857\n",
      "Epoch 99/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0056 - mae: 0.0505 - val_loss: 0.0126 - val_mae: 0.0875\n",
      "Epoch 100/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0054 - mae: 0.0501 - val_loss: 0.0125 - val_mae: 0.0855\n",
      "\u001b[1m290/290\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step\n",
      "âœ… Done with capsicum_Bangalore_daily.csv | MAE=555.46, RMSE=811.76, R2=0.5784, MAPE=18.0%, Accuracy=82.0%\n",
      "Saved updated CSV with Date, Actual & Predicted: tat_mqa_output_csv\\capsicum_Bangalore_daily_tat_mqa_updated.csv\n",
      "ğŸš€ Processing: capsicum_Belgaum_daily.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                  </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape              </span>â”ƒ<span style=\"font-weight: bold\">         Param # </span>â”ƒ<span style=\"font-weight: bold\"> Connected to               </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)             â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                          â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ multi_query_attention_1       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">17,216</span> â”‚ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiQueryAttention</span>)         â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ add_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ multi_query_attention_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>â€¦ â”‚\n",
       "â”‚                               â”‚                           â”‚                 â”‚ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization_2         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> â”‚ add_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]                â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)          â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> â”‚ layer_normalization_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">â€¦</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)             â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> â”‚ dense_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ add_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ layer_normalization_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">â€¦</span> â”‚\n",
       "â”‚                               â”‚                           â”‚                 â”‚ dense_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization_3         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> â”‚ add_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]                â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)          â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_average_pooling1d_1    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ layer_normalization_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">â€¦</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)      â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                 â”‚              <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> â”‚ global_average_pooling1d_â€¦ â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m1\u001b[0m)             â”‚               \u001b[38;5;34m0\u001b[0m â”‚ -                          â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ multi_query_attention_1       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)            â”‚          \u001b[38;5;34m17,216\u001b[0m â”‚ input_layer_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        â”‚\n",
       "â”‚ (\u001b[38;5;33mMultiQueryAttention\u001b[0m)         â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ add_2 (\u001b[38;5;33mAdd\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)            â”‚               \u001b[38;5;34m0\u001b[0m â”‚ multi_query_attention_1[\u001b[38;5;34m0\u001b[0mâ€¦ â”‚\n",
       "â”‚                               â”‚                           â”‚                 â”‚ input_layer_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization_2         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)            â”‚             \u001b[38;5;34m128\u001b[0m â”‚ add_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]                â”‚\n",
       "â”‚ (\u001b[38;5;33mLayerNormalization\u001b[0m)          â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_17 (\u001b[38;5;33mDense\u001b[0m)              â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)           â”‚           \u001b[38;5;34m8,320\u001b[0m â”‚ layer_normalization_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34mâ€¦\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_18 (\u001b[38;5;33mDense\u001b[0m)              â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m1\u001b[0m)             â”‚             \u001b[38;5;34m129\u001b[0m â”‚ dense_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ add_3 (\u001b[38;5;33mAdd\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)            â”‚               \u001b[38;5;34m0\u001b[0m â”‚ layer_normalization_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34mâ€¦\u001b[0m â”‚\n",
       "â”‚                               â”‚                           â”‚                 â”‚ dense_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization_3         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)            â”‚             \u001b[38;5;34m128\u001b[0m â”‚ add_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]                â”‚\n",
       "â”‚ (\u001b[38;5;33mLayerNormalization\u001b[0m)          â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_average_pooling1d_1    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                â”‚               \u001b[38;5;34m0\u001b[0m â”‚ layer_normalization_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34mâ€¦\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)      â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_19 (\u001b[38;5;33mDense\u001b[0m)              â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                 â”‚              \u001b[38;5;34m65\u001b[0m â”‚ global_average_pooling1d_â€¦ â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,986</span> (101.51 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m25,986\u001b[0m (101.51 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,986</span> (101.51 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m25,986\u001b[0m (101.51 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 26ms/step - loss: 0.2599 - mae: 0.3814 - val_loss: 0.0059 - val_mae: 0.0703\n",
      "Epoch 2/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0142 - mae: 0.0916 - val_loss: 0.0027 - val_mae: 0.0423\n",
      "Epoch 3/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0126 - mae: 0.0833 - val_loss: 0.0030 - val_mae: 0.0400\n",
      "Epoch 4/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0121 - mae: 0.0778 - val_loss: 0.0050 - val_mae: 0.0643\n",
      "Epoch 5/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0113 - mae: 0.0741 - val_loss: 0.0031 - val_mae: 0.0491\n",
      "Epoch 6/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0119 - mae: 0.0749 - val_loss: 0.0033 - val_mae: 0.0515\n",
      "Epoch 7/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0092 - mae: 0.0677 - val_loss: 0.0055 - val_mae: 0.0545\n",
      "Epoch 8/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0103 - mae: 0.0736 - val_loss: 0.0041 - val_mae: 0.0583\n",
      "Epoch 9/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0115 - mae: 0.0699 - val_loss: 0.0028 - val_mae: 0.0453\n",
      "Epoch 10/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0108 - mae: 0.0699 - val_loss: 0.0028 - val_mae: 0.0458\n",
      "Epoch 11/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0104 - mae: 0.0680 - val_loss: 0.0060 - val_mae: 0.0587\n",
      "Epoch 12/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0109 - mae: 0.0714 - val_loss: 0.0039 - val_mae: 0.0407\n",
      "Epoch 13/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0082 - mae: 0.0607 - val_loss: 0.0049 - val_mae: 0.0636\n",
      "Epoch 14/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0091 - mae: 0.0658 - val_loss: 0.0028 - val_mae: 0.0462\n",
      "Epoch 15/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0080 - mae: 0.0590 - val_loss: 0.0037 - val_mae: 0.0556\n",
      "Epoch 16/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0092 - mae: 0.0614 - val_loss: 0.0042 - val_mae: 0.0592\n",
      "Epoch 17/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0105 - mae: 0.0639 - val_loss: 0.0031 - val_mae: 0.0383\n",
      "Epoch 18/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0087 - mae: 0.0612 - val_loss: 0.0032 - val_mae: 0.0505\n",
      "Epoch 19/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0088 - mae: 0.0608 - val_loss: 0.0028 - val_mae: 0.0395\n",
      "Epoch 20/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0080 - mae: 0.0583 - val_loss: 0.0027 - val_mae: 0.0433\n",
      "Epoch 21/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0080 - mae: 0.0574 - val_loss: 0.0086 - val_mae: 0.0772\n",
      "Epoch 22/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0111 - mae: 0.0733 - val_loss: 0.0057 - val_mae: 0.0690\n",
      "Epoch 23/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0095 - mae: 0.0673 - val_loss: 0.0037 - val_mae: 0.0550\n",
      "Epoch 24/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0086 - mae: 0.0602 - val_loss: 0.0041 - val_mae: 0.0424\n",
      "Epoch 25/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0104 - mae: 0.0683 - val_loss: 0.0034 - val_mae: 0.0384\n",
      "Epoch 26/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0080 - mae: 0.0615 - val_loss: 0.0028 - val_mae: 0.0412\n",
      "Epoch 27/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0092 - mae: 0.0600 - val_loss: 0.0036 - val_mae: 0.0393\n",
      "Epoch 28/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0090 - mae: 0.0639 - val_loss: 0.0031 - val_mae: 0.0384\n",
      "Epoch 29/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0076 - mae: 0.0575 - val_loss: 0.0027 - val_mae: 0.0430\n",
      "Epoch 30/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0077 - mae: 0.0564 - val_loss: 0.0027 - val_mae: 0.0410\n",
      "Epoch 31/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0087 - mae: 0.0608 - val_loss: 0.0055 - val_mae: 0.0676\n",
      "Epoch 32/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0091 - mae: 0.0641 - val_loss: 0.0032 - val_mae: 0.0380\n",
      "Epoch 33/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0065 - mae: 0.0526 - val_loss: 0.0027 - val_mae: 0.0440\n",
      "Epoch 34/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0066 - mae: 0.0533 - val_loss: 0.0035 - val_mae: 0.0388\n",
      "Epoch 35/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0087 - mae: 0.0599 - val_loss: 0.0032 - val_mae: 0.0380\n",
      "Epoch 36/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0080 - mae: 0.0587 - val_loss: 0.0027 - val_mae: 0.0446\n",
      "Epoch 37/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0077 - mae: 0.0559 - val_loss: 0.0044 - val_mae: 0.0602\n",
      "Epoch 38/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0095 - mae: 0.0663 - val_loss: 0.0031 - val_mae: 0.0378\n",
      "Epoch 39/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0086 - mae: 0.0615 - val_loss: 0.0027 - val_mae: 0.0430\n",
      "Epoch 40/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0068 - mae: 0.0553 - val_loss: 0.0027 - val_mae: 0.0413\n",
      "Epoch 41/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0081 - mae: 0.0586 - val_loss: 0.0027 - val_mae: 0.0415\n",
      "Epoch 42/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0069 - mae: 0.0541 - val_loss: 0.0033 - val_mae: 0.0380\n",
      "Epoch 43/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0092 - mae: 0.0647 - val_loss: 0.0027 - val_mae: 0.0449\n",
      "Epoch 44/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0103 - mae: 0.0649 - val_loss: 0.0027 - val_mae: 0.0434\n",
      "Epoch 45/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0086 - mae: 0.0595 - val_loss: 0.0045 - val_mae: 0.0611\n",
      "Epoch 46/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0081 - mae: 0.0593 - val_loss: 0.0038 - val_mae: 0.0408\n",
      "Epoch 47/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0084 - mae: 0.0606 - val_loss: 0.0029 - val_mae: 0.0477\n",
      "Epoch 48/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0080 - mae: 0.0575 - val_loss: 0.0036 - val_mae: 0.0542\n",
      "Epoch 49/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0082 - mae: 0.0613 - val_loss: 0.0027 - val_mae: 0.0439\n",
      "Epoch 50/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0084 - mae: 0.0585 - val_loss: 0.0053 - val_mae: 0.0662\n",
      "Epoch 51/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0087 - mae: 0.0629 - val_loss: 0.0030 - val_mae: 0.0379\n",
      "Epoch 52/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0074 - mae: 0.0567 - val_loss: 0.0027 - val_mae: 0.0431\n",
      "Epoch 53/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0087 - mae: 0.0592 - val_loss: 0.0030 - val_mae: 0.0488\n",
      "Epoch 54/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0080 - mae: 0.0622 - val_loss: 0.0033 - val_mae: 0.0381\n",
      "Epoch 55/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0084 - mae: 0.0620 - val_loss: 0.0027 - val_mae: 0.0427\n",
      "Epoch 56/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0098 - mae: 0.0637 - val_loss: 0.0032 - val_mae: 0.0379\n",
      "Epoch 57/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0094 - mae: 0.0644 - val_loss: 0.0026 - val_mae: 0.0414\n",
      "Epoch 58/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0090 - mae: 0.0592 - val_loss: 0.0027 - val_mae: 0.0413\n",
      "Epoch 59/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0077 - mae: 0.0571 - val_loss: 0.0029 - val_mae: 0.0481\n",
      "Epoch 60/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0084 - mae: 0.0579 - val_loss: 0.0033 - val_mae: 0.0382\n",
      "Epoch 61/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0077 - mae: 0.0571 - val_loss: 0.0028 - val_mae: 0.0382\n",
      "Epoch 62/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0083 - mae: 0.0582 - val_loss: 0.0028 - val_mae: 0.0381\n",
      "Epoch 63/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0069 - mae: 0.0524 - val_loss: 0.0029 - val_mae: 0.0373\n",
      "Epoch 64/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0076 - mae: 0.0645 - val_loss: 0.0072 - val_mae: 0.0768\n",
      "Epoch 65/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0087 - mae: 0.0626 - val_loss: 0.0027 - val_mae: 0.0439\n",
      "Epoch 66/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0074 - mae: 0.0569 - val_loss: 0.0027 - val_mae: 0.0451\n",
      "Epoch 67/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0079 - mae: 0.0590 - val_loss: 0.0042 - val_mae: 0.0590\n",
      "Epoch 68/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0098 - mae: 0.0657 - val_loss: 0.0029 - val_mae: 0.0474\n",
      "Epoch 69/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0090 - mae: 0.0598 - val_loss: 0.0028 - val_mae: 0.0376\n",
      "Epoch 70/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0074 - mae: 0.0576 - val_loss: 0.0035 - val_mae: 0.0393\n",
      "Epoch 71/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0073 - mae: 0.0571 - val_loss: 0.0034 - val_mae: 0.0526\n",
      "Epoch 72/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0080 - mae: 0.0586 - val_loss: 0.0036 - val_mae: 0.0548\n",
      "Epoch 73/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0090 - mae: 0.0597 - val_loss: 0.0047 - val_mae: 0.0624\n",
      "Epoch 74/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0091 - mae: 0.0642 - val_loss: 0.0048 - val_mae: 0.0628\n",
      "Epoch 75/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0089 - mae: 0.0650 - val_loss: 0.0029 - val_mae: 0.0478\n",
      "Epoch 76/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0070 - mae: 0.0581 - val_loss: 0.0037 - val_mae: 0.0556\n",
      "Epoch 77/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0072 - mae: 0.0556 - val_loss: 0.0031 - val_mae: 0.0376\n",
      "Epoch 78/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0081 - mae: 0.0621 - val_loss: 0.0029 - val_mae: 0.0473\n",
      "Epoch 79/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0076 - mae: 0.0581 - val_loss: 0.0029 - val_mae: 0.0372\n",
      "Epoch 80/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0085 - mae: 0.0595 - val_loss: 0.0026 - val_mae: 0.0431\n",
      "Epoch 81/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0077 - mae: 0.0583 - val_loss: 0.0034 - val_mae: 0.0385\n",
      "Epoch 82/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0092 - mae: 0.0623 - val_loss: 0.0028 - val_mae: 0.0376\n",
      "Epoch 83/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0066 - mae: 0.0541 - val_loss: 0.0027 - val_mae: 0.0454\n",
      "Epoch 84/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0078 - mae: 0.0578 - val_loss: 0.0026 - val_mae: 0.0435\n",
      "Epoch 85/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0063 - mae: 0.0529 - val_loss: 0.0030 - val_mae: 0.0489\n",
      "Epoch 86/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0079 - mae: 0.0587 - val_loss: 0.0028 - val_mae: 0.0472\n",
      "Epoch 87/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0074 - mae: 0.0559 - val_loss: 0.0027 - val_mae: 0.0384\n",
      "Epoch 88/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0084 - mae: 0.0614 - val_loss: 0.0035 - val_mae: 0.0393\n",
      "Epoch 89/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0077 - mae: 0.0601 - val_loss: 0.0029 - val_mae: 0.0471\n",
      "Epoch 90/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0082 - mae: 0.0577 - val_loss: 0.0030 - val_mae: 0.0372\n",
      "Epoch 91/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0075 - mae: 0.0596 - val_loss: 0.0026 - val_mae: 0.0385\n",
      "Epoch 92/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0077 - mae: 0.0545 - val_loss: 0.0026 - val_mae: 0.0421\n",
      "Epoch 93/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0073 - mae: 0.0567 - val_loss: 0.0025 - val_mae: 0.0389\n",
      "Epoch 94/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0070 - mae: 0.0579 - val_loss: 0.0050 - val_mae: 0.0523\n",
      "Epoch 95/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0086 - mae: 0.0610 - val_loss: 0.0026 - val_mae: 0.0418\n",
      "Epoch 96/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0070 - mae: 0.0558 - val_loss: 0.0027 - val_mae: 0.0375\n",
      "Epoch 97/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0063 - mae: 0.0524 - val_loss: 0.0034 - val_mae: 0.0524\n",
      "Epoch 98/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0073 - mae: 0.0600 - val_loss: 0.0048 - val_mae: 0.0629\n",
      "Epoch 99/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0101 - mae: 0.0742 - val_loss: 0.0055 - val_mae: 0.0566\n",
      "Epoch 100/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0090 - mae: 0.0658 - val_loss: 0.0026 - val_mae: 0.0411\n",
      "\u001b[1m38/38\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "âœ… Done with capsicum_Belgaum_daily.csv | MAE=381.75, RMSE=565.64, R2=0.4933, MAPE=13.98%, Accuracy=86.02%\n",
      "Saved updated CSV with Date, Actual & Predicted: tat_mqa_output_csv\\capsicum_Belgaum_daily_tat_mqa_updated.csv\n",
      "ğŸš€ Processing: capsicum_Bellary_daily.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                  </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape              </span>â”ƒ<span style=\"font-weight: bold\">         Param # </span>â”ƒ<span style=\"font-weight: bold\"> Connected to               </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)             â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                          â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ multi_query_attention_2       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">17,216</span> â”‚ input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiQueryAttention</span>)         â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ add_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ multi_query_attention_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>â€¦ â”‚\n",
       "â”‚                               â”‚                           â”‚                 â”‚ input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization_4         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> â”‚ add_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]                â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)          â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> â”‚ layer_normalization_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">â€¦</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)             â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> â”‚ dense_27[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ add_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ layer_normalization_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">â€¦</span> â”‚\n",
       "â”‚                               â”‚                           â”‚                 â”‚ dense_28[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization_5         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> â”‚ add_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]                â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)          â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_average_pooling1d_2    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ layer_normalization_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">â€¦</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)      â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                 â”‚              <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> â”‚ global_average_pooling1d_â€¦ â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer_2 (\u001b[38;5;33mInputLayer\u001b[0m)    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m1\u001b[0m)             â”‚               \u001b[38;5;34m0\u001b[0m â”‚ -                          â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ multi_query_attention_2       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)            â”‚          \u001b[38;5;34m17,216\u001b[0m â”‚ input_layer_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        â”‚\n",
       "â”‚ (\u001b[38;5;33mMultiQueryAttention\u001b[0m)         â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ add_4 (\u001b[38;5;33mAdd\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)            â”‚               \u001b[38;5;34m0\u001b[0m â”‚ multi_query_attention_2[\u001b[38;5;34m0\u001b[0mâ€¦ â”‚\n",
       "â”‚                               â”‚                           â”‚                 â”‚ input_layer_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization_4         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)            â”‚             \u001b[38;5;34m128\u001b[0m â”‚ add_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]                â”‚\n",
       "â”‚ (\u001b[38;5;33mLayerNormalization\u001b[0m)          â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_27 (\u001b[38;5;33mDense\u001b[0m)              â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)           â”‚           \u001b[38;5;34m8,320\u001b[0m â”‚ layer_normalization_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34mâ€¦\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_28 (\u001b[38;5;33mDense\u001b[0m)              â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m1\u001b[0m)             â”‚             \u001b[38;5;34m129\u001b[0m â”‚ dense_27[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ add_5 (\u001b[38;5;33mAdd\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)            â”‚               \u001b[38;5;34m0\u001b[0m â”‚ layer_normalization_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34mâ€¦\u001b[0m â”‚\n",
       "â”‚                               â”‚                           â”‚                 â”‚ dense_28[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization_5         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)            â”‚             \u001b[38;5;34m128\u001b[0m â”‚ add_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]                â”‚\n",
       "â”‚ (\u001b[38;5;33mLayerNormalization\u001b[0m)          â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_average_pooling1d_2    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                â”‚               \u001b[38;5;34m0\u001b[0m â”‚ layer_normalization_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34mâ€¦\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)      â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_29 (\u001b[38;5;33mDense\u001b[0m)              â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                 â”‚              \u001b[38;5;34m65\u001b[0m â”‚ global_average_pooling1d_â€¦ â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,986</span> (101.51 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m25,986\u001b[0m (101.51 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,986</span> (101.51 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m25,986\u001b[0m (101.51 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 105ms/step - loss: 2.1890 - mae: 1.2865 - val_loss: 0.6870 - val_mae: 0.7875\n",
      "Epoch 2/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.2562 - mae: 0.4347 - val_loss: 0.3056 - val_mae: 0.4939\n",
      "Epoch 3/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.3794 - mae: 0.5550 - val_loss: 0.3542 - val_mae: 0.5418\n",
      "Epoch 4/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.1356 - mae: 0.3220 - val_loss: 0.5652 - val_mae: 0.7091\n",
      "Epoch 5/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.1155 - mae: 0.2741 - val_loss: 0.0889 - val_mae: 0.2581\n",
      "Epoch 6/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0665 - mae: 0.2100 - val_loss: 0.0665 - val_mae: 0.2048\n",
      "Epoch 7/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0707 - mae: 0.2196 - val_loss: 0.2198 - val_mae: 0.4198\n",
      "Epoch 8/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0437 - mae: 0.1690 - val_loss: 0.2601 - val_mae: 0.4578\n",
      "Epoch 9/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0403 - mae: 0.1645 - val_loss: 0.1427 - val_mae: 0.3405\n",
      "Epoch 10/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0367 - mae: 0.1565 - val_loss: 0.1100 - val_mae: 0.2961\n",
      "Epoch 11/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0298 - mae: 0.1417 - val_loss: 0.1713 - val_mae: 0.3728\n",
      "Epoch 12/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0343 - mae: 0.1527 - val_loss: 0.2219 - val_mae: 0.4225\n",
      "Epoch 13/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0393 - mae: 0.1530 - val_loss: 0.1271 - val_mae: 0.3214\n",
      "Epoch 14/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0395 - mae: 0.1615 - val_loss: 0.1326 - val_mae: 0.3290\n",
      "Epoch 15/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0348 - mae: 0.1585 - val_loss: 0.1659 - val_mae: 0.3681\n",
      "Epoch 16/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0314 - mae: 0.1544 - val_loss: 0.1599 - val_mae: 0.3617\n",
      "Epoch 17/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.0324 - mae: 0.1500 - val_loss: 0.1351 - val_mae: 0.3326\n",
      "Epoch 18/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 0.0360 - mae: 0.1595 - val_loss: 0.1470 - val_mae: 0.3473\n",
      "Epoch 19/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.0273 - mae: 0.1344 - val_loss: 0.1565 - val_mae: 0.3583\n",
      "Epoch 20/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.0322 - mae: 0.1530 - val_loss: 0.1581 - val_mae: 0.3602\n",
      "Epoch 21/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 0.0312 - mae: 0.1422 - val_loss: 0.1487 - val_mae: 0.3497\n",
      "Epoch 22/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0310 - mae: 0.1481 - val_loss: 0.1640 - val_mae: 0.3670\n",
      "Epoch 23/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0319 - mae: 0.1379 - val_loss: 0.1330 - val_mae: 0.3308\n",
      "Epoch 24/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0318 - mae: 0.1474 - val_loss: 0.1561 - val_mae: 0.3588\n",
      "Epoch 25/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0373 - mae: 0.1553 - val_loss: 0.1525 - val_mae: 0.3549\n",
      "Epoch 26/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0357 - mae: 0.1574 - val_loss: 0.1488 - val_mae: 0.3508\n",
      "Epoch 27/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0434 - mae: 0.1759 - val_loss: 0.1424 - val_mae: 0.3432\n",
      "Epoch 28/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0324 - mae: 0.1434 - val_loss: 0.1424 - val_mae: 0.3433\n",
      "Epoch 29/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0276 - mae: 0.1428 - val_loss: 0.1695 - val_mae: 0.3739\n",
      "Epoch 30/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.0364 - mae: 0.1576 - val_loss: 0.1343 - val_mae: 0.3333\n",
      "Epoch 31/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0389 - mae: 0.1648 - val_loss: 0.1277 - val_mae: 0.3248\n",
      "Epoch 32/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0361 - mae: 0.1547 - val_loss: 0.1536 - val_mae: 0.3566\n",
      "Epoch 33/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.0354 - mae: 0.1564 - val_loss: 0.1561 - val_mae: 0.3594\n",
      "Epoch 34/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0267 - mae: 0.1346 - val_loss: 0.1288 - val_mae: 0.3260\n",
      "Epoch 35/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0316 - mae: 0.1514 - val_loss: 0.1629 - val_mae: 0.3667\n",
      "Epoch 36/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0343 - mae: 0.1544 - val_loss: 0.1312 - val_mae: 0.3290\n",
      "Epoch 37/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0288 - mae: 0.1395 - val_loss: 0.1607 - val_mae: 0.3640\n",
      "Epoch 38/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0328 - mae: 0.1451 - val_loss: 0.1346 - val_mae: 0.3334\n",
      "Epoch 39/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0323 - mae: 0.1482 - val_loss: 0.1535 - val_mae: 0.3561\n",
      "Epoch 40/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0362 - mae: 0.1618 - val_loss: 0.1527 - val_mae: 0.3552\n",
      "Epoch 41/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.0297 - mae: 0.1442 - val_loss: 0.1425 - val_mae: 0.3431\n",
      "Epoch 42/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0298 - mae: 0.1481 - val_loss: 0.1391 - val_mae: 0.3389\n",
      "Epoch 43/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.0349 - mae: 0.1538 - val_loss: 0.1469 - val_mae: 0.3483\n",
      "Epoch 44/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0331 - mae: 0.1458 - val_loss: 0.1392 - val_mae: 0.3387\n",
      "Epoch 45/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0361 - mae: 0.1566 - val_loss: 0.1716 - val_mae: 0.3754\n",
      "Epoch 46/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.0317 - mae: 0.1480 - val_loss: 0.1261 - val_mae: 0.3218\n",
      "Epoch 47/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0348 - mae: 0.1520 - val_loss: 0.1577 - val_mae: 0.3605\n",
      "Epoch 48/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0291 - mae: 0.1416 - val_loss: 0.1555 - val_mae: 0.3581\n",
      "Epoch 49/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.0229 - mae: 0.1270 - val_loss: 0.1387 - val_mae: 0.3382\n",
      "Epoch 50/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0330 - mae: 0.1526 - val_loss: 0.1627 - val_mae: 0.3661\n",
      "Epoch 51/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.0299 - mae: 0.1443 - val_loss: 0.1498 - val_mae: 0.3516\n",
      "Epoch 52/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0262 - mae: 0.1384 - val_loss: 0.1063 - val_mae: 0.2924\n",
      "Epoch 53/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0315 - mae: 0.1498 - val_loss: 0.1809 - val_mae: 0.3850\n",
      "Epoch 54/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0343 - mae: 0.1475 - val_loss: 0.1469 - val_mae: 0.3483\n",
      "Epoch 55/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0345 - mae: 0.1571 - val_loss: 0.1412 - val_mae: 0.3415\n",
      "Epoch 56/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0302 - mae: 0.1432 - val_loss: 0.1677 - val_mae: 0.3718\n",
      "Epoch 57/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.0319 - mae: 0.1428 - val_loss: 0.1354 - val_mae: 0.3344\n",
      "Epoch 58/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0324 - mae: 0.1500 - val_loss: 0.1498 - val_mae: 0.3521\n",
      "Epoch 59/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0354 - mae: 0.1627 - val_loss: 0.1643 - val_mae: 0.3683\n",
      "Epoch 60/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0297 - mae: 0.1372 - val_loss: 0.1258 - val_mae: 0.3219\n",
      "Epoch 61/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0335 - mae: 0.1462 - val_loss: 0.2011 - val_mae: 0.4048\n",
      "Epoch 62/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0342 - mae: 0.1578 - val_loss: 0.1349 - val_mae: 0.3335\n",
      "Epoch 63/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0307 - mae: 0.1418 - val_loss: 0.1505 - val_mae: 0.3526\n",
      "Epoch 64/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0275 - mae: 0.1390 - val_loss: 0.1723 - val_mae: 0.3764\n",
      "Epoch 65/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.0375 - mae: 0.1585 - val_loss: 0.1641 - val_mae: 0.3679\n",
      "Epoch 66/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0321 - mae: 0.1518 - val_loss: 0.1505 - val_mae: 0.3527\n",
      "Epoch 67/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0261 - mae: 0.1356 - val_loss: 0.1453 - val_mae: 0.3466\n",
      "Epoch 68/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0279 - mae: 0.1328 - val_loss: 0.1460 - val_mae: 0.3475\n",
      "Epoch 69/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0342 - mae: 0.1536 - val_loss: 0.1910 - val_mae: 0.3953\n",
      "Epoch 70/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0266 - mae: 0.1326 - val_loss: 0.1281 - val_mae: 0.3249\n",
      "Epoch 71/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0292 - mae: 0.1426 - val_loss: 0.1554 - val_mae: 0.3582\n",
      "Epoch 72/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0332 - mae: 0.1447 - val_loss: 0.1669 - val_mae: 0.3707\n",
      "Epoch 73/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.0241 - mae: 0.1257 - val_loss: 0.1211 - val_mae: 0.3151\n",
      "Epoch 74/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0283 - mae: 0.1346 - val_loss: 0.1786 - val_mae: 0.3829\n",
      "Epoch 75/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0326 - mae: 0.1523 - val_loss: 0.1442 - val_mae: 0.3452\n",
      "Epoch 76/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0364 - mae: 0.1531 - val_loss: 0.1307 - val_mae: 0.3283\n",
      "Epoch 77/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.0331 - mae: 0.1488 - val_loss: 0.1999 - val_mae: 0.4036\n",
      "Epoch 78/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0290 - mae: 0.1349 - val_loss: 0.1416 - val_mae: 0.3421\n",
      "Epoch 79/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0317 - mae: 0.1488 - val_loss: 0.1233 - val_mae: 0.3184\n",
      "Epoch 80/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.0329 - mae: 0.1518 - val_loss: 0.1885 - val_mae: 0.3929\n",
      "Epoch 81/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0280 - mae: 0.1387 - val_loss: 0.1318 - val_mae: 0.3299\n",
      "Epoch 82/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0395 - mae: 0.1573 - val_loss: 0.1224 - val_mae: 0.3173\n",
      "Epoch 83/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0264 - mae: 0.1322 - val_loss: 0.2138 - val_mae: 0.4166\n",
      "Epoch 84/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0316 - mae: 0.1483 - val_loss: 0.1266 - val_mae: 0.3232\n",
      "Epoch 85/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0297 - mae: 0.1421 - val_loss: 0.1354 - val_mae: 0.3349\n",
      "Epoch 86/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0330 - mae: 0.1571 - val_loss: 0.1934 - val_mae: 0.3980\n",
      "Epoch 87/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0270 - mae: 0.1415 - val_loss: 0.1190 - val_mae: 0.3124\n",
      "Epoch 88/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0289 - mae: 0.1411 - val_loss: 0.1712 - val_mae: 0.3758\n",
      "Epoch 89/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.0337 - mae: 0.1533 - val_loss: 0.1573 - val_mae: 0.3607\n",
      "Epoch 90/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0282 - mae: 0.1348 - val_loss: 0.1374 - val_mae: 0.3371\n",
      "Epoch 91/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0369 - mae: 0.1588 - val_loss: 0.1565 - val_mae: 0.3597\n",
      "Epoch 92/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0316 - mae: 0.1504 - val_loss: 0.1462 - val_mae: 0.3478\n",
      "Epoch 93/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0298 - mae: 0.1407 - val_loss: 0.1603 - val_mae: 0.3639\n",
      "Epoch 94/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0296 - mae: 0.1417 - val_loss: 0.1386 - val_mae: 0.3385\n",
      "Epoch 95/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0284 - mae: 0.1404 - val_loss: 0.1695 - val_mae: 0.3738\n",
      "Epoch 96/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0276 - mae: 0.1400 - val_loss: 0.1384 - val_mae: 0.3384\n",
      "Epoch 97/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.0310 - mae: 0.1429 - val_loss: 0.1613 - val_mae: 0.3654\n",
      "Epoch 98/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.0248 - mae: 0.1327 - val_loss: 0.1728 - val_mae: 0.3776\n",
      "Epoch 99/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0297 - mae: 0.1441 - val_loss: 0.1333 - val_mae: 0.3322\n",
      "Epoch 100/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0279 - mae: 0.1320 - val_loss: 0.1500 - val_mae: 0.3527\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step\n",
      "âœ… Done with capsicum_Bellary_daily.csv | MAE=384.68, RMSE=484.72, R2=0.0928, MAPE=16.05%, Accuracy=83.95%\n",
      "Saved updated CSV with Date, Actual & Predicted: tat_mqa_output_csv\\capsicum_Bellary_daily_tat_mqa_updated.csv\n",
      "ğŸš€ Processing: capsicum_Chikmagalur_daily.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                  </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape              </span>â”ƒ<span style=\"font-weight: bold\">         Param # </span>â”ƒ<span style=\"font-weight: bold\"> Connected to               </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)             â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                          â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ multi_query_attention_3       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">17,216</span> â”‚ input_layer_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiQueryAttention</span>)         â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ add_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ multi_query_attention_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>â€¦ â”‚\n",
       "â”‚                               â”‚                           â”‚                 â”‚ input_layer_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization_6         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> â”‚ add_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]                â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)          â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_37 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> â”‚ layer_normalization_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">â€¦</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_38 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)             â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> â”‚ dense_37[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ add_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ layer_normalization_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">â€¦</span> â”‚\n",
       "â”‚                               â”‚                           â”‚                 â”‚ dense_38[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization_7         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> â”‚ add_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]                â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)          â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_average_pooling1d_3    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ layer_normalization_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">â€¦</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)      â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_39 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                 â”‚              <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> â”‚ global_average_pooling1d_â€¦ â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer_3 (\u001b[38;5;33mInputLayer\u001b[0m)    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m1\u001b[0m)             â”‚               \u001b[38;5;34m0\u001b[0m â”‚ -                          â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ multi_query_attention_3       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)            â”‚          \u001b[38;5;34m17,216\u001b[0m â”‚ input_layer_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        â”‚\n",
       "â”‚ (\u001b[38;5;33mMultiQueryAttention\u001b[0m)         â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ add_6 (\u001b[38;5;33mAdd\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)            â”‚               \u001b[38;5;34m0\u001b[0m â”‚ multi_query_attention_3[\u001b[38;5;34m0\u001b[0mâ€¦ â”‚\n",
       "â”‚                               â”‚                           â”‚                 â”‚ input_layer_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization_6         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)            â”‚             \u001b[38;5;34m128\u001b[0m â”‚ add_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]                â”‚\n",
       "â”‚ (\u001b[38;5;33mLayerNormalization\u001b[0m)          â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_37 (\u001b[38;5;33mDense\u001b[0m)              â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)           â”‚           \u001b[38;5;34m8,320\u001b[0m â”‚ layer_normalization_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34mâ€¦\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_38 (\u001b[38;5;33mDense\u001b[0m)              â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m1\u001b[0m)             â”‚             \u001b[38;5;34m129\u001b[0m â”‚ dense_37[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ add_7 (\u001b[38;5;33mAdd\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)            â”‚               \u001b[38;5;34m0\u001b[0m â”‚ layer_normalization_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34mâ€¦\u001b[0m â”‚\n",
       "â”‚                               â”‚                           â”‚                 â”‚ dense_38[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization_7         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)            â”‚             \u001b[38;5;34m128\u001b[0m â”‚ add_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]                â”‚\n",
       "â”‚ (\u001b[38;5;33mLayerNormalization\u001b[0m)          â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_average_pooling1d_3    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                â”‚               \u001b[38;5;34m0\u001b[0m â”‚ layer_normalization_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34mâ€¦\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)      â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_39 (\u001b[38;5;33mDense\u001b[0m)              â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                 â”‚              \u001b[38;5;34m65\u001b[0m â”‚ global_average_pooling1d_â€¦ â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,986</span> (101.51 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m25,986\u001b[0m (101.51 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,986</span> (101.51 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m25,986\u001b[0m (101.51 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 13ms/step - loss: 0.1736 - mae: 0.2788 - val_loss: 0.0512 - val_mae: 0.1916\n",
      "Epoch 2/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0124 - mae: 0.0892 - val_loss: 0.0373 - val_mae: 0.1549\n",
      "Epoch 3/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.0103 - mae: 0.0821 - val_loss: 0.0405 - val_mae: 0.1684\n",
      "Epoch 4/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.0079 - mae: 0.0719 - val_loss: 0.0337 - val_mae: 0.1474\n",
      "Epoch 5/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.0076 - mae: 0.0712 - val_loss: 0.0352 - val_mae: 0.1581\n",
      "Epoch 6/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0072 - mae: 0.0697 - val_loss: 0.0334 - val_mae: 0.1556\n",
      "Epoch 7/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0065 - mae: 0.0648 - val_loss: 0.0288 - val_mae: 0.1421\n",
      "Epoch 8/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0062 - mae: 0.0642 - val_loss: 0.0310 - val_mae: 0.1532\n",
      "Epoch 9/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.0060 - mae: 0.0623 - val_loss: 0.0266 - val_mae: 0.1359\n",
      "Epoch 10/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0057 - mae: 0.0618 - val_loss: 0.0259 - val_mae: 0.1344\n",
      "Epoch 11/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0062 - mae: 0.0635 - val_loss: 0.0252 - val_mae: 0.1349\n",
      "Epoch 12/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.0057 - mae: 0.0603 - val_loss: 0.0278 - val_mae: 0.1476\n",
      "Epoch 13/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0057 - mae: 0.0612 - val_loss: 0.0264 - val_mae: 0.1430\n",
      "Epoch 14/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0052 - mae: 0.0576 - val_loss: 0.0234 - val_mae: 0.1294\n",
      "Epoch 15/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0050 - mae: 0.0566 - val_loss: 0.0247 - val_mae: 0.1374\n",
      "Epoch 16/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0051 - mae: 0.0567 - val_loss: 0.0221 - val_mae: 0.1252\n",
      "Epoch 17/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0051 - mae: 0.0572 - val_loss: 0.0254 - val_mae: 0.1400\n",
      "Epoch 18/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0051 - mae: 0.0572 - val_loss: 0.0211 - val_mae: 0.1225\n",
      "Epoch 19/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.0049 - mae: 0.0553 - val_loss: 0.0214 - val_mae: 0.1114\n",
      "Epoch 20/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0051 - mae: 0.0560 - val_loss: 0.0204 - val_mae: 0.1193\n",
      "Epoch 21/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0053 - mae: 0.0570 - val_loss: 0.0198 - val_mae: 0.1092\n",
      "Epoch 22/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0044 - mae: 0.0523 - val_loss: 0.0198 - val_mae: 0.1168\n",
      "Epoch 23/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.0044 - mae: 0.0517 - val_loss: 0.0193 - val_mae: 0.1093\n",
      "Epoch 24/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.0045 - mae: 0.0520 - val_loss: 0.0194 - val_mae: 0.1147\n",
      "Epoch 25/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0047 - mae: 0.0529 - val_loss: 0.0193 - val_mae: 0.1134\n",
      "Epoch 26/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.0048 - mae: 0.0537 - val_loss: 0.0203 - val_mae: 0.1109\n",
      "Epoch 27/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0045 - mae: 0.0521 - val_loss: 0.0188 - val_mae: 0.1060\n",
      "Epoch 28/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0048 - mae: 0.0539 - val_loss: 0.0198 - val_mae: 0.1113\n",
      "Epoch 29/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0049 - mae: 0.0543 - val_loss: 0.0193 - val_mae: 0.1131\n",
      "Epoch 30/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0050 - mae: 0.0551 - val_loss: 0.0190 - val_mae: 0.1094\n",
      "Epoch 31/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0046 - mae: 0.0523 - val_loss: 0.0189 - val_mae: 0.1086\n",
      "Epoch 32/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.0044 - mae: 0.0506 - val_loss: 0.0185 - val_mae: 0.1052\n",
      "Epoch 33/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.0044 - mae: 0.0510 - val_loss: 0.0209 - val_mae: 0.1159\n",
      "Epoch 34/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0048 - mae: 0.0542 - val_loss: 0.0197 - val_mae: 0.1127\n",
      "Epoch 35/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0043 - mae: 0.0496 - val_loss: 0.0235 - val_mae: 0.1249\n",
      "Epoch 36/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0053 - mae: 0.0573 - val_loss: 0.0186 - val_mae: 0.1072\n",
      "Epoch 37/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0044 - mae: 0.0513 - val_loss: 0.0188 - val_mae: 0.1084\n",
      "Epoch 38/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0047 - mae: 0.0533 - val_loss: 0.0218 - val_mae: 0.1194\n",
      "Epoch 39/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0046 - mae: 0.0518 - val_loss: 0.0195 - val_mae: 0.1120\n",
      "Epoch 40/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0045 - mae: 0.0515 - val_loss: 0.0190 - val_mae: 0.1089\n",
      "Epoch 41/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.0045 - mae: 0.0515 - val_loss: 0.0195 - val_mae: 0.1123\n",
      "Epoch 42/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.0048 - mae: 0.0537 - val_loss: 0.0214 - val_mae: 0.1178\n",
      "Epoch 43/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0044 - mae: 0.0508 - val_loss: 0.0187 - val_mae: 0.1084\n",
      "Epoch 44/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0046 - mae: 0.0535 - val_loss: 0.0195 - val_mae: 0.1118\n",
      "Epoch 45/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0047 - mae: 0.0530 - val_loss: 0.0185 - val_mae: 0.1070\n",
      "Epoch 46/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0047 - mae: 0.0538 - val_loss: 0.0185 - val_mae: 0.1071\n",
      "Epoch 47/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.0048 - mae: 0.0532 - val_loss: 0.0196 - val_mae: 0.1118\n",
      "Epoch 48/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0044 - mae: 0.0497 - val_loss: 0.0203 - val_mae: 0.1145\n",
      "Epoch 49/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0042 - mae: 0.0495 - val_loss: 0.0202 - val_mae: 0.1145\n",
      "Epoch 50/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0043 - mae: 0.0500 - val_loss: 0.0185 - val_mae: 0.1053\n",
      "Epoch 51/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0045 - mae: 0.0505 - val_loss: 0.0190 - val_mae: 0.1104\n",
      "Epoch 52/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0042 - mae: 0.0487 - val_loss: 0.0249 - val_mae: 0.1293\n",
      "Epoch 53/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.0043 - mae: 0.0506 - val_loss: 0.0187 - val_mae: 0.1078\n",
      "Epoch 54/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.0048 - mae: 0.0538 - val_loss: 0.0198 - val_mae: 0.1145\n",
      "Epoch 55/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0044 - mae: 0.0502 - val_loss: 0.0200 - val_mae: 0.1141\n",
      "Epoch 56/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.0044 - mae: 0.0506 - val_loss: 0.0197 - val_mae: 0.1125\n",
      "Epoch 57/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.0042 - mae: 0.0492 - val_loss: 0.0188 - val_mae: 0.1085\n",
      "Epoch 58/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0043 - mae: 0.0498 - val_loss: 0.0195 - val_mae: 0.1122\n",
      "Epoch 59/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.0044 - mae: 0.0514 - val_loss: 0.0186 - val_mae: 0.1068\n",
      "Epoch 60/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0045 - mae: 0.0513 - val_loss: 0.0190 - val_mae: 0.1096\n",
      "Epoch 61/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.0048 - mae: 0.0537 - val_loss: 0.0206 - val_mae: 0.1163\n",
      "Epoch 62/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.0043 - mae: 0.0497 - val_loss: 0.0188 - val_mae: 0.1088\n",
      "Epoch 63/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.0043 - mae: 0.0495 - val_loss: 0.0210 - val_mae: 0.1167\n",
      "Epoch 64/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0047 - mae: 0.0536 - val_loss: 0.0191 - val_mae: 0.1106\n",
      "Epoch 65/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0042 - mae: 0.0489 - val_loss: 0.0198 - val_mae: 0.1135\n",
      "Epoch 66/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.0047 - mae: 0.0524 - val_loss: 0.0204 - val_mae: 0.1146\n",
      "Epoch 67/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0042 - mae: 0.0497 - val_loss: 0.0193 - val_mae: 0.1101\n",
      "Epoch 68/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0044 - mae: 0.0507 - val_loss: 0.0205 - val_mae: 0.1155\n",
      "Epoch 69/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.0041 - mae: 0.0482 - val_loss: 0.0198 - val_mae: 0.1124\n",
      "Epoch 70/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.0045 - mae: 0.0511 - val_loss: 0.0208 - val_mae: 0.1161\n",
      "Epoch 71/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.0041 - mae: 0.0483 - val_loss: 0.0203 - val_mae: 0.1138\n",
      "Epoch 72/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0044 - mae: 0.0507 - val_loss: 0.0195 - val_mae: 0.1125\n",
      "Epoch 73/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0045 - mae: 0.0514 - val_loss: 0.0229 - val_mae: 0.1227\n",
      "Epoch 74/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0045 - mae: 0.0521 - val_loss: 0.0206 - val_mae: 0.1154\n",
      "Epoch 75/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.0041 - mae: 0.0487 - val_loss: 0.0190 - val_mae: 0.1077\n",
      "Epoch 76/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0043 - mae: 0.0493 - val_loss: 0.0189 - val_mae: 0.1126\n",
      "Epoch 77/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0046 - mae: 0.0520 - val_loss: 0.0216 - val_mae: 0.1190\n",
      "Epoch 78/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0044 - mae: 0.0505 - val_loss: 0.0183 - val_mae: 0.1054\n",
      "Epoch 79/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0043 - mae: 0.0502 - val_loss: 0.0203 - val_mae: 0.1140\n",
      "Epoch 80/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0041 - mae: 0.0484 - val_loss: 0.0229 - val_mae: 0.1234\n",
      "Epoch 81/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.0041 - mae: 0.0485 - val_loss: 0.0188 - val_mae: 0.1073\n",
      "Epoch 82/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0044 - mae: 0.0496 - val_loss: 0.0227 - val_mae: 0.1217\n",
      "Epoch 83/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.0047 - mae: 0.0530 - val_loss: 0.0195 - val_mae: 0.1107\n",
      "Epoch 84/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0042 - mae: 0.0496 - val_loss: 0.0183 - val_mae: 0.1055\n",
      "Epoch 85/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0044 - mae: 0.0511 - val_loss: 0.0192 - val_mae: 0.1101\n",
      "Epoch 86/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.0046 - mae: 0.0521 - val_loss: 0.0216 - val_mae: 0.1201\n",
      "Epoch 87/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.0043 - mae: 0.0487 - val_loss: 0.0187 - val_mae: 0.1074\n",
      "Epoch 88/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.0041 - mae: 0.0485 - val_loss: 0.0188 - val_mae: 0.1074\n",
      "Epoch 89/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0049 - mae: 0.0540 - val_loss: 0.0206 - val_mae: 0.1147\n",
      "Epoch 90/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0044 - mae: 0.0504 - val_loss: 0.0192 - val_mae: 0.1104\n",
      "Epoch 91/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.0045 - mae: 0.0512 - val_loss: 0.0194 - val_mae: 0.1102\n",
      "Epoch 92/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0045 - mae: 0.0508 - val_loss: 0.0195 - val_mae: 0.1118\n",
      "Epoch 93/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.0042 - mae: 0.0486 - val_loss: 0.0189 - val_mae: 0.1074\n",
      "Epoch 94/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.0043 - mae: 0.0493 - val_loss: 0.0190 - val_mae: 0.1081\n",
      "Epoch 95/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0043 - mae: 0.0502 - val_loss: 0.0212 - val_mae: 0.1190\n",
      "Epoch 96/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.0044 - mae: 0.0503 - val_loss: 0.0203 - val_mae: 0.1154\n",
      "Epoch 97/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0046 - mae: 0.0517 - val_loss: 0.0205 - val_mae: 0.1160\n",
      "Epoch 98/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0043 - mae: 0.0495 - val_loss: 0.0195 - val_mae: 0.1124\n",
      "Epoch 99/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0045 - mae: 0.0514 - val_loss: 0.0193 - val_mae: 0.1092\n",
      "Epoch 100/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.0044 - mae: 0.0500 - val_loss: 0.0191 - val_mae: 0.1080\n",
      "WARNING:tensorflow:5 out of the last 42 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x00000139BA645D00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
      "âœ… Done with capsicum_Chikmagalur_daily.csv | MAE=234.53, RMSE=305.88, R2=0.7323, MAPE=14.52%, Accuracy=85.48%\n",
      "Saved updated CSV with Date, Actual & Predicted: tat_mqa_output_csv\\capsicum_Chikmagalur_daily_tat_mqa_updated.csv\n",
      "ğŸš€ Processing: capsicum_Davangere_daily.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_4\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_4\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                  </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape              </span>â”ƒ<span style=\"font-weight: bold\">         Param # </span>â”ƒ<span style=\"font-weight: bold\"> Connected to               </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)             â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                          â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ multi_query_attention_4       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">17,216</span> â”‚ input_layer_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiQueryAttention</span>)         â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ add_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ multi_query_attention_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>â€¦ â”‚\n",
       "â”‚                               â”‚                           â”‚                 â”‚ input_layer_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization_8         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> â”‚ add_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]                â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)          â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_47 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> â”‚ layer_normalization_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">â€¦</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_48 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)             â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> â”‚ dense_47[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ add_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ layer_normalization_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">â€¦</span> â”‚\n",
       "â”‚                               â”‚                           â”‚                 â”‚ dense_48[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization_9         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> â”‚ add_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]                â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)          â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_average_pooling1d_4    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ layer_normalization_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">â€¦</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)      â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_49 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                 â”‚              <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> â”‚ global_average_pooling1d_â€¦ â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer_4 (\u001b[38;5;33mInputLayer\u001b[0m)    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m1\u001b[0m)             â”‚               \u001b[38;5;34m0\u001b[0m â”‚ -                          â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ multi_query_attention_4       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)            â”‚          \u001b[38;5;34m17,216\u001b[0m â”‚ input_layer_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        â”‚\n",
       "â”‚ (\u001b[38;5;33mMultiQueryAttention\u001b[0m)         â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ add_8 (\u001b[38;5;33mAdd\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)            â”‚               \u001b[38;5;34m0\u001b[0m â”‚ multi_query_attention_4[\u001b[38;5;34m0\u001b[0mâ€¦ â”‚\n",
       "â”‚                               â”‚                           â”‚                 â”‚ input_layer_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization_8         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)            â”‚             \u001b[38;5;34m128\u001b[0m â”‚ add_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]                â”‚\n",
       "â”‚ (\u001b[38;5;33mLayerNormalization\u001b[0m)          â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_47 (\u001b[38;5;33mDense\u001b[0m)              â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)           â”‚           \u001b[38;5;34m8,320\u001b[0m â”‚ layer_normalization_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34mâ€¦\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_48 (\u001b[38;5;33mDense\u001b[0m)              â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m1\u001b[0m)             â”‚             \u001b[38;5;34m129\u001b[0m â”‚ dense_47[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ add_9 (\u001b[38;5;33mAdd\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)            â”‚               \u001b[38;5;34m0\u001b[0m â”‚ layer_normalization_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34mâ€¦\u001b[0m â”‚\n",
       "â”‚                               â”‚                           â”‚                 â”‚ dense_48[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization_9         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)            â”‚             \u001b[38;5;34m128\u001b[0m â”‚ add_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]                â”‚\n",
       "â”‚ (\u001b[38;5;33mLayerNormalization\u001b[0m)          â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_average_pooling1d_4    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                â”‚               \u001b[38;5;34m0\u001b[0m â”‚ layer_normalization_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34mâ€¦\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)      â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_49 (\u001b[38;5;33mDense\u001b[0m)              â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                 â”‚              \u001b[38;5;34m65\u001b[0m â”‚ global_average_pooling1d_â€¦ â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,986</span> (101.51 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m25,986\u001b[0m (101.51 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,986</span> (101.51 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m25,986\u001b[0m (101.51 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 13ms/step - loss: 0.2075 - mae: 0.2588 - val_loss: 0.0153 - val_mae: 0.0870\n",
      "Epoch 2/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.0063 - mae: 0.0612 - val_loss: 0.0130 - val_mae: 0.0770\n",
      "Epoch 3/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.0053 - mae: 0.0551 - val_loss: 0.0112 - val_mae: 0.0698\n",
      "Epoch 4/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.0045 - mae: 0.0507 - val_loss: 0.0103 - val_mae: 0.0656\n",
      "Epoch 5/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0035 - mae: 0.0444 - val_loss: 0.0095 - val_mae: 0.0617\n",
      "Epoch 6/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.0030 - mae: 0.0411 - val_loss: 0.0093 - val_mae: 0.0606\n",
      "Epoch 7/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0026 - mae: 0.0377 - val_loss: 0.0102 - val_mae: 0.0653\n",
      "Epoch 8/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.0024 - mae: 0.0354 - val_loss: 0.0094 - val_mae: 0.0613\n",
      "Epoch 9/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.0024 - mae: 0.0352 - val_loss: 0.0074 - val_mae: 0.0521\n",
      "Epoch 10/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.0021 - mae: 0.0335 - val_loss: 0.0107 - val_mae: 0.0704\n",
      "Epoch 11/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.0021 - mae: 0.0335 - val_loss: 0.0069 - val_mae: 0.0503\n",
      "Epoch 12/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.0018 - mae: 0.0296 - val_loss: 0.0065 - val_mae: 0.0493\n",
      "Epoch 13/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.0019 - mae: 0.0317 - val_loss: 0.0062 - val_mae: 0.0491\n",
      "Epoch 14/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.0018 - mae: 0.0297 - val_loss: 0.0062 - val_mae: 0.0488\n",
      "Epoch 15/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.0016 - mae: 0.0286 - val_loss: 0.0062 - val_mae: 0.0490\n",
      "Epoch 16/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0017 - mae: 0.0287 - val_loss: 0.0063 - val_mae: 0.0495\n",
      "Epoch 17/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0015 - mae: 0.0269 - val_loss: 0.0066 - val_mae: 0.0516\n",
      "Epoch 18/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0017 - mae: 0.0284 - val_loss: 0.0068 - val_mae: 0.0540\n",
      "Epoch 19/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0016 - mae: 0.0280 - val_loss: 0.0073 - val_mae: 0.0562\n",
      "Epoch 20/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0016 - mae: 0.0285 - val_loss: 0.0079 - val_mae: 0.0584\n",
      "Epoch 21/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0018 - mae: 0.0296 - val_loss: 0.0092 - val_mae: 0.0633\n",
      "Epoch 22/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0017 - mae: 0.0284 - val_loss: 0.0074 - val_mae: 0.0526\n",
      "Epoch 23/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0017 - mae: 0.0290 - val_loss: 0.0069 - val_mae: 0.0515\n",
      "Epoch 24/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0017 - mae: 0.0290 - val_loss: 0.0099 - val_mae: 0.0662\n",
      "Epoch 25/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 20ms/step - loss: 0.0016 - mae: 0.0282 - val_loss: 0.0076 - val_mae: 0.0531\n",
      "Epoch 26/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - loss: 0.0017 - mae: 0.0280 - val_loss: 0.0081 - val_mae: 0.0540\n",
      "Epoch 27/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - loss: 0.0018 - mae: 0.0306 - val_loss: 0.0079 - val_mae: 0.0555\n",
      "Epoch 28/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0017 - mae: 0.0285 - val_loss: 0.0091 - val_mae: 0.0626\n",
      "Epoch 29/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0016 - mae: 0.0284 - val_loss: 0.0078 - val_mae: 0.0563\n",
      "Epoch 30/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0017 - mae: 0.0288 - val_loss: 0.0083 - val_mae: 0.0541\n",
      "Epoch 31/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0016 - mae: 0.0278 - val_loss: 0.0078 - val_mae: 0.0529\n",
      "Epoch 32/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0016 - mae: 0.0276 - val_loss: 0.0084 - val_mae: 0.0543\n",
      "Epoch 33/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.0015 - mae: 0.0268 - val_loss: 0.0082 - val_mae: 0.0545\n",
      "Epoch 34/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0016 - mae: 0.0266 - val_loss: 0.0087 - val_mae: 0.0580\n",
      "Epoch 35/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.0016 - mae: 0.0274 - val_loss: 0.0098 - val_mae: 0.0577\n",
      "Epoch 36/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.0015 - mae: 0.0268 - val_loss: 0.0092 - val_mae: 0.0557\n",
      "Epoch 37/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0015 - mae: 0.0266 - val_loss: 0.0086 - val_mae: 0.0546\n",
      "Epoch 38/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0014 - mae: 0.0265 - val_loss: 0.0097 - val_mae: 0.0575\n",
      "Epoch 39/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.0017 - mae: 0.0283 - val_loss: 0.0083 - val_mae: 0.0553\n",
      "Epoch 40/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.0016 - mae: 0.0267 - val_loss: 0.0094 - val_mae: 0.0586\n",
      "Epoch 41/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.0017 - mae: 0.0281 - val_loss: 0.0111 - val_mae: 0.0621\n",
      "Epoch 42/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.0015 - mae: 0.0262 - val_loss: 0.0083 - val_mae: 0.0543\n",
      "Epoch 43/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.0016 - mae: 0.0276 - val_loss: 0.0103 - val_mae: 0.0607\n",
      "Epoch 44/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0016 - mae: 0.0272 - val_loss: 0.0086 - val_mae: 0.0545\n",
      "Epoch 45/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0015 - mae: 0.0257 - val_loss: 0.0093 - val_mae: 0.0558\n",
      "Epoch 46/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.0014 - mae: 0.0260 - val_loss: 0.0128 - val_mae: 0.0712\n",
      "Epoch 47/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.0016 - mae: 0.0280 - val_loss: 0.0076 - val_mae: 0.0528\n",
      "Epoch 48/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0015 - mae: 0.0269 - val_loss: 0.0104 - val_mae: 0.0600\n",
      "Epoch 49/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0014 - mae: 0.0255 - val_loss: 0.0115 - val_mae: 0.0647\n",
      "Epoch 50/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0014 - mae: 0.0261 - val_loss: 0.0088 - val_mae: 0.0554\n",
      "Epoch 51/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.0015 - mae: 0.0267 - val_loss: 0.0085 - val_mae: 0.0548\n",
      "Epoch 52/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.0014 - mae: 0.0261 - val_loss: 0.0104 - val_mae: 0.0588\n",
      "Epoch 53/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.0014 - mae: 0.0252 - val_loss: 0.0087 - val_mae: 0.0572\n",
      "Epoch 54/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0014 - mae: 0.0251 - val_loss: 0.0093 - val_mae: 0.0582\n",
      "Epoch 55/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0015 - mae: 0.0273 - val_loss: 0.0123 - val_mae: 0.0678\n",
      "Epoch 56/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.0015 - mae: 0.0260 - val_loss: 0.0095 - val_mae: 0.0584\n",
      "Epoch 57/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.0014 - mae: 0.0266 - val_loss: 0.0135 - val_mae: 0.0704\n",
      "Epoch 58/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.0015 - mae: 0.0257 - val_loss: 0.0099 - val_mae: 0.0599\n",
      "Epoch 59/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0014 - mae: 0.0254 - val_loss: 0.0088 - val_mae: 0.0548\n",
      "Epoch 60/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.0014 - mae: 0.0256 - val_loss: 0.0077 - val_mae: 0.0535\n",
      "Epoch 61/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0013 - mae: 0.0247 - val_loss: 0.0081 - val_mae: 0.0538\n",
      "Epoch 62/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.0016 - mae: 0.0277 - val_loss: 0.0126 - val_mae: 0.0722\n",
      "Epoch 63/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.0015 - mae: 0.0262 - val_loss: 0.0088 - val_mae: 0.0557\n",
      "Epoch 64/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.0014 - mae: 0.0254 - val_loss: 0.0091 - val_mae: 0.0581\n",
      "Epoch 65/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.0014 - mae: 0.0247 - val_loss: 0.0126 - val_mae: 0.0701\n",
      "Epoch 66/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0015 - mae: 0.0262 - val_loss: 0.0080 - val_mae: 0.0536\n",
      "Epoch 67/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0016 - mae: 0.0274 - val_loss: 0.0081 - val_mae: 0.0539\n",
      "Epoch 68/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0015 - mae: 0.0268 - val_loss: 0.0071 - val_mae: 0.0522\n",
      "Epoch 69/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0014 - mae: 0.0256 - val_loss: 0.0088 - val_mae: 0.0555\n",
      "Epoch 70/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0014 - mae: 0.0256 - val_loss: 0.0077 - val_mae: 0.0532\n",
      "Epoch 71/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0013 - mae: 0.0252 - val_loss: 0.0075 - val_mae: 0.0530\n",
      "Epoch 72/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.0015 - mae: 0.0263 - val_loss: 0.0072 - val_mae: 0.0522\n",
      "Epoch 73/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0015 - mae: 0.0261 - val_loss: 0.0075 - val_mae: 0.0527\n",
      "Epoch 74/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0014 - mae: 0.0255 - val_loss: 0.0096 - val_mae: 0.0609\n",
      "Epoch 75/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0014 - mae: 0.0261 - val_loss: 0.0096 - val_mae: 0.0578\n",
      "Epoch 76/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.0013 - mae: 0.0252 - val_loss: 0.0099 - val_mae: 0.0610\n",
      "Epoch 77/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0014 - mae: 0.0256 - val_loss: 0.0077 - val_mae: 0.0532\n",
      "Epoch 78/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0015 - mae: 0.0270 - val_loss: 0.0093 - val_mae: 0.0557\n",
      "Epoch 79/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0015 - mae: 0.0258 - val_loss: 0.0108 - val_mae: 0.0627\n",
      "Epoch 80/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0014 - mae: 0.0256 - val_loss: 0.0075 - val_mae: 0.0539\n",
      "Epoch 81/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0014 - mae: 0.0263 - val_loss: 0.0064 - val_mae: 0.0521\n",
      "Epoch 82/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.0014 - mae: 0.0253 - val_loss: 0.0075 - val_mae: 0.0529\n",
      "Epoch 83/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.0014 - mae: 0.0263 - val_loss: 0.0077 - val_mae: 0.0524\n",
      "Epoch 84/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.0014 - mae: 0.0252 - val_loss: 0.0115 - val_mae: 0.0645\n",
      "Epoch 85/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.0013 - mae: 0.0249 - val_loss: 0.0082 - val_mae: 0.0535\n",
      "Epoch 86/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0014 - mae: 0.0260 - val_loss: 0.0125 - val_mae: 0.0688\n",
      "Epoch 87/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0013 - mae: 0.0255 - val_loss: 0.0081 - val_mae: 0.0533\n",
      "Epoch 88/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0014 - mae: 0.0257 - val_loss: 0.0105 - val_mae: 0.0583\n",
      "Epoch 89/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.0013 - mae: 0.0248 - val_loss: 0.0087 - val_mae: 0.0544\n",
      "Epoch 90/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0014 - mae: 0.0253 - val_loss: 0.0082 - val_mae: 0.0529\n",
      "Epoch 91/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.0014 - mae: 0.0259 - val_loss: 0.0088 - val_mae: 0.0538\n",
      "Epoch 92/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.0014 - mae: 0.0263 - val_loss: 0.0124 - val_mae: 0.0673\n",
      "Epoch 93/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0014 - mae: 0.0260 - val_loss: 0.0100 - val_mae: 0.0565\n",
      "Epoch 94/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0013 - mae: 0.0242 - val_loss: 0.0089 - val_mae: 0.0538\n",
      "Epoch 95/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0013 - mae: 0.0244 - val_loss: 0.0102 - val_mae: 0.0570\n",
      "Epoch 96/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0014 - mae: 0.0257 - val_loss: 0.0100 - val_mae: 0.0554\n",
      "Epoch 97/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.0013 - mae: 0.0253 - val_loss: 0.0104 - val_mae: 0.0581\n",
      "Epoch 98/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0013 - mae: 0.0251 - val_loss: 0.0078 - val_mae: 0.0521\n",
      "Epoch 99/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0015 - mae: 0.0261 - val_loss: 0.0079 - val_mae: 0.0523\n",
      "Epoch 100/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.0013 - mae: 0.0249 - val_loss: 0.0110 - val_mae: 0.0587\n",
      "\u001b[1m140/140\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step\n",
      "âœ… Done with capsicum_Davangere_daily.csv | MAE=504.89, RMSE=914.67, R2=0.722, MAPE=15.69%, Accuracy=84.31%\n",
      "Saved updated CSV with Date, Actual & Predicted: tat_mqa_output_csv\\capsicum_Davangere_daily_tat_mqa_updated.csv\n",
      "ğŸš€ Processing: capsicum_Dharwad_daily.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_5\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_5\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                  </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape              </span>â”ƒ<span style=\"font-weight: bold\">         Param # </span>â”ƒ<span style=\"font-weight: bold\"> Connected to               </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)             â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                          â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ multi_query_attention_5       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">17,216</span> â”‚ input_layer_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiQueryAttention</span>)         â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ add_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ multi_query_attention_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>â€¦ â”‚\n",
       "â”‚                               â”‚                           â”‚                 â”‚ input_layer_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization_10        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> â”‚ add_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]               â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)          â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_57 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> â”‚ layer_normalization_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]â€¦ â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_58 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)             â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> â”‚ dense_57[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ add_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ layer_normalization_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]â€¦ â”‚\n",
       "â”‚                               â”‚                           â”‚                 â”‚ dense_58[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization_11        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> â”‚ add_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]               â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)          â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_average_pooling1d_5    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ layer_normalization_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]â€¦ â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)      â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_59 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                 â”‚              <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> â”‚ global_average_pooling1d_â€¦ â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer_5 (\u001b[38;5;33mInputLayer\u001b[0m)    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m1\u001b[0m)             â”‚               \u001b[38;5;34m0\u001b[0m â”‚ -                          â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ multi_query_attention_5       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)            â”‚          \u001b[38;5;34m17,216\u001b[0m â”‚ input_layer_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        â”‚\n",
       "â”‚ (\u001b[38;5;33mMultiQueryAttention\u001b[0m)         â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ add_10 (\u001b[38;5;33mAdd\u001b[0m)                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)            â”‚               \u001b[38;5;34m0\u001b[0m â”‚ multi_query_attention_5[\u001b[38;5;34m0\u001b[0mâ€¦ â”‚\n",
       "â”‚                               â”‚                           â”‚                 â”‚ input_layer_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization_10        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)            â”‚             \u001b[38;5;34m128\u001b[0m â”‚ add_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]               â”‚\n",
       "â”‚ (\u001b[38;5;33mLayerNormalization\u001b[0m)          â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_57 (\u001b[38;5;33mDense\u001b[0m)              â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)           â”‚           \u001b[38;5;34m8,320\u001b[0m â”‚ layer_normalization_10[\u001b[38;5;34m0\u001b[0m]â€¦ â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_58 (\u001b[38;5;33mDense\u001b[0m)              â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m1\u001b[0m)             â”‚             \u001b[38;5;34m129\u001b[0m â”‚ dense_57[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ add_11 (\u001b[38;5;33mAdd\u001b[0m)                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)            â”‚               \u001b[38;5;34m0\u001b[0m â”‚ layer_normalization_10[\u001b[38;5;34m0\u001b[0m]â€¦ â”‚\n",
       "â”‚                               â”‚                           â”‚                 â”‚ dense_58[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization_11        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)            â”‚             \u001b[38;5;34m128\u001b[0m â”‚ add_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]               â”‚\n",
       "â”‚ (\u001b[38;5;33mLayerNormalization\u001b[0m)          â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_average_pooling1d_5    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                â”‚               \u001b[38;5;34m0\u001b[0m â”‚ layer_normalization_11[\u001b[38;5;34m0\u001b[0m]â€¦ â”‚\n",
       "â”‚ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)      â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_59 (\u001b[38;5;33mDense\u001b[0m)              â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                 â”‚              \u001b[38;5;34m65\u001b[0m â”‚ global_average_pooling1d_â€¦ â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,986</span> (101.51 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m25,986\u001b[0m (101.51 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,986</span> (101.51 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m25,986\u001b[0m (101.51 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 80ms/step - loss: 0.7713 - mae: 0.7690 - val_loss: 0.2363 - val_mae: 0.4813\n",
      "Epoch 2/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1622 - mae: 0.3423 - val_loss: 0.7207 - val_mae: 0.8462\n",
      "Epoch 3/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1127 - mae: 0.2533 - val_loss: 0.1878 - val_mae: 0.4279\n",
      "Epoch 4/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0924 - mae: 0.2544 - val_loss: 0.1919 - val_mae: 0.4327\n",
      "Epoch 5/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.1256 - mae: 0.2980 - val_loss: 0.5112 - val_mae: 0.7116\n",
      "Epoch 6/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.1005 - mae: 0.2512 - val_loss: 0.2412 - val_mae: 0.4863\n",
      "Epoch 7/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0820 - mae: 0.2385 - val_loss: 0.1346 - val_mae: 0.3603\n",
      "Epoch 8/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0953 - mae: 0.2605 - val_loss: 0.2196 - val_mae: 0.4634\n",
      "Epoch 9/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0693 - mae: 0.2243 - val_loss: 0.1863 - val_mae: 0.4260\n",
      "Epoch 10/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0722 - mae: 0.2205 - val_loss: 0.2417 - val_mae: 0.4866\n",
      "Epoch 11/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0871 - mae: 0.2428 - val_loss: 0.2078 - val_mae: 0.4503\n",
      "Epoch 12/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0823 - mae: 0.2346 - val_loss: 0.1336 - val_mae: 0.3585\n",
      "Epoch 13/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0744 - mae: 0.2249 - val_loss: 0.1510 - val_mae: 0.3819\n",
      "Epoch 14/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0691 - mae: 0.2168 - val_loss: 0.0942 - val_mae: 0.2984\n",
      "Epoch 15/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0784 - mae: 0.2364 - val_loss: 0.1660 - val_mae: 0.4010\n",
      "Epoch 16/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0900 - mae: 0.2418 - val_loss: 0.1510 - val_mae: 0.3817\n",
      "Epoch 17/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0719 - mae: 0.2204 - val_loss: 0.1955 - val_mae: 0.4360\n",
      "Epoch 18/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0867 - mae: 0.2375 - val_loss: 0.0976 - val_mae: 0.3037\n",
      "Epoch 19/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0671 - mae: 0.2143 - val_loss: 0.0273 - val_mae: 0.1480\n",
      "Epoch 20/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0782 - mae: 0.2335 - val_loss: 0.0884 - val_mae: 0.2881\n",
      "Epoch 21/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0761 - mae: 0.2312 - val_loss: 0.1632 - val_mae: 0.3971\n",
      "Epoch 22/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0965 - mae: 0.2613 - val_loss: 0.1338 - val_mae: 0.3583\n",
      "Epoch 23/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0712 - mae: 0.2244 - val_loss: 0.0729 - val_mae: 0.2597\n",
      "Epoch 24/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0897 - mae: 0.2528 - val_loss: 0.0320 - val_mae: 0.1632\n",
      "Epoch 25/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0867 - mae: 0.2368 - val_loss: 0.0563 - val_mae: 0.2256\n",
      "Epoch 26/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0771 - mae: 0.2339 - val_loss: 0.0548 - val_mae: 0.2222\n",
      "Epoch 27/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0781 - mae: 0.2337 - val_loss: 0.1543 - val_mae: 0.3859\n",
      "Epoch 28/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0845 - mae: 0.2294 - val_loss: 0.0403 - val_mae: 0.1869\n",
      "Epoch 29/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0711 - mae: 0.2218 - val_loss: 0.0246 - val_mae: 0.1387\n",
      "Epoch 30/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0935 - mae: 0.2510 - val_loss: 0.1274 - val_mae: 0.3493\n",
      "Epoch 31/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0748 - mae: 0.2306 - val_loss: 0.1494 - val_mae: 0.3794\n",
      "Epoch 32/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0859 - mae: 0.2343 - val_loss: 0.0334 - val_mae: 0.1675\n",
      "Epoch 33/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0834 - mae: 0.2352 - val_loss: 0.0491 - val_mae: 0.2093\n",
      "Epoch 34/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0847 - mae: 0.2412 - val_loss: 0.0432 - val_mae: 0.1947\n",
      "Epoch 35/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0810 - mae: 0.2433 - val_loss: 0.1019 - val_mae: 0.3108\n",
      "Epoch 36/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0860 - mae: 0.2500 - val_loss: 0.1860 - val_mae: 0.4251\n",
      "Epoch 37/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0948 - mae: 0.2605 - val_loss: 0.1191 - val_mae: 0.3373\n",
      "Epoch 38/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0792 - mae: 0.2319 - val_loss: 0.0527 - val_mae: 0.2177\n",
      "Epoch 39/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0750 - mae: 0.2272 - val_loss: 0.2012 - val_mae: 0.4426\n",
      "Epoch 40/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0813 - mae: 0.2223 - val_loss: 0.0320 - val_mae: 0.1637\n",
      "Epoch 41/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0803 - mae: 0.2381 - val_loss: 0.0961 - val_mae: 0.3014\n",
      "Epoch 42/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0835 - mae: 0.2452 - val_loss: 0.2156 - val_mae: 0.4586\n",
      "Epoch 43/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0913 - mae: 0.2462 - val_loss: 0.1265 - val_mae: 0.3482\n",
      "Epoch 44/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0797 - mae: 0.2385 - val_loss: 0.0838 - val_mae: 0.2804\n",
      "Epoch 45/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0678 - mae: 0.2158 - val_loss: 0.0791 - val_mae: 0.2719\n",
      "Epoch 46/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0797 - mae: 0.2349 - val_loss: 0.1710 - val_mae: 0.4071\n",
      "Epoch 47/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0836 - mae: 0.2294 - val_loss: 0.0454 - val_mae: 0.2004\n",
      "Epoch 48/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0815 - mae: 0.2432 - val_loss: 0.1757 - val_mae: 0.4129\n",
      "Epoch 49/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0874 - mae: 0.2498 - val_loss: 0.0976 - val_mae: 0.3040\n",
      "Epoch 50/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0749 - mae: 0.2313 - val_loss: 0.0841 - val_mae: 0.2810\n",
      "Epoch 51/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0712 - mae: 0.2239 - val_loss: 0.0979 - val_mae: 0.3045\n",
      "Epoch 52/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0675 - mae: 0.2147 - val_loss: 0.1338 - val_mae: 0.3586\n",
      "Epoch 53/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0824 - mae: 0.2392 - val_loss: 0.1145 - val_mae: 0.3306\n",
      "Epoch 54/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0770 - mae: 0.2338 - val_loss: 0.1675 - val_mae: 0.4029\n",
      "Epoch 55/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0845 - mae: 0.2409 - val_loss: 0.0626 - val_mae: 0.2398\n",
      "Epoch 56/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0793 - mae: 0.2337 - val_loss: 0.1308 - val_mae: 0.3545\n",
      "Epoch 57/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0731 - mae: 0.2279 - val_loss: 0.0895 - val_mae: 0.2904\n",
      "Epoch 58/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0699 - mae: 0.2213 - val_loss: 0.1176 - val_mae: 0.3353\n",
      "Epoch 59/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0615 - mae: 0.2089 - val_loss: 0.0723 - val_mae: 0.2590\n",
      "Epoch 60/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0739 - mae: 0.2244 - val_loss: 0.0621 - val_mae: 0.2386\n",
      "Epoch 61/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0742 - mae: 0.2316 - val_loss: 0.1652 - val_mae: 0.4001\n",
      "Epoch 62/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0840 - mae: 0.2327 - val_loss: 0.0599 - val_mae: 0.2341\n",
      "Epoch 63/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0815 - mae: 0.2395 - val_loss: 0.1724 - val_mae: 0.4090\n",
      "Epoch 64/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0811 - mae: 0.2349 - val_loss: 0.0684 - val_mae: 0.2516\n",
      "Epoch 65/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0718 - mae: 0.2154 - val_loss: 0.1014 - val_mae: 0.3103\n",
      "Epoch 66/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0796 - mae: 0.2371 - val_loss: 0.0956 - val_mae: 0.3007\n",
      "Epoch 67/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0735 - mae: 0.2244 - val_loss: 0.0575 - val_mae: 0.2289\n",
      "Epoch 68/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0706 - mae: 0.2207 - val_loss: 0.1757 - val_mae: 0.4130\n",
      "Epoch 69/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0825 - mae: 0.2379 - val_loss: 0.0935 - val_mae: 0.2974\n",
      "Epoch 70/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0787 - mae: 0.2325 - val_loss: 0.1206 - val_mae: 0.3399\n",
      "Epoch 71/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0761 - mae: 0.2324 - val_loss: 0.1149 - val_mae: 0.3314\n",
      "Epoch 72/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0837 - mae: 0.2492 - val_loss: 0.1047 - val_mae: 0.3156\n",
      "Epoch 73/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0663 - mae: 0.2170 - val_loss: 0.1043 - val_mae: 0.3150\n",
      "Epoch 74/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0816 - mae: 0.2414 - val_loss: 0.1590 - val_mae: 0.3923\n",
      "Epoch 75/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0864 - mae: 0.2510 - val_loss: 0.1159 - val_mae: 0.3328\n",
      "Epoch 76/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0774 - mae: 0.2294 - val_loss: 0.0539 - val_mae: 0.2210\n",
      "Epoch 77/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0771 - mae: 0.2280 - val_loss: 0.1441 - val_mae: 0.3729\n",
      "Epoch 78/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0653 - mae: 0.2111 - val_loss: 0.0975 - val_mae: 0.3040\n",
      "Epoch 79/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0682 - mae: 0.2193 - val_loss: 0.1159 - val_mae: 0.3329\n",
      "Epoch 80/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0666 - mae: 0.2189 - val_loss: 0.1071 - val_mae: 0.3194\n",
      "Epoch 81/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0767 - mae: 0.2309 - val_loss: 0.1622 - val_mae: 0.3963\n",
      "Epoch 82/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0745 - mae: 0.2264 - val_loss: 0.0344 - val_mae: 0.1713\n",
      "Epoch 83/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0824 - mae: 0.2295 - val_loss: 0.1292 - val_mae: 0.3523\n",
      "Epoch 84/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0719 - mae: 0.2241 - val_loss: 0.1480 - val_mae: 0.3781\n",
      "Epoch 85/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0812 - mae: 0.2367 - val_loss: 0.0721 - val_mae: 0.2589\n",
      "Epoch 86/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0797 - mae: 0.2368 - val_loss: 0.1110 - val_mae: 0.3254\n",
      "Epoch 87/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0773 - mae: 0.2332 - val_loss: 0.0868 - val_mae: 0.2859\n",
      "Epoch 88/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0619 - mae: 0.2025 - val_loss: 0.1710 - val_mae: 0.4073\n",
      "Epoch 89/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0656 - mae: 0.2109 - val_loss: 0.0809 - val_mae: 0.2752\n",
      "Epoch 90/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0791 - mae: 0.2370 - val_loss: 0.1517 - val_mae: 0.3828\n",
      "Epoch 91/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0654 - mae: 0.2191 - val_loss: 0.1075 - val_mae: 0.3199\n",
      "Epoch 92/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0682 - mae: 0.2178 - val_loss: 0.1147 - val_mae: 0.3311\n",
      "Epoch 93/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0757 - mae: 0.2308 - val_loss: 0.1850 - val_mae: 0.4240\n",
      "Epoch 94/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0726 - mae: 0.2205 - val_loss: 0.1273 - val_mae: 0.3494\n",
      "Epoch 95/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0734 - mae: 0.2236 - val_loss: 0.0929 - val_mae: 0.2963\n",
      "Epoch 96/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0637 - mae: 0.2094 - val_loss: 0.1392 - val_mae: 0.3661\n",
      "Epoch 97/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0706 - mae: 0.2197 - val_loss: 0.1546 - val_mae: 0.3866\n",
      "Epoch 98/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0677 - mae: 0.2180 - val_loss: 0.1463 - val_mae: 0.3756\n",
      "Epoch 99/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0772 - mae: 0.2267 - val_loss: 0.1527 - val_mae: 0.3840\n",
      "Epoch 100/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0688 - mae: 0.2208 - val_loss: 0.1475 - val_mae: 0.3773\n",
      "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step\n",
      "âœ… Done with capsicum_Dharwad_daily.csv | MAE=948.78, RMSE=1096.2, R2=0.2544, MAPE=55.02%, Accuracy=44.98%\n",
      "Saved updated CSV with Date, Actual & Predicted: tat_mqa_output_csv\\capsicum_Dharwad_daily_tat_mqa_updated.csv\n",
      "ğŸš€ Processing: capsicum_Hassan_daily.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_6\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_6\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                  </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape              </span>â”ƒ<span style=\"font-weight: bold\">         Param # </span>â”ƒ<span style=\"font-weight: bold\"> Connected to               </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)             â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                          â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ multi_query_attention_6       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">17,216</span> â”‚ input_layer_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiQueryAttention</span>)         â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ add_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ multi_query_attention_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>â€¦ â”‚\n",
       "â”‚                               â”‚                           â”‚                 â”‚ input_layer_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization_12        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> â”‚ add_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]               â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)          â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_67 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> â”‚ layer_normalization_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]â€¦ â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_68 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)             â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> â”‚ dense_67[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ add_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ layer_normalization_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]â€¦ â”‚\n",
       "â”‚                               â”‚                           â”‚                 â”‚ dense_68[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization_13        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> â”‚ add_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]               â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)          â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_average_pooling1d_6    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ layer_normalization_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]â€¦ â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)      â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_69 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                 â”‚              <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> â”‚ global_average_pooling1d_â€¦ â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer_6 (\u001b[38;5;33mInputLayer\u001b[0m)    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m1\u001b[0m)             â”‚               \u001b[38;5;34m0\u001b[0m â”‚ -                          â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ multi_query_attention_6       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)            â”‚          \u001b[38;5;34m17,216\u001b[0m â”‚ input_layer_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        â”‚\n",
       "â”‚ (\u001b[38;5;33mMultiQueryAttention\u001b[0m)         â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ add_12 (\u001b[38;5;33mAdd\u001b[0m)                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)            â”‚               \u001b[38;5;34m0\u001b[0m â”‚ multi_query_attention_6[\u001b[38;5;34m0\u001b[0mâ€¦ â”‚\n",
       "â”‚                               â”‚                           â”‚                 â”‚ input_layer_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization_12        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)            â”‚             \u001b[38;5;34m128\u001b[0m â”‚ add_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]               â”‚\n",
       "â”‚ (\u001b[38;5;33mLayerNormalization\u001b[0m)          â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_67 (\u001b[38;5;33mDense\u001b[0m)              â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)           â”‚           \u001b[38;5;34m8,320\u001b[0m â”‚ layer_normalization_12[\u001b[38;5;34m0\u001b[0m]â€¦ â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_68 (\u001b[38;5;33mDense\u001b[0m)              â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m1\u001b[0m)             â”‚             \u001b[38;5;34m129\u001b[0m â”‚ dense_67[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ add_13 (\u001b[38;5;33mAdd\u001b[0m)                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)            â”‚               \u001b[38;5;34m0\u001b[0m â”‚ layer_normalization_12[\u001b[38;5;34m0\u001b[0m]â€¦ â”‚\n",
       "â”‚                               â”‚                           â”‚                 â”‚ dense_68[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization_13        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)            â”‚             \u001b[38;5;34m128\u001b[0m â”‚ add_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]               â”‚\n",
       "â”‚ (\u001b[38;5;33mLayerNormalization\u001b[0m)          â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_average_pooling1d_6    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                â”‚               \u001b[38;5;34m0\u001b[0m â”‚ layer_normalization_13[\u001b[38;5;34m0\u001b[0m]â€¦ â”‚\n",
       "â”‚ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)      â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_69 (\u001b[38;5;33mDense\u001b[0m)              â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                 â”‚              \u001b[38;5;34m65\u001b[0m â”‚ global_average_pooling1d_â€¦ â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,986</span> (101.51 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m25,986\u001b[0m (101.51 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,986</span> (101.51 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m25,986\u001b[0m (101.51 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 0.1653 - mae: 0.2368 - val_loss: 0.0330 - val_mae: 0.1300\n",
      "Epoch 2/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 0.0105 - mae: 0.0803 - val_loss: 0.0436 - val_mae: 0.1458\n",
      "Epoch 3/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0072 - mae: 0.0657 - val_loss: 0.0316 - val_mae: 0.1269\n",
      "Epoch 4/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0053 - mae: 0.0569 - val_loss: 0.0373 - val_mae: 0.1357\n",
      "Epoch 5/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0040 - mae: 0.0480 - val_loss: 0.0263 - val_mae: 0.1235\n",
      "Epoch 6/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0029 - mae: 0.0406 - val_loss: 0.0230 - val_mae: 0.1261\n",
      "Epoch 7/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0025 - mae: 0.0384 - val_loss: 0.0237 - val_mae: 0.1341\n",
      "Epoch 8/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0018 - mae: 0.0317 - val_loss: 0.0246 - val_mae: 0.1302\n",
      "Epoch 9/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 0.0015 - mae: 0.0289 - val_loss: 0.0271 - val_mae: 0.1298\n",
      "Epoch 10/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 0.0014 - mae: 0.0269 - val_loss: 0.0359 - val_mae: 0.1425\n",
      "Epoch 11/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0012 - mae: 0.0246 - val_loss: 0.0359 - val_mae: 0.1370\n",
      "Epoch 12/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0012 - mae: 0.0245 - val_loss: 0.0364 - val_mae: 0.1348\n",
      "Epoch 13/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0012 - mae: 0.0249 - val_loss: 0.0495 - val_mae: 0.1494\n",
      "Epoch 14/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0011 - mae: 0.0233 - val_loss: 0.0468 - val_mae: 0.1436\n",
      "Epoch 15/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 0.0011 - mae: 0.0234 - val_loss: 0.0461 - val_mae: 0.1440\n",
      "Epoch 16/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0010 - mae: 0.0222 - val_loss: 0.0459 - val_mae: 0.1459\n",
      "Epoch 17/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 9.9254e-04 - mae: 0.0218 - val_loss: 0.0437 - val_mae: 0.1425\n",
      "Epoch 18/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 0.0011 - mae: 0.0244 - val_loss: 0.0393 - val_mae: 0.1388\n",
      "Epoch 19/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0011 - mae: 0.0232 - val_loss: 0.0393 - val_mae: 0.1392\n",
      "Epoch 20/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0011 - mae: 0.0231 - val_loss: 0.0508 - val_mae: 0.1569\n",
      "Epoch 21/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0011 - mae: 0.0238 - val_loss: 0.0401 - val_mae: 0.1432\n",
      "Epoch 22/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0010 - mae: 0.0219 - val_loss: 0.0352 - val_mae: 0.1369\n",
      "Epoch 23/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 9.7796e-04 - mae: 0.0221 - val_loss: 0.0348 - val_mae: 0.1382\n",
      "Epoch 24/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0010 - mae: 0.0216 - val_loss: 0.0337 - val_mae: 0.1367\n",
      "Epoch 25/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0012 - mae: 0.0232 - val_loss: 0.0349 - val_mae: 0.1396\n",
      "Epoch 26/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 9.4609e-04 - mae: 0.0209 - val_loss: 0.0311 - val_mae: 0.1343\n",
      "Epoch 27/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 0.0011 - mae: 0.0233 - val_loss: 0.0326 - val_mae: 0.1378\n",
      "Epoch 28/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 0.0010 - mae: 0.0217 - val_loss: 0.0336 - val_mae: 0.1406\n",
      "Epoch 29/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 8.7841e-04 - mae: 0.0204 - val_loss: 0.0293 - val_mae: 0.1342\n",
      "Epoch 30/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0010 - mae: 0.0216 - val_loss: 0.0305 - val_mae: 0.1361\n",
      "Epoch 31/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 9.9702e-04 - mae: 0.0220 - val_loss: 0.0313 - val_mae: 0.1384\n",
      "Epoch 32/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 9.0426e-04 - mae: 0.0208 - val_loss: 0.0328 - val_mae: 0.1395\n",
      "Epoch 33/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - loss: 9.4156e-04 - mae: 0.0216 - val_loss: 0.0342 - val_mae: 0.1433\n",
      "Epoch 34/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - loss: 9.3743e-04 - mae: 0.0218 - val_loss: 0.0298 - val_mae: 0.1357\n",
      "Epoch 35/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 9.1420e-04 - mae: 0.0204 - val_loss: 0.0313 - val_mae: 0.1392\n",
      "Epoch 36/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 9.4357e-04 - mae: 0.0208 - val_loss: 0.0319 - val_mae: 0.1404\n",
      "Epoch 37/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 8.9567e-04 - mae: 0.0207 - val_loss: 0.0299 - val_mae: 0.1360\n",
      "Epoch 38/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 9.3537e-04 - mae: 0.0208 - val_loss: 0.0301 - val_mae: 0.1375\n",
      "Epoch 39/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 9.0736e-04 - mae: 0.0209 - val_loss: 0.0321 - val_mae: 0.1395\n",
      "Epoch 40/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 9.5139e-04 - mae: 0.0211 - val_loss: 0.0313 - val_mae: 0.1397\n",
      "Epoch 41/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 9.2225e-04 - mae: 0.0216 - val_loss: 0.0296 - val_mae: 0.1371\n",
      "Epoch 42/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 8.8069e-04 - mae: 0.0198 - val_loss: 0.0338 - val_mae: 0.1440\n",
      "Epoch 43/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 8.5159e-04 - mae: 0.0203 - val_loss: 0.0345 - val_mae: 0.1451\n",
      "Epoch 44/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 9.0543e-04 - mae: 0.0215 - val_loss: 0.0297 - val_mae: 0.1370\n",
      "Epoch 45/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 8.7464e-04 - mae: 0.0203 - val_loss: 0.0344 - val_mae: 0.1440\n",
      "Epoch 46/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 8.2914e-04 - mae: 0.0195 - val_loss: 0.0272 - val_mae: 0.1337\n",
      "Epoch 47/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 8.6365e-04 - mae: 0.0203 - val_loss: 0.0275 - val_mae: 0.1345\n",
      "Epoch 48/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 8.6137e-04 - mae: 0.0203 - val_loss: 0.0312 - val_mae: 0.1407\n",
      "Epoch 49/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 8.1423e-04 - mae: 0.0194 - val_loss: 0.0300 - val_mae: 0.1389\n",
      "Epoch 50/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 8.8794e-04 - mae: 0.0204 - val_loss: 0.0313 - val_mae: 0.1409\n",
      "Epoch 51/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 9.0202e-04 - mae: 0.0203 - val_loss: 0.0331 - val_mae: 0.1428\n",
      "Epoch 52/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 8.5088e-04 - mae: 0.0203 - val_loss: 0.0273 - val_mae: 0.1362\n",
      "Epoch 53/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 9.3105e-04 - mae: 0.0216 - val_loss: 0.0293 - val_mae: 0.1385\n",
      "Epoch 54/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 8.6827e-04 - mae: 0.0205 - val_loss: 0.0346 - val_mae: 0.1466\n",
      "Epoch 55/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 8.6494e-04 - mae: 0.0199 - val_loss: 0.0277 - val_mae: 0.1353\n",
      "Epoch 56/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 8.5967e-04 - mae: 0.0207 - val_loss: 0.0280 - val_mae: 0.1366\n",
      "Epoch 57/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 8.4898e-04 - mae: 0.0198 - val_loss: 0.0297 - val_mae: 0.1389\n",
      "Epoch 58/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 8.2394e-04 - mae: 0.0196 - val_loss: 0.0265 - val_mae: 0.1329\n",
      "Epoch 59/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 9.2856e-04 - mae: 0.0206 - val_loss: 0.0320 - val_mae: 0.1427\n",
      "Epoch 60/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - loss: 8.6548e-04 - mae: 0.0203 - val_loss: 0.0295 - val_mae: 0.1397\n",
      "Epoch 61/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 8.4349e-04 - mae: 0.0201 - val_loss: 0.0344 - val_mae: 0.1442\n",
      "Epoch 62/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 8.6154e-04 - mae: 0.0199 - val_loss: 0.0265 - val_mae: 0.1349\n",
      "Epoch 63/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 8.0188e-04 - mae: 0.0197 - val_loss: 0.0266 - val_mae: 0.1348\n",
      "Epoch 64/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 8.9070e-04 - mae: 0.0203 - val_loss: 0.0277 - val_mae: 0.1370\n",
      "Epoch 65/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 8.3014e-04 - mae: 0.0199 - val_loss: 0.0262 - val_mae: 0.1349\n",
      "Epoch 66/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 8.9373e-04 - mae: 0.0208 - val_loss: 0.0300 - val_mae: 0.1414\n",
      "Epoch 67/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 8.7128e-04 - mae: 0.0203 - val_loss: 0.0266 - val_mae: 0.1340\n",
      "Epoch 68/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 8.4605e-04 - mae: 0.0199 - val_loss: 0.0283 - val_mae: 0.1361\n",
      "Epoch 69/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 7.9344e-04 - mae: 0.0190 - val_loss: 0.0262 - val_mae: 0.1339\n",
      "Epoch 70/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 8.4975e-04 - mae: 0.0200 - val_loss: 0.0267 - val_mae: 0.1345\n",
      "Epoch 71/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 8.4663e-04 - mae: 0.0198 - val_loss: 0.0275 - val_mae: 0.1373\n",
      "Epoch 72/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 8.0464e-04 - mae: 0.0191 - val_loss: 0.0275 - val_mae: 0.1374\n",
      "Epoch 73/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 8.2496e-04 - mae: 0.0198 - val_loss: 0.0243 - val_mae: 0.1298\n",
      "Epoch 74/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 8.2402e-04 - mae: 0.0196 - val_loss: 0.0263 - val_mae: 0.1346\n",
      "Epoch 75/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 8.0849e-04 - mae: 0.0193 - val_loss: 0.0276 - val_mae: 0.1380\n",
      "Epoch 76/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 8.0586e-04 - mae: 0.0194 - val_loss: 0.0278 - val_mae: 0.1371\n",
      "Epoch 77/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 9.0046e-04 - mae: 0.0207 - val_loss: 0.0277 - val_mae: 0.1374\n",
      "Epoch 78/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 8.6927e-04 - mae: 0.0200 - val_loss: 0.0270 - val_mae: 0.1354\n",
      "Epoch 79/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 8.5786e-04 - mae: 0.0199 - val_loss: 0.0263 - val_mae: 0.1353\n",
      "Epoch 80/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 8.2507e-04 - mae: 0.0193 - val_loss: 0.0271 - val_mae: 0.1356\n",
      "Epoch 81/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 8.4202e-04 - mae: 0.0200 - val_loss: 0.0302 - val_mae: 0.1422\n",
      "Epoch 82/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 8.3961e-04 - mae: 0.0195 - val_loss: 0.0262 - val_mae: 0.1345\n",
      "Epoch 83/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 9.0867e-04 - mae: 0.0206 - val_loss: 0.0250 - val_mae: 0.1315\n",
      "Epoch 84/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 8.1417e-04 - mae: 0.0195 - val_loss: 0.0270 - val_mae: 0.1368\n",
      "Epoch 85/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 8.2186e-04 - mae: 0.0199 - val_loss: 0.0256 - val_mae: 0.1342\n",
      "Epoch 86/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 8.3979e-04 - mae: 0.0197 - val_loss: 0.0271 - val_mae: 0.1375\n",
      "Epoch 87/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 8.5727e-04 - mae: 0.0202 - val_loss: 0.0268 - val_mae: 0.1362\n",
      "Epoch 88/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 8.3036e-04 - mae: 0.0199 - val_loss: 0.0265 - val_mae: 0.1358\n",
      "Epoch 89/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 8.1999e-04 - mae: 0.0192 - val_loss: 0.0287 - val_mae: 0.1379\n",
      "Epoch 90/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 8.5104e-04 - mae: 0.0193 - val_loss: 0.0260 - val_mae: 0.1361\n",
      "Epoch 91/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - loss: 8.5448e-04 - mae: 0.0195 - val_loss: 0.0266 - val_mae: 0.1356\n",
      "Epoch 92/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 8.2424e-04 - mae: 0.0191 - val_loss: 0.0268 - val_mae: 0.1367\n",
      "Epoch 93/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 7.8741e-04 - mae: 0.0188 - val_loss: 0.0266 - val_mae: 0.1354\n",
      "Epoch 94/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 8.0703e-04 - mae: 0.0193 - val_loss: 0.0268 - val_mae: 0.1361\n",
      "Epoch 95/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 8.2189e-04 - mae: 0.0195 - val_loss: 0.0249 - val_mae: 0.1317\n",
      "Epoch 96/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 8.5714e-04 - mae: 0.0200 - val_loss: 0.0290 - val_mae: 0.1396\n",
      "Epoch 97/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 7.9316e-04 - mae: 0.0188 - val_loss: 0.0261 - val_mae: 0.1348\n",
      "Epoch 98/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 7.8796e-04 - mae: 0.0189 - val_loss: 0.0277 - val_mae: 0.1373\n",
      "Epoch 99/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 8.2903e-04 - mae: 0.0196 - val_loss: 0.0278 - val_mae: 0.1381\n",
      "Epoch 100/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 7.9974e-04 - mae: 0.0189 - val_loss: 0.0247 - val_mae: 0.1313\n",
      "WARNING:tensorflow:5 out of the last 148 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x00000139CB0D6A20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m217/217\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step\n",
      "âœ… Done with capsicum_Hassan_daily.csv | MAE=256.23, RMSE=462.21, R2=0.79, MAPE=10.99%, Accuracy=89.01%\n",
      "Saved updated CSV with Date, Actual & Predicted: tat_mqa_output_csv\\capsicum_Hassan_daily_tat_mqa_updated.csv\n",
      "ğŸš€ Processing: capsicum_Haveri_daily.csv\n",
      "âš ï¸ Skipping capsicum_Haveri_daily.csv â€” dataset length (1) <= look_back (30)\n",
      "ğŸš€ Processing: capsicum_Kalburgi_daily.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_7\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_7\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                  </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape              </span>â”ƒ<span style=\"font-weight: bold\">         Param # </span>â”ƒ<span style=\"font-weight: bold\"> Connected to               </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)             â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                          â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ multi_query_attention_7       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">17,216</span> â”‚ input_layer_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiQueryAttention</span>)         â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ add_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ multi_query_attention_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>â€¦ â”‚\n",
       "â”‚                               â”‚                           â”‚                 â”‚ input_layer_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization_14        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> â”‚ add_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]               â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)          â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_77 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> â”‚ layer_normalization_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]â€¦ â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_78 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)             â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> â”‚ dense_77[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ add_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ layer_normalization_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]â€¦ â”‚\n",
       "â”‚                               â”‚                           â”‚                 â”‚ dense_78[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization_15        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> â”‚ add_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]               â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)          â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_average_pooling1d_7    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ layer_normalization_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]â€¦ â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)      â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_79 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                 â”‚              <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> â”‚ global_average_pooling1d_â€¦ â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer_7 (\u001b[38;5;33mInputLayer\u001b[0m)    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m1\u001b[0m)             â”‚               \u001b[38;5;34m0\u001b[0m â”‚ -                          â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ multi_query_attention_7       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)            â”‚          \u001b[38;5;34m17,216\u001b[0m â”‚ input_layer_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        â”‚\n",
       "â”‚ (\u001b[38;5;33mMultiQueryAttention\u001b[0m)         â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ add_14 (\u001b[38;5;33mAdd\u001b[0m)                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)            â”‚               \u001b[38;5;34m0\u001b[0m â”‚ multi_query_attention_7[\u001b[38;5;34m0\u001b[0mâ€¦ â”‚\n",
       "â”‚                               â”‚                           â”‚                 â”‚ input_layer_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization_14        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)            â”‚             \u001b[38;5;34m128\u001b[0m â”‚ add_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]               â”‚\n",
       "â”‚ (\u001b[38;5;33mLayerNormalization\u001b[0m)          â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_77 (\u001b[38;5;33mDense\u001b[0m)              â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)           â”‚           \u001b[38;5;34m8,320\u001b[0m â”‚ layer_normalization_14[\u001b[38;5;34m0\u001b[0m]â€¦ â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_78 (\u001b[38;5;33mDense\u001b[0m)              â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m1\u001b[0m)             â”‚             \u001b[38;5;34m129\u001b[0m â”‚ dense_77[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ add_15 (\u001b[38;5;33mAdd\u001b[0m)                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)            â”‚               \u001b[38;5;34m0\u001b[0m â”‚ layer_normalization_14[\u001b[38;5;34m0\u001b[0m]â€¦ â”‚\n",
       "â”‚                               â”‚                           â”‚                 â”‚ dense_78[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization_15        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)            â”‚             \u001b[38;5;34m128\u001b[0m â”‚ add_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]               â”‚\n",
       "â”‚ (\u001b[38;5;33mLayerNormalization\u001b[0m)          â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_average_pooling1d_7    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                â”‚               \u001b[38;5;34m0\u001b[0m â”‚ layer_normalization_15[\u001b[38;5;34m0\u001b[0m]â€¦ â”‚\n",
       "â”‚ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)      â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_79 (\u001b[38;5;33mDense\u001b[0m)              â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                 â”‚              \u001b[38;5;34m65\u001b[0m â”‚ global_average_pooling1d_â€¦ â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,986</span> (101.51 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m25,986\u001b[0m (101.51 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,986</span> (101.51 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m25,986\u001b[0m (101.51 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 103ms/step - loss: 0.4557 - mae: 0.5085 - val_loss: 0.0222 - val_mae: 0.1005\n",
      "Epoch 2/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0282 - mae: 0.1316 - val_loss: 0.0252 - val_mae: 0.1140\n",
      "Epoch 3/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0236 - mae: 0.1227 - val_loss: 0.0198 - val_mae: 0.0905\n",
      "Epoch 4/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0205 - mae: 0.1119 - val_loss: 0.0134 - val_mae: 0.0696\n",
      "Epoch 5/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0215 - mae: 0.1133 - val_loss: 0.0180 - val_mae: 0.0825\n",
      "Epoch 6/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0189 - mae: 0.1091 - val_loss: 0.0248 - val_mae: 0.1129\n",
      "Epoch 7/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0172 - mae: 0.1020 - val_loss: 0.0271 - val_mae: 0.1222\n",
      "Epoch 8/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0185 - mae: 0.1059 - val_loss: 0.0158 - val_mae: 0.0739\n",
      "Epoch 9/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0168 - mae: 0.1016 - val_loss: 0.0154 - val_mae: 0.0726\n",
      "Epoch 10/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0200 - mae: 0.1063 - val_loss: 0.0125 - val_mae: 0.0718\n",
      "Epoch 11/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0165 - mae: 0.1001 - val_loss: 0.0164 - val_mae: 0.0765\n",
      "Epoch 12/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0156 - mae: 0.0961 - val_loss: 0.0137 - val_mae: 0.0684\n",
      "Epoch 13/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0144 - mae: 0.0913 - val_loss: 0.0155 - val_mae: 0.0728\n",
      "Epoch 14/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0155 - mae: 0.0944 - val_loss: 0.0128 - val_mae: 0.0680\n",
      "Epoch 15/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0145 - mae: 0.0944 - val_loss: 0.0121 - val_mae: 0.0712\n",
      "Epoch 16/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0144 - mae: 0.0909 - val_loss: 0.0117 - val_mae: 0.0758\n",
      "Epoch 17/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0139 - mae: 0.0910 - val_loss: 0.0116 - val_mae: 0.0832\n",
      "Epoch 18/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0138 - mae: 0.0888 - val_loss: 0.0120 - val_mae: 0.0689\n",
      "Epoch 19/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0126 - mae: 0.0834 - val_loss: 0.0133 - val_mae: 0.0667\n",
      "Epoch 20/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0140 - mae: 0.0879 - val_loss: 0.0159 - val_mae: 0.0777\n",
      "Epoch 21/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0149 - mae: 0.0935 - val_loss: 0.0135 - val_mae: 0.0678\n",
      "Epoch 22/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0126 - mae: 0.0839 - val_loss: 0.0119 - val_mae: 0.0660\n",
      "Epoch 23/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0125 - mae: 0.0829 - val_loss: 0.0140 - val_mae: 0.0708\n",
      "Epoch 24/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0116 - mae: 0.0821 - val_loss: 0.0111 - val_mae: 0.0718\n",
      "Epoch 25/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0126 - mae: 0.0807 - val_loss: 0.0131 - val_mae: 0.0678\n",
      "Epoch 26/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0115 - mae: 0.0792 - val_loss: 0.0117 - val_mae: 0.0901\n",
      "Epoch 27/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0124 - mae: 0.0832 - val_loss: 0.0114 - val_mae: 0.0667\n",
      "Epoch 28/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0110 - mae: 0.0765 - val_loss: 0.0111 - val_mae: 0.0690\n",
      "Epoch 29/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0126 - mae: 0.0778 - val_loss: 0.0150 - val_mae: 0.0775\n",
      "Epoch 30/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0107 - mae: 0.0758 - val_loss: 0.0113 - val_mae: 0.0672\n",
      "Epoch 31/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0106 - mae: 0.0764 - val_loss: 0.0116 - val_mae: 0.0888\n",
      "Epoch 32/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0111 - mae: 0.0785 - val_loss: 0.0109 - val_mae: 0.0778\n",
      "Epoch 33/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0100 - mae: 0.0731 - val_loss: 0.0134 - val_mae: 0.1015\n",
      "Epoch 34/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0101 - mae: 0.0739 - val_loss: 0.0110 - val_mae: 0.0794\n",
      "Epoch 35/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0095 - mae: 0.0713 - val_loss: 0.0126 - val_mae: 0.0957\n",
      "Epoch 36/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0091 - mae: 0.0704 - val_loss: 0.0121 - val_mae: 0.0915\n",
      "Epoch 37/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0098 - mae: 0.0706 - val_loss: 0.0111 - val_mae: 0.0803\n",
      "Epoch 38/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0102 - mae: 0.0740 - val_loss: 0.0143 - val_mae: 0.1056\n",
      "Epoch 39/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0095 - mae: 0.0707 - val_loss: 0.0116 - val_mae: 0.0869\n",
      "Epoch 40/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0090 - mae: 0.0692 - val_loss: 0.0121 - val_mae: 0.0905\n",
      "Epoch 41/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0089 - mae: 0.0678 - val_loss: 0.0120 - val_mae: 0.0897\n",
      "Epoch 42/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0102 - mae: 0.0701 - val_loss: 0.0110 - val_mae: 0.0771\n",
      "Epoch 43/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0114 - mae: 0.0751 - val_loss: 0.0111 - val_mae: 0.0717\n",
      "Epoch 44/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0121 - mae: 0.0816 - val_loss: 0.0129 - val_mae: 0.0968\n",
      "Epoch 45/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0100 - mae: 0.0740 - val_loss: 0.0111 - val_mae: 0.0791\n",
      "Epoch 46/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0095 - mae: 0.0680 - val_loss: 0.0113 - val_mae: 0.0685\n",
      "Epoch 47/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0089 - mae: 0.0683 - val_loss: 0.0119 - val_mae: 0.0883\n",
      "Epoch 48/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0100 - mae: 0.0725 - val_loss: 0.0115 - val_mae: 0.0682\n",
      "Epoch 49/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0111 - mae: 0.0773 - val_loss: 0.0112 - val_mae: 0.0720\n",
      "Epoch 50/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0109 - mae: 0.0756 - val_loss: 0.0112 - val_mae: 0.0801\n",
      "Epoch 51/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0090 - mae: 0.0692 - val_loss: 0.0124 - val_mae: 0.0917\n",
      "Epoch 52/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0106 - mae: 0.0725 - val_loss: 0.0124 - val_mae: 0.0922\n",
      "Epoch 53/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0087 - mae: 0.0653 - val_loss: 0.0126 - val_mae: 0.0927\n",
      "Epoch 54/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0091 - mae: 0.0675 - val_loss: 0.0116 - val_mae: 0.0692\n",
      "Epoch 55/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0100 - mae: 0.0710 - val_loss: 0.0148 - val_mae: 0.1076\n",
      "Epoch 56/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0112 - mae: 0.0754 - val_loss: 0.0133 - val_mae: 0.0986\n",
      "Epoch 57/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0085 - mae: 0.0658 - val_loss: 0.0112 - val_mae: 0.0726\n",
      "Epoch 58/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0090 - mae: 0.0673 - val_loss: 0.0112 - val_mae: 0.0760\n",
      "Epoch 59/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0104 - mae: 0.0732 - val_loss: 0.0118 - val_mae: 0.0861\n",
      "Epoch 60/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0093 - mae: 0.0666 - val_loss: 0.0126 - val_mae: 0.0935\n",
      "Epoch 61/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0093 - mae: 0.0697 - val_loss: 0.0158 - val_mae: 0.1119\n",
      "Epoch 62/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0099 - mae: 0.0717 - val_loss: 0.0185 - val_mae: 0.1227\n",
      "Epoch 63/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0113 - mae: 0.0739 - val_loss: 0.0133 - val_mae: 0.0973\n",
      "Epoch 64/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0097 - mae: 0.0682 - val_loss: 0.0122 - val_mae: 0.0892\n",
      "Epoch 65/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0094 - mae: 0.0683 - val_loss: 0.0135 - val_mae: 0.0993\n",
      "Epoch 66/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0101 - mae: 0.0689 - val_loss: 0.0112 - val_mae: 0.0744\n",
      "Epoch 67/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0097 - mae: 0.0682 - val_loss: 0.0118 - val_mae: 0.0854\n",
      "Epoch 68/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0095 - mae: 0.0670 - val_loss: 0.0162 - val_mae: 0.1126\n",
      "Epoch 69/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0103 - mae: 0.0716 - val_loss: 0.0126 - val_mae: 0.0937\n",
      "Epoch 70/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0107 - mae: 0.0710 - val_loss: 0.0113 - val_mae: 0.0722\n",
      "Epoch 71/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0099 - mae: 0.0700 - val_loss: 0.0114 - val_mae: 0.0723\n",
      "Epoch 72/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0095 - mae: 0.0682 - val_loss: 0.0126 - val_mae: 0.0917\n",
      "Epoch 73/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0091 - mae: 0.0690 - val_loss: 0.0182 - val_mae: 0.1221\n",
      "Epoch 74/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0083 - mae: 0.0663 - val_loss: 0.0114 - val_mae: 0.0802\n",
      "Epoch 75/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0092 - mae: 0.0680 - val_loss: 0.0157 - val_mae: 0.1111\n",
      "Epoch 76/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0104 - mae: 0.0717 - val_loss: 0.0160 - val_mae: 0.1127\n",
      "Epoch 77/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0101 - mae: 0.0750 - val_loss: 0.0209 - val_mae: 0.1317\n",
      "Epoch 78/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0103 - mae: 0.0727 - val_loss: 0.0117 - val_mae: 0.0852\n",
      "Epoch 79/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0100 - mae: 0.0738 - val_loss: 0.0124 - val_mae: 0.0697\n",
      "Epoch 80/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0100 - mae: 0.0724 - val_loss: 0.0120 - val_mae: 0.0871\n",
      "Epoch 81/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0099 - mae: 0.0689 - val_loss: 0.0113 - val_mae: 0.0724\n",
      "Epoch 82/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0093 - mae: 0.0700 - val_loss: 0.0117 - val_mae: 0.0838\n",
      "Epoch 83/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0086 - mae: 0.0659 - val_loss: 0.0112 - val_mae: 0.0743\n",
      "Epoch 84/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0102 - mae: 0.0704 - val_loss: 0.0171 - val_mae: 0.1170\n",
      "Epoch 85/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0106 - mae: 0.0742 - val_loss: 0.0121 - val_mae: 0.0888\n",
      "Epoch 86/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0098 - mae: 0.0685 - val_loss: 0.0113 - val_mae: 0.0754\n",
      "Epoch 87/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0098 - mae: 0.0695 - val_loss: 0.0151 - val_mae: 0.1079\n",
      "Epoch 88/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0085 - mae: 0.0679 - val_loss: 0.0171 - val_mae: 0.1170\n",
      "Epoch 89/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0087 - mae: 0.0664 - val_loss: 0.0115 - val_mae: 0.0808\n",
      "Epoch 90/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0102 - mae: 0.0721 - val_loss: 0.0115 - val_mae: 0.0700\n",
      "Epoch 91/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0108 - mae: 0.0744 - val_loss: 0.0151 - val_mae: 0.0812\n",
      "Epoch 92/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0112 - mae: 0.0760 - val_loss: 0.0118 - val_mae: 0.0694\n",
      "Epoch 93/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0102 - mae: 0.0741 - val_loss: 0.0113 - val_mae: 0.0761\n",
      "Epoch 94/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0090 - mae: 0.0671 - val_loss: 0.0114 - val_mae: 0.0774\n",
      "Epoch 95/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0093 - mae: 0.0688 - val_loss: 0.0161 - val_mae: 0.1132\n",
      "Epoch 96/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0116 - mae: 0.0804 - val_loss: 0.0114 - val_mae: 0.0803\n",
      "Epoch 97/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0095 - mae: 0.0702 - val_loss: 0.0114 - val_mae: 0.0742\n",
      "Epoch 98/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0098 - mae: 0.0717 - val_loss: 0.0157 - val_mae: 0.1108\n",
      "Epoch 99/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0095 - mae: 0.0707 - val_loss: 0.0143 - val_mae: 0.1039\n",
      "Epoch 100/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0103 - mae: 0.0719 - val_loss: 0.0125 - val_mae: 0.0927\n",
      "\u001b[1m36/36\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n",
      "âœ… Done with capsicum_Kalburgi_daily.csv | MAE=570.47, RMSE=728.0, R2=0.641, MAPE=31.17%, Accuracy=68.83%\n",
      "Saved updated CSV with Date, Actual & Predicted: tat_mqa_output_csv\\capsicum_Kalburgi_daily_tat_mqa_updated.csv\n",
      "ğŸš€ Processing: capsicum_Kolar_daily.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_8\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_8\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                  </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape              </span>â”ƒ<span style=\"font-weight: bold\">         Param # </span>â”ƒ<span style=\"font-weight: bold\"> Connected to               </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)             â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                          â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ multi_query_attention_8       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">17,216</span> â”‚ input_layer_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiQueryAttention</span>)         â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ add_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ multi_query_attention_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>â€¦ â”‚\n",
       "â”‚                               â”‚                           â”‚                 â”‚ input_layer_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization_16        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> â”‚ add_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]               â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)          â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_87 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> â”‚ layer_normalization_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]â€¦ â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_88 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)             â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> â”‚ dense_87[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ add_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ layer_normalization_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]â€¦ â”‚\n",
       "â”‚                               â”‚                           â”‚                 â”‚ dense_88[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization_17        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> â”‚ add_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]               â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)          â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_average_pooling1d_8    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ layer_normalization_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]â€¦ â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)      â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_89 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                 â”‚              <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> â”‚ global_average_pooling1d_â€¦ â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer_8 (\u001b[38;5;33mInputLayer\u001b[0m)    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m1\u001b[0m)             â”‚               \u001b[38;5;34m0\u001b[0m â”‚ -                          â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ multi_query_attention_8       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)            â”‚          \u001b[38;5;34m17,216\u001b[0m â”‚ input_layer_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        â”‚\n",
       "â”‚ (\u001b[38;5;33mMultiQueryAttention\u001b[0m)         â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ add_16 (\u001b[38;5;33mAdd\u001b[0m)                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)            â”‚               \u001b[38;5;34m0\u001b[0m â”‚ multi_query_attention_8[\u001b[38;5;34m0\u001b[0mâ€¦ â”‚\n",
       "â”‚                               â”‚                           â”‚                 â”‚ input_layer_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization_16        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)            â”‚             \u001b[38;5;34m128\u001b[0m â”‚ add_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]               â”‚\n",
       "â”‚ (\u001b[38;5;33mLayerNormalization\u001b[0m)          â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_87 (\u001b[38;5;33mDense\u001b[0m)              â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)           â”‚           \u001b[38;5;34m8,320\u001b[0m â”‚ layer_normalization_16[\u001b[38;5;34m0\u001b[0m]â€¦ â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_88 (\u001b[38;5;33mDense\u001b[0m)              â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m1\u001b[0m)             â”‚             \u001b[38;5;34m129\u001b[0m â”‚ dense_87[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ add_17 (\u001b[38;5;33mAdd\u001b[0m)                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)            â”‚               \u001b[38;5;34m0\u001b[0m â”‚ layer_normalization_16[\u001b[38;5;34m0\u001b[0m]â€¦ â”‚\n",
       "â”‚                               â”‚                           â”‚                 â”‚ dense_88[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization_17        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)            â”‚             \u001b[38;5;34m128\u001b[0m â”‚ add_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]               â”‚\n",
       "â”‚ (\u001b[38;5;33mLayerNormalization\u001b[0m)          â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_average_pooling1d_8    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                â”‚               \u001b[38;5;34m0\u001b[0m â”‚ layer_normalization_17[\u001b[38;5;34m0\u001b[0m]â€¦ â”‚\n",
       "â”‚ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)      â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_89 (\u001b[38;5;33mDense\u001b[0m)              â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                 â”‚              \u001b[38;5;34m65\u001b[0m â”‚ global_average_pooling1d_â€¦ â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,986</span> (101.51 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m25,986\u001b[0m (101.51 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,986</span> (101.51 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m25,986\u001b[0m (101.51 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 9ms/step - loss: 0.0594 - mae: 0.1336 - val_loss: 0.0032 - val_mae: 0.0473\n",
      "Epoch 2/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 12ms/step - loss: 0.0084 - mae: 0.0671 - val_loss: 0.0149 - val_mae: 0.1150\n",
      "Epoch 3/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - loss: 0.0086 - mae: 0.0677 - val_loss: 0.0030 - val_mae: 0.0449\n",
      "Epoch 4/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 12ms/step - loss: 0.0078 - mae: 0.0644 - val_loss: 0.0019 - val_mae: 0.0316\n",
      "Epoch 5/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 11ms/step - loss: 0.0077 - mae: 0.0628 - val_loss: 0.0018 - val_mae: 0.0312\n",
      "Epoch 6/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 12ms/step - loss: 0.0080 - mae: 0.0640 - val_loss: 0.0024 - val_mae: 0.0396\n",
      "Epoch 7/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 13ms/step - loss: 0.0076 - mae: 0.0628 - val_loss: 0.0020 - val_mae: 0.0352\n",
      "Epoch 8/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 13ms/step - loss: 0.0075 - mae: 0.0621 - val_loss: 0.0023 - val_mae: 0.0381\n",
      "Epoch 9/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 13ms/step - loss: 0.0075 - mae: 0.0624 - val_loss: 0.0018 - val_mae: 0.0311\n",
      "Epoch 10/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 13ms/step - loss: 0.0074 - mae: 0.0611 - val_loss: 0.0017 - val_mae: 0.0308\n",
      "Epoch 11/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 13ms/step - loss: 0.0077 - mae: 0.0619 - val_loss: 0.0018 - val_mae: 0.0310\n",
      "Epoch 12/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 13ms/step - loss: 0.0074 - mae: 0.0610 - val_loss: 0.0018 - val_mae: 0.0314\n",
      "Epoch 13/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 0.0074 - mae: 0.0613 - val_loss: 0.0020 - val_mae: 0.0344\n",
      "Epoch 14/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0074 - mae: 0.0604 - val_loss: 0.0025 - val_mae: 0.0405\n",
      "Epoch 15/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0072 - mae: 0.0599 - val_loss: 0.0024 - val_mae: 0.0387\n",
      "Epoch 16/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - loss: 0.0075 - mae: 0.0611 - val_loss: 0.0017 - val_mae: 0.0312\n",
      "Epoch 17/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - loss: 0.0072 - mae: 0.0598 - val_loss: 0.0017 - val_mae: 0.0310\n",
      "Epoch 18/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 0.0074 - mae: 0.0601 - val_loss: 0.0017 - val_mae: 0.0308\n",
      "Epoch 26/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 11ms/step - loss: 0.0072 - mae: 0.0597 - val_loss: 0.0017 - val_mae: 0.0311\n",
      "Epoch 27/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 11ms/step - loss: 0.0071 - mae: 0.0594 - val_loss: 0.0018 - val_mae: 0.0325\n",
      "Epoch 28/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - loss: 0.0074 - mae: 0.0602 - val_loss: 0.0017 - val_mae: 0.0312\n",
      "Epoch 29/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 11ms/step - loss: 0.0070 - mae: 0.0585 - val_loss: 0.0018 - val_mae: 0.0318\n",
      "Epoch 30/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11ms/step - loss: 0.0074 - mae: 0.0605 - val_loss: 0.0018 - val_mae: 0.0326\n",
      "Epoch 31/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 11ms/step - loss: 0.0072 - mae: 0.0596 - val_loss: 0.0017 - val_mae: 0.0308\n",
      "Epoch 32/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 11ms/step - loss: 0.0071 - mae: 0.0586 - val_loss: 0.0017 - val_mae: 0.0311\n",
      "Epoch 33/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 11ms/step - loss: 0.0073 - mae: 0.0597 - val_loss: 0.0017 - val_mae: 0.0311\n",
      "Epoch 34/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 11ms/step - loss: 0.0074 - mae: 0.0600 - val_loss: 0.0017 - val_mae: 0.0309\n",
      "Epoch 35/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 11ms/step - loss: 0.0074 - mae: 0.0600 - val_loss: 0.0017 - val_mae: 0.0315\n",
      "Epoch 36/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - loss: 0.0073 - mae: 0.0598 - val_loss: 0.0017 - val_mae: 0.0312\n",
      "Epoch 37/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 11ms/step - loss: 0.0072 - mae: 0.0594 - val_loss: 0.0017 - val_mae: 0.0308\n",
      "Epoch 38/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11ms/step - loss: 0.0070 - mae: 0.0586 - val_loss: 0.0021 - val_mae: 0.0354\n",
      "Epoch 39/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 11ms/step - loss: 0.0073 - mae: 0.0594 - val_loss: 0.0018 - val_mae: 0.0315\n",
      "Epoch 40/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - loss: 0.0072 - mae: 0.0589 - val_loss: 0.0018 - val_mae: 0.0324\n",
      "Epoch 41/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - loss: 0.0072 - mae: 0.0596 - val_loss: 0.0017 - val_mae: 0.0311\n",
      "Epoch 42/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - loss: 0.0071 - mae: 0.0589 - val_loss: 0.0018 - val_mae: 0.0316\n",
      "Epoch 43/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 11ms/step - loss: 0.0071 - mae: 0.0585 - val_loss: 0.0018 - val_mae: 0.0325\n",
      "Epoch 44/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 0.0072 - mae: 0.0592 - val_loss: 0.0017 - val_mae: 0.0308\n",
      "Epoch 45/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - loss: 0.0072 - mae: 0.0596 - val_loss: 0.0017 - val_mae: 0.0310\n",
      "Epoch 46/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - loss: 0.0074 - mae: 0.0603 - val_loss: 0.0017 - val_mae: 0.0312\n",
      "Epoch 47/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 0.0072 - mae: 0.0591 - val_loss: 0.0017 - val_mae: 0.0310\n",
      "Epoch 48/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - loss: 0.0074 - mae: 0.0603 - val_loss: 0.0017 - val_mae: 0.0308\n",
      "Epoch 49/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - loss: 0.0075 - mae: 0.0603 - val_loss: 0.0017 - val_mae: 0.0308\n",
      "Epoch 50/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - loss: 0.0073 - mae: 0.0595 - val_loss: 0.0018 - val_mae: 0.0319\n",
      "Epoch 51/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - loss: 0.0073 - mae: 0.0597 - val_loss: 0.0017 - val_mae: 0.0312\n",
      "Epoch 52/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 0.0072 - mae: 0.0592 - val_loss: 0.0021 - val_mae: 0.0354\n",
      "Epoch 53/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11ms/step - loss: 0.0071 - mae: 0.0589 - val_loss: 0.0018 - val_mae: 0.0318\n",
      "Epoch 54/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 0.0073 - mae: 0.0597 - val_loss: 0.0017 - val_mae: 0.0308\n",
      "Epoch 55/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - loss: 0.0071 - mae: 0.0588 - val_loss: 0.0018 - val_mae: 0.0315\n",
      "Epoch 56/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11ms/step - loss: 0.0074 - mae: 0.0596 - val_loss: 0.0017 - val_mae: 0.0317\n",
      "Epoch 57/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 0.0073 - mae: 0.0601 - val_loss: 0.0017 - val_mae: 0.0310\n",
      "Epoch 58/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 0.0072 - mae: 0.0589 - val_loss: 0.0017 - val_mae: 0.0310\n",
      "Epoch 59/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - loss: 0.0070 - mae: 0.0580 - val_loss: 0.0019 - val_mae: 0.0330\n",
      "Epoch 60/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 11ms/step - loss: 0.0072 - mae: 0.0597 - val_loss: 0.0018 - val_mae: 0.0320\n",
      "Epoch 61/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 11ms/step - loss: 0.0072 - mae: 0.0594 - val_loss: 0.0017 - val_mae: 0.0313\n",
      "Epoch 62/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - loss: 0.0072 - mae: 0.0594 - val_loss: 0.0018 - val_mae: 0.0319\n",
      "Epoch 63/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 11ms/step - loss: 0.0074 - mae: 0.0603 - val_loss: 0.0017 - val_mae: 0.0312\n",
      "Epoch 64/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - loss: 0.0073 - mae: 0.0603 - val_loss: 0.0018 - val_mae: 0.0322\n",
      "Epoch 65/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 11ms/step - loss: 0.0071 - mae: 0.0588 - val_loss: 0.0017 - val_mae: 0.0313\n",
      "Epoch 66/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - loss: 0.0072 - mae: 0.0593 - val_loss: 0.0017 - val_mae: 0.0315\n",
      "Epoch 67/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - loss: 0.0071 - mae: 0.0583 - val_loss: 0.0018 - val_mae: 0.0320\n",
      "Epoch 68/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 11ms/step - loss: 0.0071 - mae: 0.0587 - val_loss: 0.0018 - val_mae: 0.0317\n",
      "Epoch 69/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 12ms/step - loss: 0.0071 - mae: 0.0588 - val_loss: 0.0018 - val_mae: 0.0320\n",
      "Epoch 70/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 12ms/step - loss: 0.0072 - mae: 0.0593 - val_loss: 0.0017 - val_mae: 0.0313\n",
      "Epoch 71/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 12ms/step - loss: 0.0073 - mae: 0.0599 - val_loss: 0.0018 - val_mae: 0.0318\n",
      "Epoch 72/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 12ms/step - loss: 0.0072 - mae: 0.0591 - val_loss: 0.0017 - val_mae: 0.0313\n",
      "Epoch 73/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 12ms/step - loss: 0.0070 - mae: 0.0582 - val_loss: 0.0018 - val_mae: 0.0321\n",
      "Epoch 74/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 12ms/step - loss: 0.0071 - mae: 0.0594 - val_loss: 0.0019 - val_mae: 0.0334\n",
      "Epoch 75/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 11ms/step - loss: 0.0070 - mae: 0.0587 - val_loss: 0.0017 - val_mae: 0.0315\n",
      "Epoch 76/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - loss: 0.0070 - mae: 0.0585 - val_loss: 0.0017 - val_mae: 0.0314\n",
      "Epoch 77/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 12ms/step - loss: 0.0072 - mae: 0.0596 - val_loss: 0.0018 - val_mae: 0.0319\n",
      "Epoch 78/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 12ms/step - loss: 0.0072 - mae: 0.0595 - val_loss: 0.0017 - val_mae: 0.0315\n",
      "Epoch 79/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 12ms/step - loss: 0.0072 - mae: 0.0600 - val_loss: 0.0017 - val_mae: 0.0312\n",
      "Epoch 80/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 12ms/step - loss: 0.0071 - mae: 0.0586 - val_loss: 0.0017 - val_mae: 0.0314\n",
      "Epoch 81/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 12ms/step - loss: 0.0071 - mae: 0.0585 - val_loss: 0.0018 - val_mae: 0.0318\n",
      "Epoch 82/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 0.0071 - mae: 0.0588 - val_loss: 0.0017 - val_mae: 0.0312\n",
      "Epoch 83/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - loss: 0.0072 - mae: 0.0596 - val_loss: 0.0017 - val_mae: 0.0313\n",
      "Epoch 84/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 0.0073 - mae: 0.0598 - val_loss: 0.0017 - val_mae: 0.0312\n",
      "Epoch 85/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 0.0072 - mae: 0.0588 - val_loss: 0.0019 - val_mae: 0.0334\n",
      "Epoch 86/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 0.0072 - mae: 0.0595 - val_loss: 0.0017 - val_mae: 0.0315\n",
      "Epoch 87/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 0.0071 - mae: 0.0584 - val_loss: 0.0017 - val_mae: 0.0314\n",
      "Epoch 88/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 0.0071 - mae: 0.0587 - val_loss: 0.0018 - val_mae: 0.0319\n",
      "Epoch 89/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - loss: 0.0071 - mae: 0.0591 - val_loss: 0.0018 - val_mae: 0.0318\n",
      "Epoch 90/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - loss: 0.0071 - mae: 0.0588 - val_loss: 0.0017 - val_mae: 0.0315\n",
      "Epoch 91/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - loss: 0.0069 - mae: 0.0581 - val_loss: 0.0017 - val_mae: 0.0310\n",
      "Epoch 92/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - loss: 0.0072 - mae: 0.0598 - val_loss: 0.0018 - val_mae: 0.0328\n",
      "Epoch 93/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - loss: 0.0071 - mae: 0.0590 - val_loss: 0.0017 - val_mae: 0.0313\n",
      "Epoch 94/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - loss: 0.0074 - mae: 0.0601 - val_loss: 0.0017 - val_mae: 0.0312\n",
      "Epoch 95/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - loss: 0.0071 - mae: 0.0591 - val_loss: 0.0018 - val_mae: 0.0324\n",
      "Epoch 96/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - loss: 0.0072 - mae: 0.0592 - val_loss: 0.0017 - val_mae: 0.0315\n",
      "Epoch 97/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - loss: 0.0073 - mae: 0.0597 - val_loss: 0.0017 - val_mae: 0.0316\n",
      "Epoch 98/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - loss: 0.0071 - mae: 0.0589 - val_loss: 0.0018 - val_mae: 0.0320\n",
      "Epoch 99/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - loss: 0.0070 - mae: 0.0588 - val_loss: 0.0017 - val_mae: 0.0313\n",
      "Epoch 100/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - loss: 0.0072 - mae: 0.0590 - val_loss: 0.0018 - val_mae: 0.0328\n",
      "\u001b[1m672/672\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step\n",
      "âœ… Done with capsicum_Kolar_daily.csv | MAE=1000.65, RMSE=1408.6, R2=0.2485, MAPE=51.59%, Accuracy=48.41%\n",
      "Saved updated CSV with Date, Actual & Predicted: tat_mqa_output_csv\\capsicum_Kolar_daily_tat_mqa_updated.csv\n",
      "ğŸš€ Processing: capsicum_Mandya_daily.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_9\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_9\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                  </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape              </span>â”ƒ<span style=\"font-weight: bold\">         Param # </span>â”ƒ<span style=\"font-weight: bold\"> Connected to               </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)             â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                          â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ multi_query_attention_9       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">17,216</span> â”‚ input_layer_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiQueryAttention</span>)         â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ add_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ multi_query_attention_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>â€¦ â”‚\n",
       "â”‚                               â”‚                           â”‚                 â”‚ input_layer_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization_18        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> â”‚ add_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]               â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)          â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_97 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> â”‚ layer_normalization_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]â€¦ â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_98 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)             â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> â”‚ dense_97[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ add_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ layer_normalization_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]â€¦ â”‚\n",
       "â”‚                               â”‚                           â”‚                 â”‚ dense_98[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization_19        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> â”‚ add_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]               â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)          â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_average_pooling1d_9    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ layer_normalization_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]â€¦ â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)      â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_99 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                 â”‚              <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> â”‚ global_average_pooling1d_â€¦ â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer_9 (\u001b[38;5;33mInputLayer\u001b[0m)    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m1\u001b[0m)             â”‚               \u001b[38;5;34m0\u001b[0m â”‚ -                          â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ multi_query_attention_9       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)            â”‚          \u001b[38;5;34m17,216\u001b[0m â”‚ input_layer_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        â”‚\n",
       "â”‚ (\u001b[38;5;33mMultiQueryAttention\u001b[0m)         â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ add_18 (\u001b[38;5;33mAdd\u001b[0m)                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)            â”‚               \u001b[38;5;34m0\u001b[0m â”‚ multi_query_attention_9[\u001b[38;5;34m0\u001b[0mâ€¦ â”‚\n",
       "â”‚                               â”‚                           â”‚                 â”‚ input_layer_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization_18        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)            â”‚             \u001b[38;5;34m128\u001b[0m â”‚ add_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]               â”‚\n",
       "â”‚ (\u001b[38;5;33mLayerNormalization\u001b[0m)          â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_97 (\u001b[38;5;33mDense\u001b[0m)              â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)           â”‚           \u001b[38;5;34m8,320\u001b[0m â”‚ layer_normalization_18[\u001b[38;5;34m0\u001b[0m]â€¦ â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_98 (\u001b[38;5;33mDense\u001b[0m)              â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m1\u001b[0m)             â”‚             \u001b[38;5;34m129\u001b[0m â”‚ dense_97[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ add_19 (\u001b[38;5;33mAdd\u001b[0m)                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)            â”‚               \u001b[38;5;34m0\u001b[0m â”‚ layer_normalization_18[\u001b[38;5;34m0\u001b[0m]â€¦ â”‚\n",
       "â”‚                               â”‚                           â”‚                 â”‚ dense_98[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization_19        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)            â”‚             \u001b[38;5;34m128\u001b[0m â”‚ add_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]               â”‚\n",
       "â”‚ (\u001b[38;5;33mLayerNormalization\u001b[0m)          â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_average_pooling1d_9    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                â”‚               \u001b[38;5;34m0\u001b[0m â”‚ layer_normalization_19[\u001b[38;5;34m0\u001b[0m]â€¦ â”‚\n",
       "â”‚ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)      â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_99 (\u001b[38;5;33mDense\u001b[0m)              â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                 â”‚              \u001b[38;5;34m65\u001b[0m â”‚ global_average_pooling1d_â€¦ â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,986</span> (101.51 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m25,986\u001b[0m (101.51 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,986</span> (101.51 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m25,986\u001b[0m (101.51 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 12ms/step - loss: 0.2277 - mae: 0.2810 - val_loss: 0.0295 - val_mae: 0.1505\n",
      "Epoch 2/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.0168 - mae: 0.0960 - val_loss: 0.0375 - val_mae: 0.1729\n",
      "Epoch 3/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.0127 - mae: 0.0838 - val_loss: 0.0384 - val_mae: 0.1749\n",
      "Epoch 4/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.0100 - mae: 0.0718 - val_loss: 0.0404 - val_mae: 0.1797\n",
      "Epoch 5/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.0093 - mae: 0.0679 - val_loss: 0.0360 - val_mae: 0.1695\n",
      "Epoch 6/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.0077 - mae: 0.0610 - val_loss: 0.0256 - val_mae: 0.1392\n",
      "Epoch 7/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.0065 - mae: 0.0537 - val_loss: 0.0137 - val_mae: 0.0944\n",
      "Epoch 8/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.0060 - mae: 0.0508 - val_loss: 0.0137 - val_mae: 0.0971\n",
      "Epoch 9/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0058 - mae: 0.0498 - val_loss: 0.0101 - val_mae: 0.0801\n",
      "Epoch 10/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.0064 - mae: 0.0497 - val_loss: 0.0110 - val_mae: 0.0871\n",
      "Epoch 11/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.0061 - mae: 0.0497 - val_loss: 0.0173 - val_mae: 0.1148\n",
      "Epoch 12/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.0054 - mae: 0.0449 - val_loss: 0.0101 - val_mae: 0.0722\n",
      "Epoch 13/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.0054 - mae: 0.0439 - val_loss: 0.0087 - val_mae: 0.0688\n",
      "Epoch 14/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.0060 - mae: 0.0503 - val_loss: 0.0083 - val_mae: 0.0752\n",
      "Epoch 15/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.0052 - mae: 0.0418 - val_loss: 0.0109 - val_mae: 0.0828\n",
      "Epoch 16/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0047 - mae: 0.0424 - val_loss: 0.0082 - val_mae: 0.0745\n",
      "Epoch 17/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0051 - mae: 0.0409 - val_loss: 0.0075 - val_mae: 0.0648\n",
      "Epoch 18/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.0048 - mae: 0.0401 - val_loss: 0.0075 - val_mae: 0.0646\n",
      "Epoch 19/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.0047 - mae: 0.0377 - val_loss: 0.0073 - val_mae: 0.0649\n",
      "Epoch 20/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0051 - mae: 0.0426 - val_loss: 0.0074 - val_mae: 0.0698\n",
      "Epoch 21/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.0049 - mae: 0.0406 - val_loss: 0.0085 - val_mae: 0.0759\n",
      "Epoch 22/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0048 - mae: 0.0403 - val_loss: 0.0145 - val_mae: 0.1030\n",
      "Epoch 23/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.0046 - mae: 0.0406 - val_loss: 0.0124 - val_mae: 0.0910\n",
      "Epoch 24/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0048 - mae: 0.0396 - val_loss: 0.0074 - val_mae: 0.0691\n",
      "Epoch 25/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.0046 - mae: 0.0399 - val_loss: 0.0100 - val_mae: 0.0817\n",
      "Epoch 26/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.0047 - mae: 0.0400 - val_loss: 0.0071 - val_mae: 0.0632\n",
      "Epoch 27/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.0045 - mae: 0.0392 - val_loss: 0.0072 - val_mae: 0.0671\n",
      "Epoch 28/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.0051 - mae: 0.0405 - val_loss: 0.0071 - val_mae: 0.0628\n",
      "Epoch 29/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.0047 - mae: 0.0417 - val_loss: 0.0100 - val_mae: 0.0828\n",
      "Epoch 30/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.0045 - mae: 0.0391 - val_loss: 0.0072 - val_mae: 0.0649\n",
      "Epoch 31/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.0051 - mae: 0.0420 - val_loss: 0.0088 - val_mae: 0.0761\n",
      "Epoch 32/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0054 - mae: 0.0437 - val_loss: 0.0073 - val_mae: 0.0647\n",
      "Epoch 33/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.0048 - mae: 0.0393 - val_loss: 0.0075 - val_mae: 0.0667\n",
      "Epoch 34/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.0043 - mae: 0.0367 - val_loss: 0.0071 - val_mae: 0.0631\n",
      "Epoch 35/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.0049 - mae: 0.0409 - val_loss: 0.0071 - val_mae: 0.0656\n",
      "Epoch 36/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0045 - mae: 0.0381 - val_loss: 0.0108 - val_mae: 0.0865\n",
      "Epoch 37/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0046 - mae: 0.0382 - val_loss: 0.0137 - val_mae: 0.0998\n",
      "Epoch 38/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0042 - mae: 0.0375 - val_loss: 0.0086 - val_mae: 0.0759\n",
      "Epoch 39/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0055 - mae: 0.0426 - val_loss: 0.0094 - val_mae: 0.0795\n",
      "Epoch 40/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0041 - mae: 0.0360 - val_loss: 0.0082 - val_mae: 0.0730\n",
      "Epoch 41/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0047 - mae: 0.0394 - val_loss: 0.0073 - val_mae: 0.0655\n",
      "Epoch 42/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0046 - mae: 0.0363 - val_loss: 0.0082 - val_mae: 0.0734\n",
      "Epoch 43/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0045 - mae: 0.0366 - val_loss: 0.0072 - val_mae: 0.0638\n",
      "Epoch 44/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0047 - mae: 0.0388 - val_loss: 0.0073 - val_mae: 0.0668\n",
      "Epoch 45/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0046 - mae: 0.0380 - val_loss: 0.0081 - val_mae: 0.0721\n",
      "Epoch 46/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0044 - mae: 0.0370 - val_loss: 0.0074 - val_mae: 0.0674\n",
      "Epoch 47/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0045 - mae: 0.0374 - val_loss: 0.0072 - val_mae: 0.0668\n",
      "Epoch 48/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0045 - mae: 0.0368 - val_loss: 0.0088 - val_mae: 0.0774\n",
      "Epoch 49/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0047 - mae: 0.0387 - val_loss: 0.0098 - val_mae: 0.0839\n",
      "Epoch 50/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0046 - mae: 0.0379 - val_loss: 0.0071 - val_mae: 0.0648\n",
      "Epoch 51/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0046 - mae: 0.0383 - val_loss: 0.0111 - val_mae: 0.0909\n",
      "Epoch 52/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0049 - mae: 0.0394 - val_loss: 0.0072 - val_mae: 0.0636\n",
      "Epoch 53/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0045 - mae: 0.0384 - val_loss: 0.0105 - val_mae: 0.0880\n",
      "Epoch 54/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0046 - mae: 0.0382 - val_loss: 0.0075 - val_mae: 0.0692\n",
      "Epoch 55/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0042 - mae: 0.0355 - val_loss: 0.0076 - val_mae: 0.0703\n",
      "Epoch 56/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0043 - mae: 0.0368 - val_loss: 0.0081 - val_mae: 0.0737\n",
      "Epoch 57/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0043 - mae: 0.0360 - val_loss: 0.0072 - val_mae: 0.0644\n",
      "Epoch 58/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0042 - mae: 0.0349 - val_loss: 0.0109 - val_mae: 0.0906\n",
      "Epoch 59/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0045 - mae: 0.0375 - val_loss: 0.0094 - val_mae: 0.0822\n",
      "Epoch 60/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0048 - mae: 0.0376 - val_loss: 0.0072 - val_mae: 0.0641\n",
      "Epoch 61/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0044 - mae: 0.0359 - val_loss: 0.0099 - val_mae: 0.0857\n",
      "Epoch 62/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0046 - mae: 0.0360 - val_loss: 0.0083 - val_mae: 0.0763\n",
      "Epoch 63/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0049 - mae: 0.0392 - val_loss: 0.0072 - val_mae: 0.0643\n",
      "Epoch 64/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0045 - mae: 0.0355 - val_loss: 0.0074 - val_mae: 0.0680\n",
      "Epoch 65/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0047 - mae: 0.0377 - val_loss: 0.0176 - val_mae: 0.1169\n",
      "Epoch 66/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0045 - mae: 0.0363 - val_loss: 0.0078 - val_mae: 0.0708\n",
      "Epoch 67/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0047 - mae: 0.0368 - val_loss: 0.0088 - val_mae: 0.0790\n",
      "Epoch 68/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0045 - mae: 0.0363 - val_loss: 0.0112 - val_mae: 0.0915\n",
      "Epoch 69/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0042 - mae: 0.0365 - val_loss: 0.0115 - val_mae: 0.0931\n",
      "Epoch 70/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0045 - mae: 0.0364 - val_loss: 0.0085 - val_mae: 0.0781\n",
      "Epoch 71/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0042 - mae: 0.0335 - val_loss: 0.0088 - val_mae: 0.0798\n",
      "Epoch 72/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0049 - mae: 0.0371 - val_loss: 0.0090 - val_mae: 0.0815\n",
      "Epoch 73/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0042 - mae: 0.0353 - val_loss: 0.0101 - val_mae: 0.0807\n",
      "Epoch 74/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0052 - mae: 0.0369 - val_loss: 0.0074 - val_mae: 0.0671\n",
      "Epoch 75/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0044 - mae: 0.0355 - val_loss: 0.0073 - val_mae: 0.0650\n",
      "Epoch 76/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0044 - mae: 0.0348 - val_loss: 0.0075 - val_mae: 0.0691\n",
      "Epoch 77/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0045 - mae: 0.0380 - val_loss: 0.0074 - val_mae: 0.0679\n",
      "Epoch 78/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0044 - mae: 0.0349 - val_loss: 0.0088 - val_mae: 0.0807\n",
      "Epoch 79/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0043 - mae: 0.0360 - val_loss: 0.0097 - val_mae: 0.0855\n",
      "Epoch 80/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0042 - mae: 0.0339 - val_loss: 0.0073 - val_mae: 0.0655\n",
      "Epoch 81/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0043 - mae: 0.0352 - val_loss: 0.0076 - val_mae: 0.0716\n",
      "Epoch 82/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0042 - mae: 0.0362 - val_loss: 0.0116 - val_mae: 0.0943\n",
      "Epoch 83/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0049 - mae: 0.0382 - val_loss: 0.0091 - val_mae: 0.0827\n",
      "Epoch 84/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0043 - mae: 0.0335 - val_loss: 0.0074 - val_mae: 0.0676\n",
      "Epoch 85/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0048 - mae: 0.0370 - val_loss: 0.0098 - val_mae: 0.0868\n",
      "Epoch 86/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0043 - mae: 0.0357 - val_loss: 0.0075 - val_mae: 0.0694\n",
      "Epoch 87/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0043 - mae: 0.0373 - val_loss: 0.0104 - val_mae: 0.0897\n",
      "Epoch 88/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0040 - mae: 0.0331 - val_loss: 0.0076 - val_mae: 0.0714\n",
      "Epoch 89/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0041 - mae: 0.0338 - val_loss: 0.0082 - val_mae: 0.0770\n",
      "Epoch 90/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0043 - mae: 0.0346 - val_loss: 0.0080 - val_mae: 0.0753\n",
      "Epoch 91/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0046 - mae: 0.0356 - val_loss: 0.0103 - val_mae: 0.0890\n",
      "Epoch 92/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0040 - mae: 0.0339 - val_loss: 0.0075 - val_mae: 0.0694\n",
      "Epoch 93/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0046 - mae: 0.0367 - val_loss: 0.0073 - val_mae: 0.0659\n",
      "Epoch 94/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0050 - mae: 0.0367 - val_loss: 0.0075 - val_mae: 0.0699\n",
      "Epoch 95/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0044 - mae: 0.0346 - val_loss: 0.0082 - val_mae: 0.0769\n",
      "Epoch 96/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0040 - mae: 0.0340 - val_loss: 0.0145 - val_mae: 0.1068\n",
      "Epoch 97/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0043 - mae: 0.0351 - val_loss: 0.0113 - val_mae: 0.0930\n",
      "Epoch 98/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0046 - mae: 0.0364 - val_loss: 0.0073 - val_mae: 0.0654\n",
      "Epoch 99/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0043 - mae: 0.0345 - val_loss: 0.0082 - val_mae: 0.0772\n",
      "Epoch 100/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0046 - mae: 0.0349 - val_loss: 0.0074 - val_mae: 0.0676\n",
      "\u001b[1m178/178\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "âœ… Done with capsicum_Mandya_daily.csv | MAE=214.69, RMSE=392.76, R2=0.8836, MAPE=9.41%, Accuracy=90.59%\n",
      "Saved updated CSV with Date, Actual & Predicted: tat_mqa_output_csv\\capsicum_Mandya_daily_tat_mqa_updated.csv\n",
      "ğŸš€ Processing: capsicum_Mysore_daily.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_10\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_10\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                  </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape              </span>â”ƒ<span style=\"font-weight: bold\">         Param # </span>â”ƒ<span style=\"font-weight: bold\"> Connected to               </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)             â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                          â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ multi_query_attention_10      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">17,216</span> â”‚ input_layer_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiQueryAttention</span>)         â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ add_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ multi_query_attention_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">â€¦</span> â”‚\n",
       "â”‚                               â”‚                           â”‚                 â”‚ input_layer_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization_20        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> â”‚ add_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]               â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)          â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_107 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> â”‚ layer_normalization_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]â€¦ â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_108 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)             â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> â”‚ dense_107[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ add_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ layer_normalization_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]â€¦ â”‚\n",
       "â”‚                               â”‚                           â”‚                 â”‚ dense_108[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization_21        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> â”‚ add_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]               â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)          â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_average_pooling1d_10   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ layer_normalization_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]â€¦ â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)      â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_109 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                 â”‚              <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> â”‚ global_average_pooling1d_â€¦ â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer_10 (\u001b[38;5;33mInputLayer\u001b[0m)   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m1\u001b[0m)             â”‚               \u001b[38;5;34m0\u001b[0m â”‚ -                          â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ multi_query_attention_10      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)            â”‚          \u001b[38;5;34m17,216\u001b[0m â”‚ input_layer_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       â”‚\n",
       "â”‚ (\u001b[38;5;33mMultiQueryAttention\u001b[0m)         â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ add_20 (\u001b[38;5;33mAdd\u001b[0m)                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)            â”‚               \u001b[38;5;34m0\u001b[0m â”‚ multi_query_attention_10[\u001b[38;5;34mâ€¦\u001b[0m â”‚\n",
       "â”‚                               â”‚                           â”‚                 â”‚ input_layer_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization_20        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)            â”‚             \u001b[38;5;34m128\u001b[0m â”‚ add_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]               â”‚\n",
       "â”‚ (\u001b[38;5;33mLayerNormalization\u001b[0m)          â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_107 (\u001b[38;5;33mDense\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)           â”‚           \u001b[38;5;34m8,320\u001b[0m â”‚ layer_normalization_20[\u001b[38;5;34m0\u001b[0m]â€¦ â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_108 (\u001b[38;5;33mDense\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m1\u001b[0m)             â”‚             \u001b[38;5;34m129\u001b[0m â”‚ dense_107[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ add_21 (\u001b[38;5;33mAdd\u001b[0m)                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)            â”‚               \u001b[38;5;34m0\u001b[0m â”‚ layer_normalization_20[\u001b[38;5;34m0\u001b[0m]â€¦ â”‚\n",
       "â”‚                               â”‚                           â”‚                 â”‚ dense_108[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization_21        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)            â”‚             \u001b[38;5;34m128\u001b[0m â”‚ add_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]               â”‚\n",
       "â”‚ (\u001b[38;5;33mLayerNormalization\u001b[0m)          â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_average_pooling1d_10   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                â”‚               \u001b[38;5;34m0\u001b[0m â”‚ layer_normalization_21[\u001b[38;5;34m0\u001b[0m]â€¦ â”‚\n",
       "â”‚ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)      â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_109 (\u001b[38;5;33mDense\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                 â”‚              \u001b[38;5;34m65\u001b[0m â”‚ global_average_pooling1d_â€¦ â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,986</span> (101.51 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m25,986\u001b[0m (101.51 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,986</span> (101.51 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m25,986\u001b[0m (101.51 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11ms/step - loss: 0.1544 - mae: 0.1982 - val_loss: 0.0021 - val_mae: 0.0322\n",
      "Epoch 2/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0048 - mae: 0.0547 - val_loss: 0.0026 - val_mae: 0.0398\n",
      "Epoch 3/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0027 - mae: 0.0404 - val_loss: 0.0026 - val_mae: 0.0396\n",
      "Epoch 4/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0018 - mae: 0.0322 - val_loss: 0.0020 - val_mae: 0.0322\n",
      "Epoch 5/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0015 - mae: 0.0288 - val_loss: 0.0021 - val_mae: 0.0320\n",
      "Epoch 6/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0014 - mae: 0.0278 - val_loss: 0.0021 - val_mae: 0.0318\n",
      "Epoch 7/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0013 - mae: 0.0262 - val_loss: 0.0020 - val_mae: 0.0316\n",
      "Epoch 8/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0013 - mae: 0.0269 - val_loss: 0.0030 - val_mae: 0.0443\n",
      "Epoch 9/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0013 - mae: 0.0267 - val_loss: 0.0020 - val_mae: 0.0325\n",
      "Epoch 10/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0013 - mae: 0.0265 - val_loss: 0.0021 - val_mae: 0.0340\n",
      "Epoch 11/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0012 - mae: 0.0258 - val_loss: 0.0020 - val_mae: 0.0314\n",
      "Epoch 12/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0013 - mae: 0.0264 - val_loss: 0.0031 - val_mae: 0.0446\n",
      "Epoch 13/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0013 - mae: 0.0268 - val_loss: 0.0022 - val_mae: 0.0323\n",
      "Epoch 14/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0012 - mae: 0.0257 - val_loss: 0.0023 - val_mae: 0.0361\n",
      "Epoch 15/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0013 - mae: 0.0259 - val_loss: 0.0020 - val_mae: 0.0318\n",
      "Epoch 16/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0012 - mae: 0.0259 - val_loss: 0.0023 - val_mae: 0.0354\n",
      "Epoch 17/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 11ms/step - loss: 0.0012 - mae: 0.0255 - val_loss: 0.0033 - val_mae: 0.0461\n",
      "Epoch 18/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 11ms/step - loss: 0.0013 - mae: 0.0264 - val_loss: 0.0020 - val_mae: 0.0321\n",
      "Epoch 19/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 11ms/step - loss: 0.0012 - mae: 0.0252 - val_loss: 0.0021 - val_mae: 0.0330\n",
      "Epoch 20/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0012 - mae: 0.0249 - val_loss: 0.0020 - val_mae: 0.0315\n",
      "Epoch 21/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0012 - mae: 0.0256 - val_loss: 0.0020 - val_mae: 0.0313\n",
      "Epoch 22/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 11ms/step - loss: 0.0012 - mae: 0.0251 - val_loss: 0.0028 - val_mae: 0.0409\n",
      "Epoch 23/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 11ms/step - loss: 0.0012 - mae: 0.0252 - val_loss: 0.0020 - val_mae: 0.0315\n",
      "Epoch 24/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0012 - mae: 0.0257 - val_loss: 0.0020 - val_mae: 0.0321\n",
      "Epoch 25/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0012 - mae: 0.0248 - val_loss: 0.0021 - val_mae: 0.0334\n",
      "Epoch 26/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0012 - mae: 0.0246 - val_loss: 0.0020 - val_mae: 0.0314\n",
      "Epoch 27/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0012 - mae: 0.0247 - val_loss: 0.0023 - val_mae: 0.0335\n",
      "Epoch 28/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0011 - mae: 0.0244 - val_loss: 0.0020 - val_mae: 0.0322\n",
      "Epoch 29/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0011 - mae: 0.0239 - val_loss: 0.0038 - val_mae: 0.0501\n",
      "Epoch 30/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0014 - mae: 0.0273 - val_loss: 0.0021 - val_mae: 0.0335\n",
      "Epoch 31/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0011 - mae: 0.0242 - val_loss: 0.0020 - val_mae: 0.0313\n",
      "Epoch 32/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0011 - mae: 0.0242 - val_loss: 0.0020 - val_mae: 0.0324\n",
      "Epoch 33/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0011 - mae: 0.0239 - val_loss: 0.0020 - val_mae: 0.0320\n",
      "Epoch 34/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0011 - mae: 0.0239 - val_loss: 0.0021 - val_mae: 0.0334\n",
      "Epoch 35/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0011 - mae: 0.0239 - val_loss: 0.0022 - val_mae: 0.0342\n",
      "Epoch 36/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0011 - mae: 0.0232 - val_loss: 0.0020 - val_mae: 0.0318\n",
      "Epoch 37/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0011 - mae: 0.0240 - val_loss: 0.0020 - val_mae: 0.0318\n",
      "Epoch 38/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0011 - mae: 0.0238 - val_loss: 0.0020 - val_mae: 0.0320\n",
      "Epoch 39/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0011 - mae: 0.0239 - val_loss: 0.0021 - val_mae: 0.0328\n",
      "Epoch 40/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0011 - mae: 0.0238 - val_loss: 0.0020 - val_mae: 0.0314\n",
      "Epoch 41/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0011 - mae: 0.0239 - val_loss: 0.0022 - val_mae: 0.0352\n",
      "Epoch 42/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0012 - mae: 0.0245 - val_loss: 0.0021 - val_mae: 0.0336\n",
      "Epoch 43/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0011 - mae: 0.0236 - val_loss: 0.0020 - val_mae: 0.0326\n",
      "Epoch 44/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0011 - mae: 0.0237 - val_loss: 0.0020 - val_mae: 0.0313\n",
      "Epoch 45/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0011 - mae: 0.0236 - val_loss: 0.0020 - val_mae: 0.0317\n",
      "Epoch 46/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0011 - mae: 0.0239 - val_loss: 0.0020 - val_mae: 0.0317\n",
      "Epoch 47/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0011 - mae: 0.0237 - val_loss: 0.0021 - val_mae: 0.0322\n",
      "Epoch 48/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0011 - mae: 0.0237 - val_loss: 0.0022 - val_mae: 0.0349\n",
      "Epoch 49/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0011 - mae: 0.0244 - val_loss: 0.0020 - val_mae: 0.0325\n",
      "Epoch 50/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0011 - mae: 0.0234 - val_loss: 0.0020 - val_mae: 0.0325\n",
      "Epoch 51/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0011 - mae: 0.0230 - val_loss: 0.0020 - val_mae: 0.0320\n",
      "Epoch 52/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - loss: 0.0011 - mae: 0.0239 - val_loss: 0.0020 - val_mae: 0.0325\n",
      "Epoch 53/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0011 - mae: 0.0234 - val_loss: 0.0022 - val_mae: 0.0343\n",
      "Epoch 54/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0011 - mae: 0.0235 - val_loss: 0.0024 - val_mae: 0.0369\n",
      "Epoch 55/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0012 - mae: 0.0241 - val_loss: 0.0020 - val_mae: 0.0313\n",
      "Epoch 56/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0011 - mae: 0.0236 - val_loss: 0.0020 - val_mae: 0.0325\n",
      "Epoch 57/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0011 - mae: 0.0240 - val_loss: 0.0020 - val_mae: 0.0314\n",
      "Epoch 58/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0011 - mae: 0.0236 - val_loss: 0.0020 - val_mae: 0.0316\n",
      "Epoch 59/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0011 - mae: 0.0234 - val_loss: 0.0023 - val_mae: 0.0354\n",
      "Epoch 60/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0011 - mae: 0.0234 - val_loss: 0.0021 - val_mae: 0.0337\n",
      "Epoch 61/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0011 - mae: 0.0236 - val_loss: 0.0020 - val_mae: 0.0317\n",
      "Epoch 62/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0012 - mae: 0.0239 - val_loss: 0.0022 - val_mae: 0.0342\n",
      "Epoch 63/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0011 - mae: 0.0239 - val_loss: 0.0022 - val_mae: 0.0339\n",
      "Epoch 64/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0011 - mae: 0.0232 - val_loss: 0.0022 - val_mae: 0.0346\n",
      "Epoch 65/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0011 - mae: 0.0234 - val_loss: 0.0026 - val_mae: 0.0384\n",
      "Epoch 66/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0011 - mae: 0.0235 - val_loss: 0.0020 - val_mae: 0.0325\n",
      "Epoch 67/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0011 - mae: 0.0238 - val_loss: 0.0020 - val_mae: 0.0319\n",
      "Epoch 68/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0012 - mae: 0.0237 - val_loss: 0.0020 - val_mae: 0.0318\n",
      "Epoch 69/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0011 - mae: 0.0234 - val_loss: 0.0026 - val_mae: 0.0382\n",
      "Epoch 70/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0010 - mae: 0.0230 - val_loss: 0.0021 - val_mae: 0.0338\n",
      "Epoch 71/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0011 - mae: 0.0233 - val_loss: 0.0021 - val_mae: 0.0337\n",
      "Epoch 72/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0010 - mae: 0.0228 - val_loss: 0.0020 - val_mae: 0.0322\n",
      "Epoch 73/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0011 - mae: 0.0234 - val_loss: 0.0020 - val_mae: 0.0321\n",
      "Epoch 74/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0011 - mae: 0.0232 - val_loss: 0.0020 - val_mae: 0.0314\n",
      "Epoch 75/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0011 - mae: 0.0229 - val_loss: 0.0020 - val_mae: 0.0326\n",
      "Epoch 76/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0011 - mae: 0.0233 - val_loss: 0.0020 - val_mae: 0.0318\n",
      "Epoch 77/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0011 - mae: 0.0230 - val_loss: 0.0021 - val_mae: 0.0336\n",
      "Epoch 78/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0010 - mae: 0.0229 - val_loss: 0.0020 - val_mae: 0.0316\n",
      "Epoch 79/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0010 - mae: 0.0227 - val_loss: 0.0020 - val_mae: 0.0320\n",
      "Epoch 80/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0011 - mae: 0.0230 - val_loss: 0.0020 - val_mae: 0.0327\n",
      "Epoch 81/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0011 - mae: 0.0231 - val_loss: 0.0022 - val_mae: 0.0344\n",
      "Epoch 82/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0010 - mae: 0.0227 - val_loss: 0.0024 - val_mae: 0.0362\n",
      "Epoch 83/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0010 - mae: 0.0229 - val_loss: 0.0022 - val_mae: 0.0346\n",
      "Epoch 84/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0011 - mae: 0.0234 - val_loss: 0.0020 - val_mae: 0.0321\n",
      "Epoch 85/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0011 - mae: 0.0229 - val_loss: 0.0020 - val_mae: 0.0325\n",
      "Epoch 86/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0011 - mae: 0.0234 - val_loss: 0.0021 - val_mae: 0.0330\n",
      "Epoch 87/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0010 - mae: 0.0225 - val_loss: 0.0020 - val_mae: 0.0320\n",
      "Epoch 88/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0011 - mae: 0.0230 - val_loss: 0.0021 - val_mae: 0.0331\n",
      "Epoch 89/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0011 - mae: 0.0228 - val_loss: 0.0025 - val_mae: 0.0370\n",
      "Epoch 90/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 0.0011 - mae: 0.0235 - val_loss: 0.0022 - val_mae: 0.0347\n",
      "Epoch 91/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0010 - mae: 0.0227 - val_loss: 0.0021 - val_mae: 0.0333\n",
      "Epoch 92/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0011 - mae: 0.0230 - val_loss: 0.0023 - val_mae: 0.0349\n",
      "Epoch 93/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0011 - mae: 0.0231 - val_loss: 0.0020 - val_mae: 0.0325\n",
      "Epoch 94/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0011 - mae: 0.0231 - val_loss: 0.0023 - val_mae: 0.0351\n",
      "Epoch 95/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0011 - mae: 0.0232 - val_loss: 0.0022 - val_mae: 0.0339\n",
      "Epoch 96/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0011 - mae: 0.0232 - val_loss: 0.0021 - val_mae: 0.0330\n",
      "Epoch 97/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0011 - mae: 0.0233 - val_loss: 0.0021 - val_mae: 0.0329\n",
      "Epoch 98/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0010 - mae: 0.0226 - val_loss: 0.0020 - val_mae: 0.0322\n",
      "Epoch 99/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0011 - mae: 0.0230 - val_loss: 0.0020 - val_mae: 0.0324\n",
      "Epoch 100/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0010 - mae: 0.0228 - val_loss: 0.0023 - val_mae: 0.0349\n",
      "\u001b[1m319/319\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step\n",
      "âœ… Done with capsicum_Mysore_daily.csv | MAE=683.64, RMSE=913.12, R2=0.4694, MAPE=29.78%, Accuracy=70.22%\n",
      "Saved updated CSV with Date, Actual & Predicted: tat_mqa_output_csv\\capsicum_Mysore_daily_tat_mqa_updated.csv\n",
      "ğŸš€ Processing: capsicum_Shimoga_daily.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_11\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_11\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                  </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape              </span>â”ƒ<span style=\"font-weight: bold\">         Param # </span>â”ƒ<span style=\"font-weight: bold\"> Connected to               </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)             â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                          â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ multi_query_attention_11      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">17,216</span> â”‚ input_layer_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiQueryAttention</span>)         â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ add_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ multi_query_attention_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">â€¦</span> â”‚\n",
       "â”‚                               â”‚                           â”‚                 â”‚ input_layer_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization_22        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> â”‚ add_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]               â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)          â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_117 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> â”‚ layer_normalization_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]â€¦ â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_118 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)             â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> â”‚ dense_117[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ add_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ layer_normalization_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]â€¦ â”‚\n",
       "â”‚                               â”‚                           â”‚                 â”‚ dense_118[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization_23        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> â”‚ add_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]               â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)          â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_average_pooling1d_11   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ layer_normalization_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]â€¦ â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)      â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_119 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                 â”‚              <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> â”‚ global_average_pooling1d_â€¦ â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer_11 (\u001b[38;5;33mInputLayer\u001b[0m)   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m1\u001b[0m)             â”‚               \u001b[38;5;34m0\u001b[0m â”‚ -                          â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ multi_query_attention_11      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)            â”‚          \u001b[38;5;34m17,216\u001b[0m â”‚ input_layer_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       â”‚\n",
       "â”‚ (\u001b[38;5;33mMultiQueryAttention\u001b[0m)         â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ add_22 (\u001b[38;5;33mAdd\u001b[0m)                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)            â”‚               \u001b[38;5;34m0\u001b[0m â”‚ multi_query_attention_11[\u001b[38;5;34mâ€¦\u001b[0m â”‚\n",
       "â”‚                               â”‚                           â”‚                 â”‚ input_layer_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization_22        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)            â”‚             \u001b[38;5;34m128\u001b[0m â”‚ add_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]               â”‚\n",
       "â”‚ (\u001b[38;5;33mLayerNormalization\u001b[0m)          â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_117 (\u001b[38;5;33mDense\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)           â”‚           \u001b[38;5;34m8,320\u001b[0m â”‚ layer_normalization_22[\u001b[38;5;34m0\u001b[0m]â€¦ â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_118 (\u001b[38;5;33mDense\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m1\u001b[0m)             â”‚             \u001b[38;5;34m129\u001b[0m â”‚ dense_117[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ add_23 (\u001b[38;5;33mAdd\u001b[0m)                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)            â”‚               \u001b[38;5;34m0\u001b[0m â”‚ layer_normalization_22[\u001b[38;5;34m0\u001b[0m]â€¦ â”‚\n",
       "â”‚                               â”‚                           â”‚                 â”‚ dense_118[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization_23        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)            â”‚             \u001b[38;5;34m128\u001b[0m â”‚ add_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]               â”‚\n",
       "â”‚ (\u001b[38;5;33mLayerNormalization\u001b[0m)          â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_average_pooling1d_11   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                â”‚               \u001b[38;5;34m0\u001b[0m â”‚ layer_normalization_23[\u001b[38;5;34m0\u001b[0m]â€¦ â”‚\n",
       "â”‚ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)      â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_119 (\u001b[38;5;33mDense\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                 â”‚              \u001b[38;5;34m65\u001b[0m â”‚ global_average_pooling1d_â€¦ â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,986</span> (101.51 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m25,986\u001b[0m (101.51 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,986</span> (101.51 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m25,986\u001b[0m (101.51 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 184ms/step - loss: 0.8374 - mae: 0.8339 - val_loss: 1.2577 - val_mae: 1.1207\n",
      "Epoch 2/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.5145 - mae: 0.6788 - val_loss: 0.0386 - val_mae: 0.1918\n",
      "Epoch 3/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.2325 - mae: 0.4480 - val_loss: 0.0248 - val_mae: 0.1525\n",
      "Epoch 4/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.0713 - mae: 0.2184 - val_loss: 0.2904 - val_mae: 0.5376\n",
      "Epoch 5/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0741 - mae: 0.2417 - val_loss: 0.0324 - val_mae: 0.1760\n",
      "Epoch 6/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.1029 - mae: 0.2968 - val_loss: 0.0122 - val_mae: 0.1044\n",
      "Epoch 7/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.0331 - mae: 0.1413 - val_loss: 0.1056 - val_mae: 0.3230\n",
      "Epoch 8/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.0404 - mae: 0.1675 - val_loss: 0.0021 - val_mae: 0.0376\n",
      "Epoch 9/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0311 - mae: 0.1457 - val_loss: 0.0199 - val_mae: 0.1367\n",
      "Epoch 10/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0174 - mae: 0.1053 - val_loss: 0.0539 - val_mae: 0.2296\n",
      "Epoch 11/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.0255 - mae: 0.1292 - val_loss: 0.0013 - val_mae: 0.0310\n",
      "Epoch 12/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.0215 - mae: 0.1132 - val_loss: 0.0117 - val_mae: 0.1023\n",
      "Epoch 13/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.0160 - mae: 0.1047 - val_loss: 0.0315 - val_mae: 0.1739\n",
      "Epoch 14/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.0110 - mae: 0.0807 - val_loss: 0.0076 - val_mae: 0.0794\n",
      "Epoch 15/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0201 - mae: 0.1124 - val_loss: 0.0146 - val_mae: 0.1152\n",
      "Epoch 16/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.0162 - mae: 0.1014 - val_loss: 0.0340 - val_mae: 0.1808\n",
      "Epoch 17/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0166 - mae: 0.1038 - val_loss: 0.0024 - val_mae: 0.0394\n",
      "Epoch 18/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.0134 - mae: 0.0951 - val_loss: 0.0180 - val_mae: 0.1292\n",
      "Epoch 19/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0149 - mae: 0.0958 - val_loss: 0.0250 - val_mae: 0.1541\n",
      "Epoch 20/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.0155 - mae: 0.0964 - val_loss: 0.0075 - val_mae: 0.0785\n",
      "Epoch 21/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0127 - mae: 0.0860 - val_loss: 0.0142 - val_mae: 0.1134\n",
      "Epoch 22/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0124 - mae: 0.0899 - val_loss: 0.0133 - val_mae: 0.1097\n",
      "Epoch 23/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.0115 - mae: 0.0943 - val_loss: 0.0268 - val_mae: 0.1596\n",
      "Epoch 24/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.0118 - mae: 0.0858 - val_loss: 0.0168 - val_mae: 0.1244\n",
      "Epoch 25/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0181 - mae: 0.1052 - val_loss: 0.0184 - val_mae: 0.1310\n",
      "Epoch 26/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0124 - mae: 0.0906 - val_loss: 0.0134 - val_mae: 0.1098\n",
      "Epoch 27/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0120 - mae: 0.0911 - val_loss: 0.0196 - val_mae: 0.1353\n",
      "Epoch 28/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.0077 - mae: 0.0678 - val_loss: 0.0095 - val_mae: 0.0907\n",
      "Epoch 29/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.0124 - mae: 0.0871 - val_loss: 0.0180 - val_mae: 0.1295\n",
      "Epoch 30/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.0122 - mae: 0.0915 - val_loss: 0.0046 - val_mae: 0.0582\n",
      "Epoch 31/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.0166 - mae: 0.1029 - val_loss: 0.0273 - val_mae: 0.1615\n",
      "Epoch 32/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.0151 - mae: 0.1027 - val_loss: 0.0070 - val_mae: 0.0763\n",
      "Epoch 33/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0184 - mae: 0.1145 - val_loss: 0.0039 - val_mae: 0.0524\n",
      "Epoch 34/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.0212 - mae: 0.1164 - val_loss: 0.0282 - val_mae: 0.1643\n",
      "Epoch 35/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.0164 - mae: 0.1083 - val_loss: 0.0067 - val_mae: 0.0741\n",
      "Epoch 36/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0125 - mae: 0.0878 - val_loss: 0.0178 - val_mae: 0.1287\n",
      "Epoch 37/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0120 - mae: 0.0866 - val_loss: 0.0059 - val_mae: 0.0683\n",
      "Epoch 38/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0139 - mae: 0.0994 - val_loss: 0.0234 - val_mae: 0.1486\n",
      "Epoch 39/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.0137 - mae: 0.0971 - val_loss: 0.0108 - val_mae: 0.0974\n",
      "Epoch 40/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.0163 - mae: 0.1043 - val_loss: 0.0294 - val_mae: 0.1678\n",
      "Epoch 41/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0173 - mae: 0.1061 - val_loss: 0.0197 - val_mae: 0.1357\n",
      "Epoch 42/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.0119 - mae: 0.0905 - val_loss: 0.0036 - val_mae: 0.0499\n",
      "Epoch 43/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0165 - mae: 0.1045 - val_loss: 0.0241 - val_mae: 0.1513\n",
      "Epoch 44/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0165 - mae: 0.1098 - val_loss: 0.0102 - val_mae: 0.0946\n",
      "Epoch 45/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0160 - mae: 0.0971 - val_loss: 0.0219 - val_mae: 0.1439\n",
      "Epoch 46/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.0204 - mae: 0.1176 - val_loss: 0.0074 - val_mae: 0.0785\n",
      "Epoch 47/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0113 - mae: 0.0868 - val_loss: 0.0082 - val_mae: 0.0834\n",
      "Epoch 48/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.0148 - mae: 0.0956 - val_loss: 0.0125 - val_mae: 0.1060\n",
      "Epoch 49/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0104 - mae: 0.0817 - val_loss: 0.0123 - val_mae: 0.1050\n",
      "Epoch 50/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.0107 - mae: 0.0832 - val_loss: 0.0238 - val_mae: 0.1503\n",
      "Epoch 51/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.0113 - mae: 0.0886 - val_loss: 0.0120 - val_mae: 0.1038\n",
      "Epoch 52/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.0133 - mae: 0.0909 - val_loss: 0.0102 - val_mae: 0.0948\n",
      "Epoch 53/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.0106 - mae: 0.0856 - val_loss: 0.0148 - val_mae: 0.1165\n",
      "Epoch 54/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.0153 - mae: 0.1001 - val_loss: 0.0150 - val_mae: 0.1173\n",
      "Epoch 55/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.0144 - mae: 0.1021 - val_loss: 0.0024 - val_mae: 0.0401\n",
      "Epoch 56/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0098 - mae: 0.0790 - val_loss: 0.0409 - val_mae: 0.1991\n",
      "Epoch 57/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.0126 - mae: 0.0893 - val_loss: 0.0027 - val_mae: 0.0424\n",
      "Epoch 58/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.0147 - mae: 0.0988 - val_loss: 0.0504 - val_mae: 0.2218\n",
      "Epoch 59/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.0143 - mae: 0.0980 - val_loss: 0.0064 - val_mae: 0.0718\n",
      "Epoch 60/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0152 - mae: 0.1003 - val_loss: 0.0151 - val_mae: 0.1175\n",
      "Epoch 61/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.0098 - mae: 0.0800 - val_loss: 0.0229 - val_mae: 0.1473\n",
      "Epoch 62/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 0.0138 - mae: 0.0954 - val_loss: 0.0092 - val_mae: 0.0891\n",
      "Epoch 63/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 0.0125 - mae: 0.0868 - val_loss: 0.0207 - val_mae: 0.1396\n",
      "Epoch 64/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0114 - mae: 0.0865 - val_loss: 0.0012 - val_mae: 0.0301\n",
      "Epoch 65/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0122 - mae: 0.0892 - val_loss: 0.0395 - val_mae: 0.1957\n",
      "Epoch 66/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.0213 - mae: 0.1176 - val_loss: 0.0013 - val_mae: 0.0312\n",
      "Epoch 67/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.0142 - mae: 0.0975 - val_loss: 0.0310 - val_mae: 0.1726\n",
      "Epoch 68/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0119 - mae: 0.0833 - val_loss: 0.0045 - val_mae: 0.0571\n",
      "Epoch 69/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.0191 - mae: 0.1092 - val_loss: 0.0319 - val_mae: 0.1753\n",
      "Epoch 70/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0119 - mae: 0.0866 - val_loss: 0.0057 - val_mae: 0.0674\n",
      "Epoch 71/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.0153 - mae: 0.1001 - val_loss: 0.0227 - val_mae: 0.1468\n",
      "Epoch 72/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.0129 - mae: 0.0909 - val_loss: 0.0108 - val_mae: 0.0978\n",
      "Epoch 73/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.0112 - mae: 0.0850 - val_loss: 0.0068 - val_mae: 0.0746\n",
      "Epoch 74/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0084 - mae: 0.0805 - val_loss: 0.0103 - val_mae: 0.0952\n",
      "Epoch 75/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.0090 - mae: 0.0785 - val_loss: 0.0153 - val_mae: 0.1189\n",
      "Epoch 76/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0133 - mae: 0.0959 - val_loss: 0.0123 - val_mae: 0.1051\n",
      "Epoch 77/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.0084 - mae: 0.0725 - val_loss: 0.0062 - val_mae: 0.0706\n",
      "Epoch 78/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0107 - mae: 0.0807 - val_loss: 0.0328 - val_mae: 0.1778\n",
      "Epoch 79/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0112 - mae: 0.0872 - val_loss: 0.0025 - val_mae: 0.0406\n",
      "Epoch 80/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.0118 - mae: 0.0887 - val_loss: 0.0305 - val_mae: 0.1713\n",
      "Epoch 81/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0149 - mae: 0.0926 - val_loss: 0.0016 - val_mae: 0.0332\n",
      "Epoch 82/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.0141 - mae: 0.0946 - val_loss: 0.0256 - val_mae: 0.1563\n",
      "Epoch 83/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.0128 - mae: 0.0903 - val_loss: 0.0078 - val_mae: 0.0815\n",
      "Epoch 84/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.0071 - mae: 0.0656 - val_loss: 0.0121 - val_mae: 0.1044\n",
      "Epoch 85/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0097 - mae: 0.0832 - val_loss: 0.0140 - val_mae: 0.1134\n",
      "Epoch 86/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.0106 - mae: 0.0838 - val_loss: 0.0114 - val_mae: 0.1010\n",
      "Epoch 87/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0078 - mae: 0.0698 - val_loss: 0.0166 - val_mae: 0.1239\n",
      "Epoch 88/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.0079 - mae: 0.0692 - val_loss: 0.0224 - val_mae: 0.1454\n",
      "Epoch 89/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 0.0099 - mae: 0.0803 - val_loss: 0.0213 - val_mae: 0.1416\n",
      "Epoch 90/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0098 - mae: 0.0811 - val_loss: 0.0149 - val_mae: 0.1170\n",
      "Epoch 91/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0071 - mae: 0.0691 - val_loss: 0.0094 - val_mae: 0.0908\n",
      "Epoch 92/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0115 - mae: 0.0922 - val_loss: 0.0193 - val_mae: 0.1345\n",
      "Epoch 93/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.0146 - mae: 0.1007 - val_loss: 0.0175 - val_mae: 0.1277\n",
      "Epoch 94/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.0107 - mae: 0.0816 - val_loss: 0.0073 - val_mae: 0.0778\n",
      "Epoch 95/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0118 - mae: 0.0871 - val_loss: 0.0217 - val_mae: 0.1433\n",
      "Epoch 96/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0082 - mae: 0.0680 - val_loss: 0.0085 - val_mae: 0.0853\n",
      "Epoch 97/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0066 - mae: 0.0702 - val_loss: 0.0187 - val_mae: 0.1323\n",
      "Epoch 98/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.0072 - mae: 0.0703 - val_loss: 0.0058 - val_mae: 0.0676\n",
      "Epoch 99/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.0099 - mae: 0.0804 - val_loss: 0.0171 - val_mae: 0.1263\n",
      "Epoch 100/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.0123 - mae: 0.0847 - val_loss: 0.0120 - val_mae: 0.1039\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
      "âœ… Done with capsicum_Shimoga_daily.csv | MAE=52.85, RMSE=59.5, R2=0.8137, MAPE=3.2%, Accuracy=96.8%\n",
      "Saved updated CSV with Date, Actual & Predicted: tat_mqa_output_csv\\capsicum_Shimoga_daily_tat_mqa_updated.csv\n",
      "ğŸš€ Processing: capsicum_Udupi_daily.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_12\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_12\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                  </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape              </span>â”ƒ<span style=\"font-weight: bold\">         Param # </span>â”ƒ<span style=\"font-weight: bold\"> Connected to               </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)             â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                          â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ multi_query_attention_12      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">17,216</span> â”‚ input_layer_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiQueryAttention</span>)         â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ add_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ multi_query_attention_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">â€¦</span> â”‚\n",
       "â”‚                               â”‚                           â”‚                 â”‚ input_layer_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization_24        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> â”‚ add_24[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]               â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)          â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_127 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> â”‚ layer_normalization_24[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]â€¦ â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_128 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)             â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> â”‚ dense_127[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ add_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ layer_normalization_24[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]â€¦ â”‚\n",
       "â”‚                               â”‚                           â”‚                 â”‚ dense_128[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization_25        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> â”‚ add_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]               â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)          â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_average_pooling1d_12   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ layer_normalization_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]â€¦ â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)      â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_129 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                 â”‚              <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> â”‚ global_average_pooling1d_â€¦ â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer_12 (\u001b[38;5;33mInputLayer\u001b[0m)   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m1\u001b[0m)             â”‚               \u001b[38;5;34m0\u001b[0m â”‚ -                          â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ multi_query_attention_12      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)            â”‚          \u001b[38;5;34m17,216\u001b[0m â”‚ input_layer_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       â”‚\n",
       "â”‚ (\u001b[38;5;33mMultiQueryAttention\u001b[0m)         â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ add_24 (\u001b[38;5;33mAdd\u001b[0m)                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)            â”‚               \u001b[38;5;34m0\u001b[0m â”‚ multi_query_attention_12[\u001b[38;5;34mâ€¦\u001b[0m â”‚\n",
       "â”‚                               â”‚                           â”‚                 â”‚ input_layer_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization_24        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)            â”‚             \u001b[38;5;34m128\u001b[0m â”‚ add_24[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]               â”‚\n",
       "â”‚ (\u001b[38;5;33mLayerNormalization\u001b[0m)          â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_127 (\u001b[38;5;33mDense\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)           â”‚           \u001b[38;5;34m8,320\u001b[0m â”‚ layer_normalization_24[\u001b[38;5;34m0\u001b[0m]â€¦ â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_128 (\u001b[38;5;33mDense\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m1\u001b[0m)             â”‚             \u001b[38;5;34m129\u001b[0m â”‚ dense_127[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ add_25 (\u001b[38;5;33mAdd\u001b[0m)                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)            â”‚               \u001b[38;5;34m0\u001b[0m â”‚ layer_normalization_24[\u001b[38;5;34m0\u001b[0m]â€¦ â”‚\n",
       "â”‚                               â”‚                           â”‚                 â”‚ dense_128[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization_25        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)            â”‚             \u001b[38;5;34m128\u001b[0m â”‚ add_25[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]               â”‚\n",
       "â”‚ (\u001b[38;5;33mLayerNormalization\u001b[0m)          â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_average_pooling1d_12   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                â”‚               \u001b[38;5;34m0\u001b[0m â”‚ layer_normalization_25[\u001b[38;5;34m0\u001b[0m]â€¦ â”‚\n",
       "â”‚ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)      â”‚                           â”‚                 â”‚                            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_129 (\u001b[38;5;33mDense\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                 â”‚              \u001b[38;5;34m65\u001b[0m â”‚ global_average_pooling1d_â€¦ â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,986</span> (101.51 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m25,986\u001b[0m (101.51 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,986</span> (101.51 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m25,986\u001b[0m (101.51 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - loss: 0.2229 - mae: 0.2700 - val_loss: 0.0230 - val_mae: 0.1110\n",
      "Epoch 2/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0116 - mae: 0.0847 - val_loss: 0.0177 - val_mae: 0.0987\n",
      "Epoch 3/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0090 - mae: 0.0728 - val_loss: 0.0156 - val_mae: 0.0944\n",
      "Epoch 4/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0069 - mae: 0.0636 - val_loss: 0.0157 - val_mae: 0.0941\n",
      "Epoch 5/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0061 - mae: 0.0590 - val_loss: 0.0153 - val_mae: 0.0927\n",
      "Epoch 6/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0050 - mae: 0.0537 - val_loss: 0.0154 - val_mae: 0.0919\n",
      "Epoch 7/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0046 - mae: 0.0508 - val_loss: 0.0158 - val_mae: 0.0948\n",
      "Epoch 8/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0040 - mae: 0.0472 - val_loss: 0.0155 - val_mae: 0.0902\n",
      "Epoch 9/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0033 - mae: 0.0418 - val_loss: 0.0154 - val_mae: 0.0904\n",
      "Epoch 10/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0029 - mae: 0.0386 - val_loss: 0.0156 - val_mae: 0.0938\n",
      "Epoch 11/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0026 - mae: 0.0364 - val_loss: 0.0165 - val_mae: 0.0899\n",
      "Epoch 12/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0025 - mae: 0.0348 - val_loss: 0.0154 - val_mae: 0.0889\n",
      "Epoch 13/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0021 - mae: 0.0309 - val_loss: 0.0161 - val_mae: 0.0903\n",
      "Epoch 14/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0022 - mae: 0.0311 - val_loss: 0.0159 - val_mae: 0.0893\n",
      "Epoch 15/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0023 - mae: 0.0328 - val_loss: 0.0195 - val_mae: 0.0996\n",
      "Epoch 16/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0024 - mae: 0.0332 - val_loss: 0.0168 - val_mae: 0.0920\n",
      "Epoch 17/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0020 - mae: 0.0278 - val_loss: 0.0157 - val_mae: 0.0901\n",
      "Epoch 18/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0019 - mae: 0.0290 - val_loss: 0.0167 - val_mae: 0.0932\n",
      "Epoch 19/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0020 - mae: 0.0286 - val_loss: 0.0154 - val_mae: 0.0908\n",
      "Epoch 20/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0020 - mae: 0.0292 - val_loss: 0.0150 - val_mae: 0.0907\n",
      "Epoch 21/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0018 - mae: 0.0277 - val_loss: 0.0145 - val_mae: 0.0940\n",
      "Epoch 22/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0018 - mae: 0.0277 - val_loss: 0.0149 - val_mae: 0.0911\n",
      "Epoch 23/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0017 - mae: 0.0277 - val_loss: 0.0188 - val_mae: 0.1008\n",
      "Epoch 24/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0016 - mae: 0.0263 - val_loss: 0.0155 - val_mae: 0.0933\n",
      "Epoch 25/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0017 - mae: 0.0275 - val_loss: 0.0173 - val_mae: 0.0964\n",
      "Epoch 26/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0016 - mae: 0.0271 - val_loss: 0.0170 - val_mae: 0.0975\n",
      "Epoch 27/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0017 - mae: 0.0273 - val_loss: 0.0154 - val_mae: 0.0934\n",
      "Epoch 28/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0016 - mae: 0.0264 - val_loss: 0.0180 - val_mae: 0.0987\n",
      "Epoch 29/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0015 - mae: 0.0255 - val_loss: 0.0171 - val_mae: 0.0971\n",
      "Epoch 30/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0019 - mae: 0.0299 - val_loss: 0.0177 - val_mae: 0.0977\n",
      "Epoch 31/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0015 - mae: 0.0250 - val_loss: 0.0151 - val_mae: 0.0936\n",
      "Epoch 32/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0017 - mae: 0.0280 - val_loss: 0.0180 - val_mae: 0.0985\n",
      "Epoch 33/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0014 - mae: 0.0253 - val_loss: 0.0192 - val_mae: 0.0994\n",
      "Epoch 34/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0015 - mae: 0.0261 - val_loss: 0.0157 - val_mae: 0.0966\n",
      "Epoch 35/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0015 - mae: 0.0252 - val_loss: 0.0174 - val_mae: 0.0968\n",
      "Epoch 36/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0014 - mae: 0.0239 - val_loss: 0.0177 - val_mae: 0.0975\n",
      "Epoch 37/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0013 - mae: 0.0239 - val_loss: 0.0161 - val_mae: 0.0973\n",
      "Epoch 38/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0014 - mae: 0.0241 - val_loss: 0.0194 - val_mae: 0.1004\n",
      "Epoch 39/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0013 - mae: 0.0234 - val_loss: 0.0173 - val_mae: 0.0970\n",
      "Epoch 40/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0014 - mae: 0.0246 - val_loss: 0.0174 - val_mae: 0.0970\n",
      "Epoch 41/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0014 - mae: 0.0245 - val_loss: 0.0231 - val_mae: 0.1084\n",
      "Epoch 42/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0015 - mae: 0.0251 - val_loss: 0.0158 - val_mae: 0.0943\n",
      "Epoch 43/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0014 - mae: 0.0257 - val_loss: 0.0155 - val_mae: 0.0957\n",
      "Epoch 44/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0015 - mae: 0.0247 - val_loss: 0.0178 - val_mae: 0.0978\n",
      "Epoch 45/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0014 - mae: 0.0239 - val_loss: 0.0174 - val_mae: 0.0970\n",
      "Epoch 46/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0015 - mae: 0.0270 - val_loss: 0.0179 - val_mae: 0.0982\n",
      "Epoch 47/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0013 - mae: 0.0227 - val_loss: 0.0215 - val_mae: 0.1050\n",
      "Epoch 48/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0014 - mae: 0.0241 - val_loss: 0.0200 - val_mae: 0.1021\n",
      "Epoch 49/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0013 - mae: 0.0231 - val_loss: 0.0177 - val_mae: 0.0975\n",
      "Epoch 50/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0015 - mae: 0.0253 - val_loss: 0.0175 - val_mae: 0.0969\n",
      "Epoch 51/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0015 - mae: 0.0259 - val_loss: 0.0204 - val_mae: 0.1025\n",
      "Epoch 52/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0014 - mae: 0.0240 - val_loss: 0.0164 - val_mae: 0.0963\n",
      "Epoch 53/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0013 - mae: 0.0237 - val_loss: 0.0229 - val_mae: 0.1073\n",
      "Epoch 54/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0014 - mae: 0.0243 - val_loss: 0.0174 - val_mae: 0.0973\n",
      "Epoch 55/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0014 - mae: 0.0247 - val_loss: 0.0201 - val_mae: 0.1017\n",
      "Epoch 56/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0014 - mae: 0.0252 - val_loss: 0.0185 - val_mae: 0.0990\n",
      "Epoch 57/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0013 - mae: 0.0239 - val_loss: 0.0220 - val_mae: 0.1062\n",
      "Epoch 58/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0014 - mae: 0.0236 - val_loss: 0.0175 - val_mae: 0.0973\n",
      "Epoch 59/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0013 - mae: 0.0230 - val_loss: 0.0180 - val_mae: 0.0978\n",
      "Epoch 60/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0014 - mae: 0.0238 - val_loss: 0.0197 - val_mae: 0.1009\n",
      "Epoch 61/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0013 - mae: 0.0238 - val_loss: 0.0201 - val_mae: 0.1026\n",
      "Epoch 62/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0013 - mae: 0.0231 - val_loss: 0.0186 - val_mae: 0.0986\n",
      "Epoch 63/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0012 - mae: 0.0227 - val_loss: 0.0167 - val_mae: 0.0952\n",
      "Epoch 64/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0014 - mae: 0.0236 - val_loss: 0.0207 - val_mae: 0.1026\n",
      "Epoch 65/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0012 - mae: 0.0218 - val_loss: 0.0182 - val_mae: 0.0985\n",
      "Epoch 66/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0012 - mae: 0.0222 - val_loss: 0.0191 - val_mae: 0.0998\n",
      "Epoch 67/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0013 - mae: 0.0227 - val_loss: 0.0244 - val_mae: 0.1119\n",
      "Epoch 68/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0014 - mae: 0.0245 - val_loss: 0.0183 - val_mae: 0.0986\n",
      "Epoch 69/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0013 - mae: 0.0234 - val_loss: 0.0210 - val_mae: 0.1031\n",
      "Epoch 70/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0013 - mae: 0.0228 - val_loss: 0.0199 - val_mae: 0.1011\n",
      "Epoch 71/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0014 - mae: 0.0243 - val_loss: 0.0177 - val_mae: 0.0973\n",
      "Epoch 72/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0015 - mae: 0.0248 - val_loss: 0.0202 - val_mae: 0.1021\n",
      "Epoch 73/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0013 - mae: 0.0235 - val_loss: 0.0191 - val_mae: 0.0992\n",
      "Epoch 74/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0013 - mae: 0.0244 - val_loss: 0.0205 - val_mae: 0.1023\n",
      "Epoch 75/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0014 - mae: 0.0231 - val_loss: 0.0172 - val_mae: 0.0960\n",
      "Epoch 76/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0013 - mae: 0.0226 - val_loss: 0.0208 - val_mae: 0.1029\n",
      "Epoch 77/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0013 - mae: 0.0226 - val_loss: 0.0204 - val_mae: 0.1031\n",
      "Epoch 78/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0014 - mae: 0.0249 - val_loss: 0.0188 - val_mae: 0.0993\n",
      "Epoch 79/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0013 - mae: 0.0227 - val_loss: 0.0207 - val_mae: 0.1023\n",
      "Epoch 80/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0013 - mae: 0.0223 - val_loss: 0.0179 - val_mae: 0.0973\n",
      "Epoch 81/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0012 - mae: 0.0212 - val_loss: 0.0203 - val_mae: 0.1014\n",
      "Epoch 82/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0012 - mae: 0.0217 - val_loss: 0.0200 - val_mae: 0.1017\n",
      "Epoch 83/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0014 - mae: 0.0256 - val_loss: 0.0212 - val_mae: 0.1033\n",
      "Epoch 84/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0011 - mae: 0.0213 - val_loss: 0.0219 - val_mae: 0.1046\n",
      "Epoch 85/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0011 - mae: 0.0217 - val_loss: 0.0217 - val_mae: 0.1040\n",
      "Epoch 86/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0013 - mae: 0.0232 - val_loss: 0.0242 - val_mae: 0.1096\n",
      "Epoch 87/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0014 - mae: 0.0250 - val_loss: 0.0177 - val_mae: 0.0966\n",
      "Epoch 88/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0014 - mae: 0.0243 - val_loss: 0.0246 - val_mae: 0.1085\n",
      "Epoch 89/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0012 - mae: 0.0230 - val_loss: 0.0192 - val_mae: 0.0995\n",
      "Epoch 90/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0013 - mae: 0.0230 - val_loss: 0.0260 - val_mae: 0.1115\n",
      "Epoch 91/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0014 - mae: 0.0231 - val_loss: 0.0188 - val_mae: 0.0994\n",
      "Epoch 92/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0013 - mae: 0.0222 - val_loss: 0.0178 - val_mae: 0.0976\n",
      "Epoch 93/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0012 - mae: 0.0216 - val_loss: 0.0177 - val_mae: 0.0969\n",
      "Epoch 94/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0013 - mae: 0.0225 - val_loss: 0.0197 - val_mae: 0.1005\n",
      "Epoch 95/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0014 - mae: 0.0238 - val_loss: 0.0198 - val_mae: 0.1001\n",
      "Epoch 96/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0013 - mae: 0.0223 - val_loss: 0.0213 - val_mae: 0.1028\n",
      "Epoch 97/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0012 - mae: 0.0216 - val_loss: 0.0206 - val_mae: 0.1021\n",
      "Epoch 98/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0012 - mae: 0.0227 - val_loss: 0.0234 - val_mae: 0.1065\n",
      "Epoch 99/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0012 - mae: 0.0211 - val_loss: 0.0192 - val_mae: 0.0992\n",
      "Epoch 100/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0013 - mae: 0.0221 - val_loss: 0.0202 - val_mae: 0.1011\n",
      "\u001b[1m155/155\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step\n",
      "âœ… Done with capsicum_Udupi_daily.csv | MAE=358.09, RMSE=636.46, R2=0.8189, MAPE=9.17%, Accuracy=90.83%\n",
      "Saved updated CSV with Date, Actual & Predicted: tat_mqa_output_csv\\capsicum_Udupi_daily_tat_mqa_updated.csv\n",
      "ğŸ“Š Metrics saved to tat_mqa_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "import joblib  # for .pkl model saving (best-effort)\n",
    "warnings_imported = False\n",
    "try:\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    warnings_imported = True\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# -----------------------------\n",
    "# Output directories\n",
    "# -----------------------------\n",
    "input_folder = \"dataset\"\n",
    "output_models = \"tat_mqa_output_models\"\n",
    "output_csv = \"tat_mqa_output_csv\"\n",
    "output_graphs = \"tat_mqa_output_graphs\"\n",
    "output_logs = \"tat_mqa_output_logs\"\n",
    "metrics_file = \"tat_mqa_metrics.csv\"\n",
    "\n",
    "os.makedirs(output_models, exist_ok=True)\n",
    "os.makedirs(output_csv, exist_ok=True)\n",
    "os.makedirs(output_graphs, exist_ok=True)\n",
    "os.makedirs(output_logs, exist_ok=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Function to create dataset\n",
    "# -----------------------------\n",
    "def create_dataset(data, look_back=30):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - look_back):\n",
    "        X.append(data[i:i+look_back, 0])\n",
    "        y.append(data[i+look_back, 0])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# -----------------------------\n",
    "# Multi-Query Attention Layer\n",
    "# -----------------------------\n",
    "class MultiQueryAttention(layers.Layer):\n",
    "    def __init__(self, num_heads, key_dim, dropout_rate=0.1, **kwargs):\n",
    "        super(MultiQueryAttention, self).__init__(**kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.key_dim = key_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.q_dense = [layers.Dense(key_dim) for _ in range(num_heads)]\n",
    "        self.k_dense = layers.Dense(key_dim)\n",
    "        self.v_dense = layers.Dense(key_dim)\n",
    "        self.dropout = layers.Dropout(dropout_rate)\n",
    "        self.output_dense = layers.Dense(key_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        K = self.k_dense(x)\n",
    "        V = self.v_dense(x)\n",
    "        head_outputs = []\n",
    "\n",
    "        for q_layer in self.q_dense:\n",
    "            Q = q_layer(x)\n",
    "            scores = tf.matmul(Q, K, transpose_b=True) / tf.math.sqrt(tf.cast(self.key_dim, tf.float32))\n",
    "            attention_weights = tf.nn.softmax(scores, axis=-1)\n",
    "            attention_output = tf.matmul(attention_weights, V)\n",
    "            head_outputs.append(attention_output)\n",
    "\n",
    "        concat = tf.concat(head_outputs, axis=-1)\n",
    "        output = self.output_dense(concat)\n",
    "        output = self.dropout(output)\n",
    "        return output\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(MultiQueryAttention, self).get_config()\n",
    "        config.update({\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"key_dim\": self.key_dim,\n",
    "            \"dropout_rate\": self.dropout_rate\n",
    "        })\n",
    "        return config\n",
    "\n",
    "# -----------------------------\n",
    "# TAT-MQA Model\n",
    "# -----------------------------\n",
    "def build_tat_mqa_model(input_shape, d_model=64, num_heads=4, ff_dim=128, dropout_rate=0.2):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = MultiQueryAttention(num_heads=num_heads, key_dim=d_model, dropout_rate=dropout_rate)(inputs)\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x + inputs)\n",
    "\n",
    "    ff = layers.Dense(ff_dim, activation='relu')(x)\n",
    "    ff = layers.Dense(input_shape[1])(ff)\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x + ff)\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    outputs = layers.Dense(1)(x)\n",
    "\n",
    "    model = models.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "                  loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# -----------------------------\n",
    "# Function to robustly parse dates\n",
    "# -----------------------------\n",
    "def parse_dates_safe(date_series):\n",
    "    # keep same behavior as you used before: infer_datetime_format True fallback\n",
    "    try:\n",
    "        return pd.to_datetime(date_series, infer_datetime_format=True, errors='coerce')\n",
    "    except:\n",
    "        return pd.to_datetime(date_series, errors='coerce')\n",
    "\n",
    "# -----------------------------\n",
    "# Metrics storage\n",
    "# -----------------------------\n",
    "metrics_list = []\n",
    "\n",
    "# -----------------------------\n",
    "# Process each CSV file\n",
    "# -----------------------------\n",
    "look_back = 30\n",
    "for file in os.listdir(input_folder):\n",
    "    if not file.endswith(\".csv\"):\n",
    "        continue\n",
    "\n",
    "    file_path = os.path.join(input_folder, file)\n",
    "    print(f\"ğŸš€ Processing: {file}\")\n",
    "\n",
    "    # Load CSV\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Robust date parsing\n",
    "    df['Date'] = parse_dates_safe(df['Date'])\n",
    "    df = df.dropna(subset=['Date'])\n",
    "    df = df.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Handle missing Average Price\n",
    "    # -----------------------------\n",
    "    if 'Average Price' not in df.columns:\n",
    "        raise KeyError(\"Input CSV must contain 'Average Price' column.\")\n",
    "    if df['Average Price'].isna().sum() > 0:\n",
    "        mean_val = df['Average Price'].mean()\n",
    "        df['Average Price'].fillna(mean_val, inplace=True)\n",
    "        print(f\"Filled {df['Average Price'].isna().sum()} missing Average Price values with mean {mean_val:.2f}\")\n",
    "\n",
    "    # Round Actual values early\n",
    "    df['Average Price'] = df['Average Price'].astype(float).round(2)\n",
    "\n",
    "    # Moving averages (fill remaining NaNs with column mean to avoid plotting gaps)\n",
    "    df['MA_7'] = df['Average Price'].rolling(window=7).mean()\n",
    "    df['MA_30'] = df['Average Price'].rolling(window=30).mean()\n",
    "    df['MA_7'].fillna(df['MA_7'].mean(), inplace=True)\n",
    "    df['MA_30'].fillna(df['MA_30'].mean(), inplace=True)\n",
    "\n",
    "    # Prepare data\n",
    "    values = df[['Average Price']].values.astype('float32')\n",
    "    if len(values) <= look_back:\n",
    "        # Not enough data for the chosen look_back; skip file with a warning\n",
    "        print(f\"âš ï¸ Skipping {file} â€” dataset length ({len(values)}) <= look_back ({look_back})\")\n",
    "        continue\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled_values = scaler.fit_transform(values)\n",
    "\n",
    "    X, y = create_dataset(scaled_values, look_back)\n",
    "    X = np.reshape(X, (X.shape[0], X.shape[1], 1))\n",
    "\n",
    "    # Build and train model\n",
    "    model = build_tat_mqa_model(input_shape=(look_back,1))\n",
    "    model.summary()\n",
    "\n",
    "    history = model.fit(X, y, epochs=100, batch_size=16, validation_split=0.2, verbose=1)\n",
    "\n",
    "    # Save training logs\n",
    "    log_file = os.path.join(output_logs, file.replace(\".csv\", \"_tat_mqa_training.txt\"))\n",
    "    with open(log_file, \"w\") as f:\n",
    "        f.write(\"Training Loss per Epoch:\\n\")\n",
    "        for i, loss in enumerate(history.history['loss']):\n",
    "            val_loss = history.history['val_loss'][i] if 'val_loss' in history.history else None\n",
    "            f.write(f\"Epoch {i+1}: Loss={loss}, Val_Loss={val_loss}\\n\")\n",
    "\n",
    "    # Predictions (rescale)\n",
    "    predictions = model.predict(X)\n",
    "    predictions_rescaled = scaler.inverse_transform(predictions).flatten()\n",
    "    # pad with NaNs at start so predicted series aligns with original df index\n",
    "    padded_preds = np.concatenate([np.full(look_back, np.nan), predictions_rescaled])\n",
    "    df['Predicted'] = np.round(padded_preds, 2)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Compute metrics using only the matched portion (exclude leading NaNs)\n",
    "    # -----------------------------\n",
    "    y_true = df['Average Price'].values[look_back:]\n",
    "    y_pred = predictions_rescaled\n",
    "    mae = round(mean_absolute_error(y_true, y_pred), 2)\n",
    "    rmse = round(np.sqrt(mean_squared_error(y_true, y_pred)), 2)\n",
    "    r2 = round(r2_score(y_true, y_pred), 4)\n",
    "    # MAPE: avoid divide-by-zero by masking zeros\n",
    "    mask = y_true != 0\n",
    "    if mask.sum() == 0:\n",
    "        mape = np.nan\n",
    "        accuracy = np.nan\n",
    "    else:\n",
    "        mape = round(np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100, 2)\n",
    "        accuracy = round(100 - mape, 2)\n",
    "\n",
    "    metrics_list.append([file.replace(\".csv\",\"\"), mae, rmse, r2, mape, accuracy])\n",
    "\n",
    "    print(f\"âœ… Done with {file} | MAE={mae}, RMSE={rmse}, R2={r2}, MAPE={mape}%, Accuracy={accuracy}%\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # Save model: native Keras and attempt .pkl (best-effort)\n",
    "    # -----------------------------\n",
    "    keras_model_path = os.path.join(output_models, file.replace(\".csv\", \"_tat_mqa_model.keras\"))\n",
    "    model.save(keras_model_path)\n",
    "    pkl_model_path = os.path.join(output_models, file.replace(\".csv\", \"_tat_mqa_model.pkl\"))\n",
    "    try:\n",
    "        # joblib.dump the Keras model object (may fail on some TF versions)\n",
    "        joblib.dump(model, pkl_model_path)\n",
    "    except Exception as e:\n",
    "        # fallback: save model architecture + weights dict to pickle (best-effort)\n",
    "        try:\n",
    "            model_info = {\n",
    "                \"config\": model.get_config(),\n",
    "                \"weights\": model.get_weights()\n",
    "            }\n",
    "            joblib.dump(model_info, pkl_model_path)\n",
    "        except Exception as e2:\n",
    "            print(f\"âš ï¸ Could not save .pkl for {file}: {e}; {e2}\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # Save CSV with only Date, Actual, Predicted (Actual renamed)\n",
    "    # -----------------------------\n",
    "    save_df = pd.DataFrame({\n",
    "        'Date': df['Date'],\n",
    "        'Actual': df['Average Price'].round(2),\n",
    "        'Predicted': df['Predicted']\n",
    "    })\n",
    "    updated_csv_path = os.path.join(output_csv, file.replace(\".csv\", \"_tat_mqa_updated.csv\"))\n",
    "    save_df.to_csv(updated_csv_path, index=False)\n",
    "    print(f\"Saved updated CSV with Date, Actual & Predicted: {updated_csv_path}\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # Save graph with Actual, Predicted, MA7, MA30\n",
    "    # -----------------------------\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.plot(df['Date'], df['Average Price'], label='Actual', color='blue')\n",
    "    plt.plot(df['Date'], df['Predicted'], label='Predicted', color='red', linestyle='dashed')\n",
    "    plt.plot(df['Date'], df['MA_7'], label='MA 7', color='orange')\n",
    "    plt.plot(df['Date'], df['MA_30'], label='MA 30', color='green')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Average Price')\n",
    "    plt.title(f'Price Prediction (TAT-MQA) - {file}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    graph_file = os.path.join(output_graphs, file.replace(\".csv\", \"_tat_mqa_graph.png\"))\n",
    "    plt.savefig(graph_file, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# -----------------------------\n",
    "# Save all metrics\n",
    "# -----------------------------\n",
    "metrics_df = pd.DataFrame(metrics_list, columns=['District', 'MAE', 'RMSE', 'R2', 'MAPE(%)', 'Accuracy(%)'])\n",
    "metrics_df.to_csv(metrics_file, index=False)\n",
    "print(f\"ğŸ“Š Metrics saved to {metrics_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0ae462-9789-44a2-8e36-afa90ad4d586",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TAT+GQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a09c991-2bf8-4332-a4d4-424724dcbba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Processing: capsicum_Bangalore_daily.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\2878872630.py:115: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Average Price'].fillna(df['Average Price'].mean(), inplace=True)\n",
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\2878872630.py:120: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['MA_7'].fillna(df['MA_7'].mean(), inplace=True)\n",
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\2878872630.py:121: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['MA_30'].fillna(df['MA_30'].mean(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.1247 - mae: 0.1988 - val_loss: 0.0125 - val_mae: 0.0794\n",
      "Epoch 2/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0106 - mae: 0.0749 - val_loss: 0.0118 - val_mae: 0.0792\n",
      "Epoch 3/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0079 - mae: 0.0642 - val_loss: 0.0118 - val_mae: 0.0832\n",
      "Epoch 4/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0071 - mae: 0.0610 - val_loss: 0.0159 - val_mae: 0.1026\n",
      "Epoch 5/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0064 - mae: 0.0568 - val_loss: 0.0117 - val_mae: 0.0813\n",
      "Epoch 6/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0067 - mae: 0.0580 - val_loss: 0.0128 - val_mae: 0.0890\n",
      "Epoch 7/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0066 - mae: 0.0566 - val_loss: 0.0118 - val_mae: 0.0826\n",
      "Epoch 8/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0066 - mae: 0.0574 - val_loss: 0.0120 - val_mae: 0.0845\n",
      "Epoch 9/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0062 - mae: 0.0553 - val_loss: 0.0124 - val_mae: 0.0869\n",
      "Epoch 10/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0058 - mae: 0.0540 - val_loss: 0.0117 - val_mae: 0.0823\n",
      "Epoch 11/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0061 - mae: 0.0548 - val_loss: 0.0120 - val_mae: 0.0852\n",
      "Epoch 12/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0057 - mae: 0.0526 - val_loss: 0.0156 - val_mae: 0.1015\n",
      "Epoch 13/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0062 - mae: 0.0556 - val_loss: 0.0118 - val_mae: 0.0826\n",
      "Epoch 14/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0060 - mae: 0.0542 - val_loss: 0.0118 - val_mae: 0.0832\n",
      "Epoch 15/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0060 - mae: 0.0548 - val_loss: 0.0129 - val_mae: 0.0900\n",
      "Epoch 16/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0061 - mae: 0.0539 - val_loss: 0.0119 - val_mae: 0.0841\n",
      "Epoch 17/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0063 - mae: 0.0544 - val_loss: 0.0122 - val_mae: 0.0862\n",
      "Epoch 18/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0059 - mae: 0.0527 - val_loss: 0.0149 - val_mae: 0.0988\n",
      "Epoch 19/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0062 - mae: 0.0545 - val_loss: 0.0135 - val_mae: 0.0920\n",
      "Epoch 20/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0062 - mae: 0.0549 - val_loss: 0.0117 - val_mae: 0.0823\n",
      "Epoch 21/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0059 - mae: 0.0532 - val_loss: 0.0127 - val_mae: 0.0891\n",
      "Epoch 22/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0057 - mae: 0.0520 - val_loss: 0.0139 - val_mae: 0.0942\n",
      "Epoch 23/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0060 - mae: 0.0530 - val_loss: 0.0122 - val_mae: 0.0847\n",
      "Epoch 24/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0060 - mae: 0.0526 - val_loss: 0.0117 - val_mae: 0.0827\n",
      "Epoch 25/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0057 - mae: 0.0520 - val_loss: 0.0128 - val_mae: 0.0892\n",
      "Epoch 26/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0059 - mae: 0.0533 - val_loss: 0.0132 - val_mae: 0.0908\n",
      "Epoch 27/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0058 - mae: 0.0526 - val_loss: 0.0122 - val_mae: 0.0846\n",
      "Epoch 28/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0060 - mae: 0.0531 - val_loss: 0.0122 - val_mae: 0.0862\n",
      "Epoch 29/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0059 - mae: 0.0528 - val_loss: 0.0134 - val_mae: 0.0918\n",
      "Epoch 30/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0057 - mae: 0.0522 - val_loss: 0.0119 - val_mae: 0.0840\n",
      "Epoch 31/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0061 - mae: 0.0539 - val_loss: 0.0124 - val_mae: 0.0874\n",
      "Epoch 32/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0057 - mae: 0.0518 - val_loss: 0.0136 - val_mae: 0.0928\n",
      "Epoch 33/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0057 - mae: 0.0518 - val_loss: 0.0129 - val_mae: 0.0889\n",
      "Epoch 34/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0062 - mae: 0.0530 - val_loss: 0.0117 - val_mae: 0.0822\n",
      "Epoch 35/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0056 - mae: 0.0514 - val_loss: 0.0117 - val_mae: 0.0820\n",
      "Epoch 36/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0061 - mae: 0.0541 - val_loss: 0.0137 - val_mae: 0.0939\n",
      "Epoch 37/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0058 - mae: 0.0524 - val_loss: 0.0123 - val_mae: 0.0865\n",
      "Epoch 38/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0058 - mae: 0.0514 - val_loss: 0.0122 - val_mae: 0.0866\n",
      "Epoch 39/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0060 - mae: 0.0528 - val_loss: 0.0136 - val_mae: 0.0932\n",
      "Epoch 40/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0059 - mae: 0.0525 - val_loss: 0.0142 - val_mae: 0.0942\n",
      "Epoch 41/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0059 - mae: 0.0521 - val_loss: 0.0138 - val_mae: 0.0937\n",
      "Epoch 42/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0059 - mae: 0.0521 - val_loss: 0.0125 - val_mae: 0.0857\n",
      "Epoch 43/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0058 - mae: 0.0522 - val_loss: 0.0140 - val_mae: 0.0923\n",
      "Epoch 44/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0058 - mae: 0.0523 - val_loss: 0.0132 - val_mae: 0.0898\n",
      "Epoch 45/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0057 - mae: 0.0512 - val_loss: 0.0123 - val_mae: 0.0852\n",
      "Epoch 46/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0059 - mae: 0.0528 - val_loss: 0.0137 - val_mae: 0.0923\n",
      "Epoch 47/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0059 - mae: 0.0523 - val_loss: 0.0157 - val_mae: 0.1008\n",
      "Epoch 48/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0058 - mae: 0.0517 - val_loss: 0.0155 - val_mae: 0.0989\n",
      "Epoch 49/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0059 - mae: 0.0517 - val_loss: 0.0152 - val_mae: 0.0992\n",
      "Epoch 50/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0057 - mae: 0.0515 - val_loss: 0.0139 - val_mae: 0.0936\n",
      "Epoch 51/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0055 - mae: 0.0514 - val_loss: 0.0125 - val_mae: 0.0878\n",
      "Epoch 52/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0060 - mae: 0.0526 - val_loss: 0.0121 - val_mae: 0.0855\n",
      "Epoch 53/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0059 - mae: 0.0526 - val_loss: 0.0127 - val_mae: 0.0888\n",
      "Epoch 54/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0055 - mae: 0.0509 - val_loss: 0.0123 - val_mae: 0.0859\n",
      "Epoch 55/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0058 - mae: 0.0518 - val_loss: 0.0128 - val_mae: 0.0884\n",
      "Epoch 56/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0058 - mae: 0.0524 - val_loss: 0.0149 - val_mae: 0.0975\n",
      "Epoch 57/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0056 - mae: 0.0515 - val_loss: 0.0135 - val_mae: 0.0927\n",
      "Epoch 58/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0057 - mae: 0.0518 - val_loss: 0.0123 - val_mae: 0.0865\n",
      "Epoch 59/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0056 - mae: 0.0511 - val_loss: 0.0118 - val_mae: 0.0827\n",
      "Epoch 60/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0056 - mae: 0.0512 - val_loss: 0.0130 - val_mae: 0.0891\n",
      "Epoch 61/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0057 - mae: 0.0516 - val_loss: 0.0119 - val_mae: 0.0841\n",
      "Epoch 62/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0061 - mae: 0.0533 - val_loss: 0.0116 - val_mae: 0.0818\n",
      "Epoch 63/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0058 - mae: 0.0518 - val_loss: 0.0126 - val_mae: 0.0867\n",
      "Epoch 64/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0058 - mae: 0.0513 - val_loss: 0.0123 - val_mae: 0.0864\n",
      "Epoch 65/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0057 - mae: 0.0509 - val_loss: 0.0120 - val_mae: 0.0846\n",
      "Epoch 66/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0058 - mae: 0.0520 - val_loss: 0.0126 - val_mae: 0.0877\n",
      "Epoch 67/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0059 - mae: 0.0519 - val_loss: 0.0130 - val_mae: 0.0893\n",
      "Epoch 68/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0059 - mae: 0.0522 - val_loss: 0.0130 - val_mae: 0.0905\n",
      "Epoch 69/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0058 - mae: 0.0515 - val_loss: 0.0121 - val_mae: 0.0855\n",
      "Epoch 70/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0054 - mae: 0.0504 - val_loss: 0.0172 - val_mae: 0.1055\n",
      "Epoch 71/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0056 - mae: 0.0509 - val_loss: 0.0132 - val_mae: 0.0903\n",
      "Epoch 72/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0057 - mae: 0.0521 - val_loss: 0.0121 - val_mae: 0.0846\n",
      "Epoch 73/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0058 - mae: 0.0517 - val_loss: 0.0124 - val_mae: 0.0861\n",
      "Epoch 74/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0058 - mae: 0.0519 - val_loss: 0.0117 - val_mae: 0.0808\n",
      "Epoch 75/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0056 - mae: 0.0512 - val_loss: 0.0140 - val_mae: 0.0946\n",
      "Epoch 76/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0059 - mae: 0.0520 - val_loss: 0.0131 - val_mae: 0.0895\n",
      "Epoch 77/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0057 - mae: 0.0514 - val_loss: 0.0145 - val_mae: 0.0952\n",
      "Epoch 78/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0058 - mae: 0.0520 - val_loss: 0.0125 - val_mae: 0.0877\n",
      "Epoch 79/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0057 - mae: 0.0521 - val_loss: 0.0118 - val_mae: 0.0834\n",
      "Epoch 80/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0058 - mae: 0.0514 - val_loss: 0.0139 - val_mae: 0.0935\n",
      "Epoch 81/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0056 - mae: 0.0515 - val_loss: 0.0141 - val_mae: 0.0943\n",
      "Epoch 82/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0055 - mae: 0.0511 - val_loss: 0.0118 - val_mae: 0.0836\n",
      "Epoch 83/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0058 - mae: 0.0513 - val_loss: 0.0134 - val_mae: 0.0910\n",
      "Epoch 84/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0059 - mae: 0.0524 - val_loss: 0.0123 - val_mae: 0.0847\n",
      "Epoch 85/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0057 - mae: 0.0516 - val_loss: 0.0119 - val_mae: 0.0834\n",
      "Epoch 86/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0057 - mae: 0.0514 - val_loss: 0.0119 - val_mae: 0.0845\n",
      "Epoch 87/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0055 - mae: 0.0502 - val_loss: 0.0130 - val_mae: 0.0897\n",
      "Epoch 88/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0056 - mae: 0.0509 - val_loss: 0.0117 - val_mae: 0.0824\n",
      "Epoch 89/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0054 - mae: 0.0502 - val_loss: 0.0119 - val_mae: 0.0834\n",
      "Epoch 90/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0058 - mae: 0.0516 - val_loss: 0.0144 - val_mae: 0.0957\n",
      "Epoch 91/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0059 - mae: 0.0526 - val_loss: 0.0131 - val_mae: 0.0892\n",
      "Epoch 92/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0056 - mae: 0.0512 - val_loss: 0.0125 - val_mae: 0.0868\n",
      "Epoch 93/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0053 - mae: 0.0497 - val_loss: 0.0117 - val_mae: 0.0817\n",
      "Epoch 94/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0058 - mae: 0.0516 - val_loss: 0.0120 - val_mae: 0.0845\n",
      "Epoch 95/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0056 - mae: 0.0510 - val_loss: 0.0129 - val_mae: 0.0892\n",
      "Epoch 96/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0059 - mae: 0.0518 - val_loss: 0.0123 - val_mae: 0.0855\n",
      "Epoch 97/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0055 - mae: 0.0504 - val_loss: 0.0124 - val_mae: 0.0860\n",
      "Epoch 98/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0056 - mae: 0.0510 - val_loss: 0.0126 - val_mae: 0.0871\n",
      "Epoch 99/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0058 - mae: 0.0515 - val_loss: 0.0120 - val_mae: 0.0836\n",
      "Epoch 100/100\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 0.0057 - mae: 0.0511 - val_loss: 0.0122 - val_mae: 0.0850\n",
      "\u001b[1m290/290\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "âœ… Done with capsicum_Bangalore_daily.csv | MAE=557.61, RMSE=804.61, R2=0.59, MAPE=18.17%, Accuracy=81.83%\n",
      "\n",
      "ğŸš€ Processing: capsicum_Belgaum_daily.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\2878872630.py:115: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Average Price'].fillna(df['Average Price'].mean(), inplace=True)\n",
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\2878872630.py:120: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['MA_7'].fillna(df['MA_7'].mean(), inplace=True)\n",
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\2878872630.py:121: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['MA_30'].fillna(df['MA_30'].mean(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - loss: 0.8333 - mae: 0.6508 - val_loss: 0.0039 - val_mae: 0.0449\n",
      "Epoch 2/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0171 - mae: 0.0992 - val_loss: 0.0041 - val_mae: 0.0584\n",
      "Epoch 3/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0154 - mae: 0.0932 - val_loss: 0.0027 - val_mae: 0.0397\n",
      "Epoch 4/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0152 - mae: 0.0926 - val_loss: 0.0034 - val_mae: 0.0382\n",
      "Epoch 5/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0149 - mae: 0.0902 - val_loss: 0.0026 - val_mae: 0.0428\n",
      "Epoch 6/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0146 - mae: 0.0883 - val_loss: 0.0094 - val_mae: 0.0885\n",
      "Epoch 7/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0148 - mae: 0.0925 - val_loss: 0.0035 - val_mae: 0.0395\n",
      "Epoch 8/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0152 - mae: 0.0885 - val_loss: 0.0027 - val_mae: 0.0394\n",
      "Epoch 9/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0108 - mae: 0.0761 - val_loss: 0.0027 - val_mae: 0.0462\n",
      "Epoch 10/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0126 - mae: 0.0785 - val_loss: 0.0028 - val_mae: 0.0459\n",
      "Epoch 11/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0104 - mae: 0.0734 - val_loss: 0.0046 - val_mae: 0.0618\n",
      "Epoch 12/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0121 - mae: 0.0804 - val_loss: 0.0132 - val_mae: 0.1052\n",
      "Epoch 13/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0112 - mae: 0.0784 - val_loss: 0.0031 - val_mae: 0.0372\n",
      "Epoch 14/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0102 - mae: 0.0715 - val_loss: 0.0043 - val_mae: 0.0452\n",
      "Epoch 15/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0107 - mae: 0.0738 - val_loss: 0.0026 - val_mae: 0.0400\n",
      "Epoch 16/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0114 - mae: 0.0759 - val_loss: 0.0026 - val_mae: 0.0392\n",
      "Epoch 17/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0091 - mae: 0.0661 - val_loss: 0.0030 - val_mae: 0.0490\n",
      "Epoch 18/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0098 - mae: 0.0702 - val_loss: 0.0026 - val_mae: 0.0417\n",
      "Epoch 19/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0104 - mae: 0.0686 - val_loss: 0.0036 - val_mae: 0.0544\n",
      "Epoch 20/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0103 - mae: 0.0655 - val_loss: 0.0027 - val_mae: 0.0444\n",
      "Epoch 21/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0102 - mae: 0.0692 - val_loss: 0.0034 - val_mae: 0.0384\n",
      "Epoch 22/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0100 - mae: 0.0724 - val_loss: 0.0028 - val_mae: 0.0395\n",
      "Epoch 23/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0091 - mae: 0.0643 - val_loss: 0.0027 - val_mae: 0.0447\n",
      "Epoch 24/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0083 - mae: 0.0625 - val_loss: 0.0049 - val_mae: 0.0501\n",
      "Epoch 25/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0096 - mae: 0.0687 - val_loss: 0.0040 - val_mae: 0.0423\n",
      "Epoch 26/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0083 - mae: 0.0636 - val_loss: 0.0029 - val_mae: 0.0377\n",
      "Epoch 27/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0097 - mae: 0.0634 - val_loss: 0.0027 - val_mae: 0.0409\n",
      "Epoch 28/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0077 - mae: 0.0591 - val_loss: 0.0027 - val_mae: 0.0390\n",
      "Epoch 29/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0080 - mae: 0.0606 - val_loss: 0.0027 - val_mae: 0.0451\n",
      "Epoch 30/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0084 - mae: 0.0595 - val_loss: 0.0030 - val_mae: 0.0490\n",
      "Epoch 31/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0109 - mae: 0.0736 - val_loss: 0.0030 - val_mae: 0.0375\n",
      "Epoch 32/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0077 - mae: 0.0577 - val_loss: 0.0046 - val_mae: 0.0479\n",
      "Epoch 33/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0097 - mae: 0.0661 - val_loss: 0.0030 - val_mae: 0.0375\n",
      "Epoch 34/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0097 - mae: 0.0650 - val_loss: 0.0029 - val_mae: 0.0478\n",
      "Epoch 35/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0102 - mae: 0.0656 - val_loss: 0.0026 - val_mae: 0.0407\n",
      "Epoch 36/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0084 - mae: 0.0592 - val_loss: 0.0041 - val_mae: 0.0430\n",
      "Epoch 37/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0085 - mae: 0.0586 - val_loss: 0.0030 - val_mae: 0.0381\n",
      "Epoch 38/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0090 - mae: 0.0624 - val_loss: 0.0030 - val_mae: 0.0375\n",
      "Epoch 39/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0078 - mae: 0.0610 - val_loss: 0.0026 - val_mae: 0.0407\n",
      "Epoch 40/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0087 - mae: 0.0637 - val_loss: 0.0046 - val_mae: 0.0472\n",
      "Epoch 41/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0101 - mae: 0.0645 - val_loss: 0.0027 - val_mae: 0.0427\n",
      "Epoch 42/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0084 - mae: 0.0598 - val_loss: 0.0030 - val_mae: 0.0375\n",
      "Epoch 43/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0079 - mae: 0.0583 - val_loss: 0.0032 - val_mae: 0.0377\n",
      "Epoch 44/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0091 - mae: 0.0621 - val_loss: 0.0052 - val_mae: 0.0527\n",
      "Epoch 45/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0098 - mae: 0.0688 - val_loss: 0.0031 - val_mae: 0.0375\n",
      "Epoch 46/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0091 - mae: 0.0665 - val_loss: 0.0028 - val_mae: 0.0462\n",
      "Epoch 47/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0095 - mae: 0.0645 - val_loss: 0.0029 - val_mae: 0.0465\n",
      "Epoch 48/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0100 - mae: 0.0617 - val_loss: 0.0037 - val_mae: 0.0400\n",
      "Epoch 49/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0085 - mae: 0.0615 - val_loss: 0.0027 - val_mae: 0.0389\n",
      "Epoch 50/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0088 - mae: 0.0597 - val_loss: 0.0026 - val_mae: 0.0402\n",
      "Epoch 51/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0079 - mae: 0.0620 - val_loss: 0.0027 - val_mae: 0.0408\n",
      "Epoch 52/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0062 - mae: 0.0533 - val_loss: 0.0036 - val_mae: 0.0396\n",
      "Epoch 53/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0082 - mae: 0.0578 - val_loss: 0.0047 - val_mae: 0.0480\n",
      "Epoch 54/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0088 - mae: 0.0638 - val_loss: 0.0026 - val_mae: 0.0408\n",
      "Epoch 55/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0066 - mae: 0.0558 - val_loss: 0.0033 - val_mae: 0.0382\n",
      "Epoch 56/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0103 - mae: 0.0682 - val_loss: 0.0037 - val_mae: 0.0404\n",
      "Epoch 57/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0080 - mae: 0.0605 - val_loss: 0.0055 - val_mae: 0.0554\n",
      "Epoch 58/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0081 - mae: 0.0602 - val_loss: 0.0062 - val_mae: 0.0612\n",
      "Epoch 59/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0097 - mae: 0.0660 - val_loss: 0.0061 - val_mae: 0.0592\n",
      "Epoch 60/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0084 - mae: 0.0607 - val_loss: 0.0028 - val_mae: 0.0398\n",
      "Epoch 61/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0100 - mae: 0.0617 - val_loss: 0.0027 - val_mae: 0.0418\n",
      "Epoch 62/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0075 - mae: 0.0567 - val_loss: 0.0029 - val_mae: 0.0478\n",
      "Epoch 63/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0083 - mae: 0.0585 - val_loss: 0.0027 - val_mae: 0.0415\n",
      "Epoch 64/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0112 - mae: 0.0710 - val_loss: 0.0029 - val_mae: 0.0481\n",
      "Epoch 65/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0093 - mae: 0.0607 - val_loss: 0.0026 - val_mae: 0.0424\n",
      "Epoch 66/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0085 - mae: 0.0624 - val_loss: 0.0038 - val_mae: 0.0558\n",
      "Epoch 67/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0084 - mae: 0.0658 - val_loss: 0.0054 - val_mae: 0.0542\n",
      "Epoch 68/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0082 - mae: 0.0605 - val_loss: 0.0048 - val_mae: 0.0495\n",
      "Epoch 69/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0089 - mae: 0.0644 - val_loss: 0.0041 - val_mae: 0.0435\n",
      "Epoch 70/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0081 - mae: 0.0582 - val_loss: 0.0064 - val_mae: 0.0623\n",
      "Epoch 71/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0096 - mae: 0.0648 - val_loss: 0.0031 - val_mae: 0.0376\n",
      "Epoch 72/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0075 - mae: 0.0606 - val_loss: 0.0037 - val_mae: 0.0400\n",
      "Epoch 73/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0086 - mae: 0.0648 - val_loss: 0.0028 - val_mae: 0.0468\n",
      "Epoch 74/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0075 - mae: 0.0577 - val_loss: 0.0048 - val_mae: 0.0495\n",
      "Epoch 75/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0076 - mae: 0.0591 - val_loss: 0.0027 - val_mae: 0.0387\n",
      "Epoch 76/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0087 - mae: 0.0614 - val_loss: 0.0031 - val_mae: 0.0376\n",
      "Epoch 77/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0066 - mae: 0.0537 - val_loss: 0.0027 - val_mae: 0.0439\n",
      "Epoch 78/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0080 - mae: 0.0578 - val_loss: 0.0028 - val_mae: 0.0399\n",
      "Epoch 79/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0084 - mae: 0.0584 - val_loss: 0.0031 - val_mae: 0.0499\n",
      "Epoch 80/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0082 - mae: 0.0600 - val_loss: 0.0028 - val_mae: 0.0454\n",
      "Epoch 81/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0076 - mae: 0.0607 - val_loss: 0.0029 - val_mae: 0.0376\n",
      "Epoch 82/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0080 - mae: 0.0588 - val_loss: 0.0030 - val_mae: 0.0372\n",
      "Epoch 83/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0081 - mae: 0.0587 - val_loss: 0.0026 - val_mae: 0.0407\n",
      "Epoch 84/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0083 - mae: 0.0593 - val_loss: 0.0047 - val_mae: 0.0489\n",
      "Epoch 85/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0089 - mae: 0.0652 - val_loss: 0.0028 - val_mae: 0.0459\n",
      "Epoch 86/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0090 - mae: 0.0609 - val_loss: 0.0027 - val_mae: 0.0440\n",
      "Epoch 87/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0092 - mae: 0.0633 - val_loss: 0.0040 - val_mae: 0.0433\n",
      "Epoch 88/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0099 - mae: 0.0635 - val_loss: 0.0029 - val_mae: 0.0377\n",
      "Epoch 89/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0077 - mae: 0.0558 - val_loss: 0.0038 - val_mae: 0.0413\n",
      "Epoch 90/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0071 - mae: 0.0571 - val_loss: 0.0026 - val_mae: 0.0401\n",
      "Epoch 91/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0082 - mae: 0.0580 - val_loss: 0.0041 - val_mae: 0.0583\n",
      "Epoch 92/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0083 - mae: 0.0596 - val_loss: 0.0045 - val_mae: 0.0468\n",
      "Epoch 93/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0079 - mae: 0.0593 - val_loss: 0.0027 - val_mae: 0.0380\n",
      "Epoch 94/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0091 - mae: 0.0611 - val_loss: 0.0032 - val_mae: 0.0378\n",
      "Epoch 95/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0085 - mae: 0.0628 - val_loss: 0.0029 - val_mae: 0.0384\n",
      "Epoch 96/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0078 - mae: 0.0585 - val_loss: 0.0030 - val_mae: 0.0376\n",
      "Epoch 97/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0088 - mae: 0.0590 - val_loss: 0.0026 - val_mae: 0.0396\n",
      "Epoch 98/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0084 - mae: 0.0606 - val_loss: 0.0043 - val_mae: 0.0453\n",
      "Epoch 99/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0077 - mae: 0.0595 - val_loss: 0.0032 - val_mae: 0.0507\n",
      "Epoch 100/100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0078 - mae: 0.0563 - val_loss: 0.0032 - val_mae: 0.0509\n",
      "\u001b[1m38/38\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n",
      "âœ… Done with capsicum_Belgaum_daily.csv | MAE=492.6, RMSE=641.75, R2=0.35, MAPE=18.46%, Accuracy=81.54%\n",
      "\n",
      "ğŸš€ Processing: capsicum_Bellary_daily.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\2878872630.py:115: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Average Price'].fillna(df['Average Price'].mean(), inplace=True)\n",
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\2878872630.py:120: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['MA_7'].fillna(df['MA_7'].mean(), inplace=True)\n",
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\2878872630.py:121: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['MA_30'].fillna(df['MA_30'].mean(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 119ms/step - loss: 0.5727 - mae: 0.6806 - val_loss: 0.5056 - val_mae: 0.6663\n",
      "Epoch 2/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.1938 - mae: 0.3806 - val_loss: 0.1066 - val_mae: 0.2740\n",
      "Epoch 3/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0864 - mae: 0.2444 - val_loss: 0.1691 - val_mae: 0.3620\n",
      "Epoch 4/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0699 - mae: 0.2199 - val_loss: 0.2649 - val_mae: 0.4608\n",
      "Epoch 5/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0652 - mae: 0.2151 - val_loss: 0.0686 - val_mae: 0.2074\n",
      "Epoch 6/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.0693 - mae: 0.2224 - val_loss: 0.2994 - val_mae: 0.4941\n",
      "Epoch 7/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0632 - mae: 0.2191 - val_loss: 0.1243 - val_mae: 0.3202\n",
      "Epoch 8/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0402 - mae: 0.1716 - val_loss: 0.1834 - val_mae: 0.3896\n",
      "Epoch 9/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0368 - mae: 0.1608 - val_loss: 0.1355 - val_mae: 0.3362\n",
      "Epoch 10/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0384 - mae: 0.1545 - val_loss: 0.1355 - val_mae: 0.3358\n",
      "Epoch 11/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0404 - mae: 0.1608 - val_loss: 0.1800 - val_mae: 0.3858\n",
      "Epoch 12/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0371 - mae: 0.1675 - val_loss: 0.1325 - val_mae: 0.3312\n",
      "Epoch 13/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0344 - mae: 0.1523 - val_loss: 0.1596 - val_mae: 0.3636\n",
      "Epoch 14/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0328 - mae: 0.1469 - val_loss: 0.1419 - val_mae: 0.3431\n",
      "Epoch 15/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0365 - mae: 0.1592 - val_loss: 0.1826 - val_mae: 0.3884\n",
      "Epoch 16/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0388 - mae: 0.1579 - val_loss: 0.1409 - val_mae: 0.3424\n",
      "Epoch 17/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0335 - mae: 0.1497 - val_loss: 0.1592 - val_mae: 0.3635\n",
      "Epoch 18/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0396 - mae: 0.1622 - val_loss: 0.1839 - val_mae: 0.3887\n",
      "Epoch 19/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0369 - mae: 0.1644 - val_loss: 0.1432 - val_mae: 0.3438\n",
      "Epoch 20/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0313 - mae: 0.1418 - val_loss: 0.1543 - val_mae: 0.3566\n",
      "Epoch 21/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0313 - mae: 0.1478 - val_loss: 0.1765 - val_mae: 0.3808\n",
      "Epoch 22/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0366 - mae: 0.1589 - val_loss: 0.1586 - val_mae: 0.3615\n",
      "Epoch 23/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0333 - mae: 0.1501 - val_loss: 0.1772 - val_mae: 0.3815\n",
      "Epoch 24/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0289 - mae: 0.1435 - val_loss: 0.1621 - val_mae: 0.3654\n",
      "Epoch 25/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0322 - mae: 0.1440 - val_loss: 0.1524 - val_mae: 0.3546\n",
      "Epoch 26/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0357 - mae: 0.1518 - val_loss: 0.1664 - val_mae: 0.3703\n",
      "Epoch 27/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0422 - mae: 0.1709 - val_loss: 0.1420 - val_mae: 0.3425\n",
      "Epoch 28/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0343 - mae: 0.1475 - val_loss: 0.1859 - val_mae: 0.3904\n",
      "Epoch 29/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0341 - mae: 0.1553 - val_loss: 0.1229 - val_mae: 0.3174\n",
      "Epoch 30/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0337 - mae: 0.1531 - val_loss: 0.1771 - val_mae: 0.3815\n",
      "Epoch 31/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0301 - mae: 0.1388 - val_loss: 0.1367 - val_mae: 0.3363\n",
      "Epoch 32/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0288 - mae: 0.1391 - val_loss: 0.1715 - val_mae: 0.3763\n",
      "Epoch 33/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0314 - mae: 0.1453 - val_loss: 0.1600 - val_mae: 0.3638\n",
      "Epoch 34/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0326 - mae: 0.1535 - val_loss: 0.1812 - val_mae: 0.3861\n",
      "Epoch 35/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0331 - mae: 0.1529 - val_loss: 0.1344 - val_mae: 0.3327\n",
      "Epoch 36/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0310 - mae: 0.1400 - val_loss: 0.1686 - val_mae: 0.3724\n",
      "Epoch 37/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0329 - mae: 0.1510 - val_loss: 0.1359 - val_mae: 0.3342\n",
      "Epoch 38/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0312 - mae: 0.1406 - val_loss: 0.1854 - val_mae: 0.3896\n",
      "Epoch 39/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0298 - mae: 0.1462 - val_loss: 0.1482 - val_mae: 0.3498\n",
      "Epoch 40/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0316 - mae: 0.1485 - val_loss: 0.1434 - val_mae: 0.3442\n",
      "Epoch 41/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0338 - mae: 0.1441 - val_loss: 0.1810 - val_mae: 0.3862\n",
      "Epoch 42/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0385 - mae: 0.1666 - val_loss: 0.1564 - val_mae: 0.3599\n",
      "Epoch 43/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0269 - mae: 0.1403 - val_loss: 0.1443 - val_mae: 0.3458\n",
      "Epoch 44/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0341 - mae: 0.1540 - val_loss: 0.1761 - val_mae: 0.3813\n",
      "Epoch 45/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0363 - mae: 0.1610 - val_loss: 0.1317 - val_mae: 0.3303\n",
      "Epoch 46/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0347 - mae: 0.1528 - val_loss: 0.1689 - val_mae: 0.3741\n",
      "Epoch 47/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0282 - mae: 0.1441 - val_loss: 0.1411 - val_mae: 0.3426\n",
      "Epoch 48/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0293 - mae: 0.1402 - val_loss: 0.1357 - val_mae: 0.3362\n",
      "Epoch 49/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0332 - mae: 0.1548 - val_loss: 0.1686 - val_mae: 0.3743\n",
      "Epoch 50/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0268 - mae: 0.1302 - val_loss: 0.1506 - val_mae: 0.3540\n",
      "Epoch 51/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0342 - mae: 0.1520 - val_loss: 0.1422 - val_mae: 0.3439\n",
      "Epoch 52/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0308 - mae: 0.1426 - val_loss: 0.1716 - val_mae: 0.3773\n",
      "Epoch 53/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0296 - mae: 0.1431 - val_loss: 0.1456 - val_mae: 0.3478\n",
      "Epoch 54/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0367 - mae: 0.1531 - val_loss: 0.1814 - val_mae: 0.3865\n",
      "Epoch 55/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0281 - mae: 0.1372 - val_loss: 0.1434 - val_mae: 0.3442\n",
      "Epoch 56/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0350 - mae: 0.1522 - val_loss: 0.1667 - val_mae: 0.3704\n",
      "Epoch 57/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0358 - mae: 0.1557 - val_loss: 0.1595 - val_mae: 0.3626\n",
      "Epoch 58/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0398 - mae: 0.1674 - val_loss: 0.1596 - val_mae: 0.3630\n",
      "Epoch 59/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0365 - mae: 0.1539 - val_loss: 0.1612 - val_mae: 0.3652\n",
      "Epoch 60/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0339 - mae: 0.1515 - val_loss: 0.1567 - val_mae: 0.3604\n",
      "Epoch 61/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0293 - mae: 0.1450 - val_loss: 0.1715 - val_mae: 0.3764\n",
      "Epoch 62/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0295 - mae: 0.1397 - val_loss: 0.1532 - val_mae: 0.3562\n",
      "Epoch 63/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0323 - mae: 0.1535 - val_loss: 0.1444 - val_mae: 0.3459\n",
      "Epoch 64/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0272 - mae: 0.1386 - val_loss: 0.1800 - val_mae: 0.3851\n",
      "Epoch 65/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0337 - mae: 0.1539 - val_loss: 0.1416 - val_mae: 0.3424\n",
      "Epoch 66/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0349 - mae: 0.1526 - val_loss: 0.1613 - val_mae: 0.3650\n",
      "Epoch 67/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0314 - mae: 0.1488 - val_loss: 0.1565 - val_mae: 0.3596\n",
      "Epoch 68/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0310 - mae: 0.1468 - val_loss: 0.1592 - val_mae: 0.3626\n",
      "Epoch 69/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0277 - mae: 0.1406 - val_loss: 0.1495 - val_mae: 0.3515\n",
      "Epoch 70/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0310 - mae: 0.1449 - val_loss: 0.1690 - val_mae: 0.3731\n",
      "Epoch 71/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0258 - mae: 0.1338 - val_loss: 0.1566 - val_mae: 0.3595\n",
      "Epoch 72/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0289 - mae: 0.1395 - val_loss: 0.1590 - val_mae: 0.3627\n",
      "Epoch 73/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0310 - mae: 0.1394 - val_loss: 0.1788 - val_mae: 0.3840\n",
      "Epoch 74/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0335 - mae: 0.1525 - val_loss: 0.1317 - val_mae: 0.3303\n",
      "Epoch 75/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0303 - mae: 0.1396 - val_loss: 0.1900 - val_mae: 0.3955\n",
      "Epoch 76/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0262 - mae: 0.1397 - val_loss: 0.1127 - val_mae: 0.3034\n",
      "Epoch 77/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0321 - mae: 0.1438 - val_loss: 0.2095 - val_mae: 0.4137\n",
      "Epoch 78/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0264 - mae: 0.1368 - val_loss: 0.1189 - val_mae: 0.3125\n",
      "Epoch 79/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0325 - mae: 0.1398 - val_loss: 0.1847 - val_mae: 0.3899\n",
      "Epoch 80/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0303 - mae: 0.1483 - val_loss: 0.1603 - val_mae: 0.3644\n",
      "Epoch 81/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0329 - mae: 0.1516 - val_loss: 0.1585 - val_mae: 0.3626\n",
      "Epoch 82/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0307 - mae: 0.1481 - val_loss: 0.1372 - val_mae: 0.3375\n",
      "Epoch 83/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0338 - mae: 0.1584 - val_loss: 0.1607 - val_mae: 0.3650\n",
      "Epoch 84/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0285 - mae: 0.1342 - val_loss: 0.1434 - val_mae: 0.3448\n",
      "Epoch 85/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0309 - mae: 0.1471 - val_loss: 0.1860 - val_mae: 0.3913\n",
      "Epoch 86/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0292 - mae: 0.1428 - val_loss: 0.1346 - val_mae: 0.3341\n",
      "Epoch 87/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0256 - mae: 0.1326 - val_loss: 0.1885 - val_mae: 0.3941\n",
      "Epoch 88/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0256 - mae: 0.1242 - val_loss: 0.1209 - val_mae: 0.3156\n",
      "Epoch 89/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0267 - mae: 0.1376 - val_loss: 0.2085 - val_mae: 0.4126\n",
      "Epoch 90/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0319 - mae: 0.1518 - val_loss: 0.1245 - val_mae: 0.3209\n",
      "Epoch 91/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0325 - mae: 0.1492 - val_loss: 0.1746 - val_mae: 0.3803\n",
      "Epoch 92/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0338 - mae: 0.1546 - val_loss: 0.1747 - val_mae: 0.3805\n",
      "Epoch 93/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0274 - mae: 0.1434 - val_loss: 0.1467 - val_mae: 0.3494\n",
      "Epoch 94/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0285 - mae: 0.1422 - val_loss: 0.1715 - val_mae: 0.3771\n",
      "Epoch 95/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0288 - mae: 0.1469 - val_loss: 0.1503 - val_mae: 0.3536\n",
      "Epoch 96/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0291 - mae: 0.1400 - val_loss: 0.1440 - val_mae: 0.3464\n",
      "Epoch 97/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0320 - mae: 0.1522 - val_loss: 0.1642 - val_mae: 0.3697\n",
      "Epoch 98/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0315 - mae: 0.1492 - val_loss: 0.1592 - val_mae: 0.3641\n",
      "Epoch 99/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0325 - mae: 0.1495 - val_loss: 0.1519 - val_mae: 0.3558\n",
      "Epoch 100/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0352 - mae: 0.1533 - val_loss: 0.1695 - val_mae: 0.3752\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step\n",
      "âœ… Done with capsicum_Bellary_daily.csv | MAE=393.91, RMSE=497.99, R2=0.04, MAPE=15.97%, Accuracy=84.03%\n",
      "\n",
      "ğŸš€ Processing: capsicum_Chikmagalur_daily.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\2878872630.py:115: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Average Price'].fillna(df['Average Price'].mean(), inplace=True)\n",
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\2878872630.py:120: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['MA_7'].fillna(df['MA_7'].mean(), inplace=True)\n",
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\2878872630.py:121: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['MA_30'].fillna(df['MA_30'].mean(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0786 - mae: 0.1962 - val_loss: 0.0349 - val_mae: 0.1511\n",
      "Epoch 2/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0101 - mae: 0.0801 - val_loss: 0.0331 - val_mae: 0.1472\n",
      "Epoch 3/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0080 - mae: 0.0723 - val_loss: 0.0354 - val_mae: 0.1598\n",
      "Epoch 4/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0070 - mae: 0.0671 - val_loss: 0.0320 - val_mae: 0.1434\n",
      "Epoch 5/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0074 - mae: 0.0697 - val_loss: 0.0315 - val_mae: 0.1536\n",
      "Epoch 6/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0063 - mae: 0.0646 - val_loss: 0.0283 - val_mae: 0.1395\n",
      "Epoch 7/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0061 - mae: 0.0633 - val_loss: 0.0275 - val_mae: 0.1377\n",
      "Epoch 8/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0069 - mae: 0.0670 - val_loss: 0.0261 - val_mae: 0.1343\n",
      "Epoch 9/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0057 - mae: 0.0603 - val_loss: 0.0250 - val_mae: 0.1315\n",
      "Epoch 10/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0060 - mae: 0.0626 - val_loss: 0.0270 - val_mae: 0.1425\n",
      "Epoch 11/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0058 - mae: 0.0617 - val_loss: 0.0242 - val_mae: 0.1328\n",
      "Epoch 12/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0052 - mae: 0.0582 - val_loss: 0.0231 - val_mae: 0.1254\n",
      "Epoch 13/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0053 - mae: 0.0588 - val_loss: 0.0245 - val_mae: 0.1351\n",
      "Epoch 14/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0055 - mae: 0.0592 - val_loss: 0.0228 - val_mae: 0.1268\n",
      "Epoch 15/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0057 - mae: 0.0601 - val_loss: 0.0230 - val_mae: 0.1159\n",
      "Epoch 16/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0051 - mae: 0.0569 - val_loss: 0.0215 - val_mae: 0.1202\n",
      "Epoch 17/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0057 - mae: 0.0607 - val_loss: 0.0214 - val_mae: 0.1135\n",
      "Epoch 18/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0048 - mae: 0.0547 - val_loss: 0.0208 - val_mae: 0.1192\n",
      "Epoch 19/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0049 - mae: 0.0554 - val_loss: 0.0204 - val_mae: 0.1166\n",
      "Epoch 20/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0047 - mae: 0.0535 - val_loss: 0.0216 - val_mae: 0.1114\n",
      "Epoch 21/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0048 - mae: 0.0548 - val_loss: 0.0201 - val_mae: 0.1089\n",
      "Epoch 22/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0051 - mae: 0.0560 - val_loss: 0.0195 - val_mae: 0.1106\n",
      "Epoch 23/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0045 - mae: 0.0519 - val_loss: 0.0196 - val_mae: 0.1129\n",
      "Epoch 24/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0050 - mae: 0.0557 - val_loss: 0.0208 - val_mae: 0.1118\n",
      "Epoch 25/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0046 - mae: 0.0532 - val_loss: 0.0208 - val_mae: 0.1131\n",
      "Epoch 26/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0046 - mae: 0.0525 - val_loss: 0.0191 - val_mae: 0.1082\n",
      "Epoch 27/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0047 - mae: 0.0535 - val_loss: 0.0191 - val_mae: 0.1122\n",
      "Epoch 28/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.0045 - mae: 0.0516 - val_loss: 0.0193 - val_mae: 0.1069\n",
      "Epoch 29/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0043 - mae: 0.0505 - val_loss: 0.0192 - val_mae: 0.1091\n",
      "Epoch 30/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0045 - mae: 0.0523 - val_loss: 0.0193 - val_mae: 0.1094\n",
      "Epoch 31/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0045 - mae: 0.0514 - val_loss: 0.0185 - val_mae: 0.1046\n",
      "Epoch 32/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0044 - mae: 0.0504 - val_loss: 0.0191 - val_mae: 0.1096\n",
      "Epoch 33/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0047 - mae: 0.0532 - val_loss: 0.0186 - val_mae: 0.1059\n",
      "Epoch 34/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0043 - mae: 0.0497 - val_loss: 0.0187 - val_mae: 0.1062\n",
      "Epoch 35/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0042 - mae: 0.0497 - val_loss: 0.0194 - val_mae: 0.1108\n",
      "Epoch 36/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0044 - mae: 0.0502 - val_loss: 0.0226 - val_mae: 0.1220\n",
      "Epoch 37/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0046 - mae: 0.0516 - val_loss: 0.0225 - val_mae: 0.1223\n",
      "Epoch 38/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0043 - mae: 0.0499 - val_loss: 0.0207 - val_mae: 0.1162\n",
      "Epoch 39/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0046 - mae: 0.0522 - val_loss: 0.0194 - val_mae: 0.1100\n",
      "Epoch 40/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0045 - mae: 0.0515 - val_loss: 0.0207 - val_mae: 0.1165\n",
      "Epoch 41/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0043 - mae: 0.0497 - val_loss: 0.0187 - val_mae: 0.1094\n",
      "Epoch 42/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0047 - mae: 0.0540 - val_loss: 0.0204 - val_mae: 0.1163\n",
      "Epoch 43/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0044 - mae: 0.0505 - val_loss: 0.0185 - val_mae: 0.1051\n",
      "Epoch 44/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0043 - mae: 0.0500 - val_loss: 0.0194 - val_mae: 0.1105\n",
      "Epoch 45/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0047 - mae: 0.0529 - val_loss: 0.0212 - val_mae: 0.1179\n",
      "Epoch 46/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0045 - mae: 0.0520 - val_loss: 0.0261 - val_mae: 0.1340\n",
      "Epoch 47/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0053 - mae: 0.0570 - val_loss: 0.0187 - val_mae: 0.1081\n",
      "Epoch 48/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0044 - mae: 0.0503 - val_loss: 0.0186 - val_mae: 0.1075\n",
      "Epoch 49/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0042 - mae: 0.0488 - val_loss: 0.0189 - val_mae: 0.1096\n",
      "Epoch 50/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0045 - mae: 0.0520 - val_loss: 0.0194 - val_mae: 0.1114\n",
      "Epoch 51/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0044 - mae: 0.0503 - val_loss: 0.0222 - val_mae: 0.1224\n",
      "Epoch 52/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0043 - mae: 0.0512 - val_loss: 0.0250 - val_mae: 0.1306\n",
      "Epoch 53/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0045 - mae: 0.0512 - val_loss: 0.0238 - val_mae: 0.1255\n",
      "Epoch 54/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0051 - mae: 0.0568 - val_loss: 0.0190 - val_mae: 0.1098\n",
      "Epoch 55/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0042 - mae: 0.0489 - val_loss: 0.0191 - val_mae: 0.1106\n",
      "Epoch 56/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0040 - mae: 0.0477 - val_loss: 0.0185 - val_mae: 0.1049\n",
      "Epoch 57/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0042 - mae: 0.0488 - val_loss: 0.0194 - val_mae: 0.1118\n",
      "Epoch 58/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0044 - mae: 0.0501 - val_loss: 0.0231 - val_mae: 0.1249\n",
      "Epoch 59/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0043 - mae: 0.0502 - val_loss: 0.0209 - val_mae: 0.1182\n",
      "Epoch 60/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0043 - mae: 0.0494 - val_loss: 0.0213 - val_mae: 0.1181\n",
      "Epoch 61/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0042 - mae: 0.0495 - val_loss: 0.0186 - val_mae: 0.1076\n",
      "Epoch 62/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0045 - mae: 0.0508 - val_loss: 0.0187 - val_mae: 0.1074\n",
      "Epoch 63/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0044 - mae: 0.0503 - val_loss: 0.0194 - val_mae: 0.1118\n",
      "Epoch 64/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0041 - mae: 0.0485 - val_loss: 0.0214 - val_mae: 0.1172\n",
      "Epoch 65/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0045 - mae: 0.0513 - val_loss: 0.0198 - val_mae: 0.1120\n",
      "Epoch 66/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0042 - mae: 0.0498 - val_loss: 0.0203 - val_mae: 0.1151\n",
      "Epoch 67/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0046 - mae: 0.0510 - val_loss: 0.0186 - val_mae: 0.1076\n",
      "Epoch 68/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0044 - mae: 0.0496 - val_loss: 0.0231 - val_mae: 0.1246\n",
      "Epoch 69/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0042 - mae: 0.0496 - val_loss: 0.0197 - val_mae: 0.1142\n",
      "Epoch 70/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0044 - mae: 0.0501 - val_loss: 0.0195 - val_mae: 0.1117\n",
      "Epoch 71/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0044 - mae: 0.0503 - val_loss: 0.0211 - val_mae: 0.1170\n",
      "Epoch 72/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0044 - mae: 0.0503 - val_loss: 0.0244 - val_mae: 0.1280\n",
      "Epoch 73/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0044 - mae: 0.0505 - val_loss: 0.0209 - val_mae: 0.1183\n",
      "Epoch 74/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0042 - mae: 0.0490 - val_loss: 0.0192 - val_mae: 0.1095\n",
      "Epoch 75/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0043 - mae: 0.0509 - val_loss: 0.0221 - val_mae: 0.1213\n",
      "Epoch 76/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0044 - mae: 0.0502 - val_loss: 0.0217 - val_mae: 0.1206\n",
      "Epoch 77/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0044 - mae: 0.0509 - val_loss: 0.0209 - val_mae: 0.1163\n",
      "Epoch 78/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0042 - mae: 0.0482 - val_loss: 0.0206 - val_mae: 0.1154\n",
      "Epoch 79/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0043 - mae: 0.0491 - val_loss: 0.0193 - val_mae: 0.1121\n",
      "Epoch 80/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0047 - mae: 0.0518 - val_loss: 0.0193 - val_mae: 0.1118\n",
      "Epoch 81/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0044 - mae: 0.0492 - val_loss: 0.0201 - val_mae: 0.1136\n",
      "Epoch 82/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0042 - mae: 0.0492 - val_loss: 0.0186 - val_mae: 0.1080\n",
      "Epoch 83/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0043 - mae: 0.0501 - val_loss: 0.0192 - val_mae: 0.1117\n",
      "Epoch 84/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0045 - mae: 0.0504 - val_loss: 0.0241 - val_mae: 0.1262\n",
      "Epoch 85/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0044 - mae: 0.0509 - val_loss: 0.0230 - val_mae: 0.1234\n",
      "Epoch 86/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0043 - mae: 0.0506 - val_loss: 0.0211 - val_mae: 0.1173\n",
      "Epoch 87/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0044 - mae: 0.0509 - val_loss: 0.0229 - val_mae: 0.1212\n",
      "Epoch 88/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0044 - mae: 0.0508 - val_loss: 0.0206 - val_mae: 0.1149\n",
      "Epoch 89/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0042 - mae: 0.0484 - val_loss: 0.0189 - val_mae: 0.1073\n",
      "Epoch 90/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0046 - mae: 0.0510 - val_loss: 0.0193 - val_mae: 0.1088\n",
      "Epoch 91/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0046 - mae: 0.0507 - val_loss: 0.0193 - val_mae: 0.1114\n",
      "Epoch 92/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0043 - mae: 0.0496 - val_loss: 0.0189 - val_mae: 0.1084\n",
      "Epoch 93/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0041 - mae: 0.0479 - val_loss: 0.0186 - val_mae: 0.1055\n",
      "Epoch 94/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0043 - mae: 0.0496 - val_loss: 0.0215 - val_mae: 0.1176\n",
      "Epoch 95/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0045 - mae: 0.0507 - val_loss: 0.0213 - val_mae: 0.1179\n",
      "Epoch 96/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0043 - mae: 0.0499 - val_loss: 0.0195 - val_mae: 0.1104\n",
      "Epoch 97/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0045 - mae: 0.0508 - val_loss: 0.0227 - val_mae: 0.1229\n",
      "Epoch 98/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0042 - mae: 0.0487 - val_loss: 0.0200 - val_mae: 0.1146\n",
      "Epoch 99/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0044 - mae: 0.0491 - val_loss: 0.0213 - val_mae: 0.1181\n",
      "Epoch 100/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0041 - mae: 0.0489 - val_loss: 0.0219 - val_mae: 0.1194\n",
      "\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
      "âœ… Done with capsicum_Chikmagalur_daily.csv | MAE=271.49, RMSE=339.62, R2=0.67, MAPE=16.76%, Accuracy=83.24%\n",
      "\n",
      "ğŸš€ Processing: capsicum_Davangere_daily.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\2878872630.py:115: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Average Price'].fillna(df['Average Price'].mean(), inplace=True)\n",
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\2878872630.py:120: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['MA_7'].fillna(df['MA_7'].mean(), inplace=True)\n",
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\2878872630.py:121: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['MA_30'].fillna(df['MA_30'].mean(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 0.0879 - mae: 0.1853 - val_loss: 0.0126 - val_mae: 0.0755\n",
      "Epoch 2/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0067 - mae: 0.0639 - val_loss: 0.0087 - val_mae: 0.0591\n",
      "Epoch 3/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0050 - mae: 0.0539 - val_loss: 0.0085 - val_mae: 0.0563\n",
      "Epoch 4/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0030 - mae: 0.0421 - val_loss: 0.0111 - val_mae: 0.0711\n",
      "Epoch 5/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0028 - mae: 0.0396 - val_loss: 0.0076 - val_mae: 0.0529\n",
      "Epoch 6/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0027 - mae: 0.0387 - val_loss: 0.0080 - val_mae: 0.0553\n",
      "Epoch 7/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0021 - mae: 0.0337 - val_loss: 0.0089 - val_mae: 0.0604\n",
      "Epoch 8/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0023 - mae: 0.0344 - val_loss: 0.0072 - val_mae: 0.0525\n",
      "Epoch 9/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.0020 - mae: 0.0313 - val_loss: 0.0065 - val_mae: 0.0493\n",
      "Epoch 10/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.0019 - mae: 0.0318 - val_loss: 0.0063 - val_mae: 0.0502\n",
      "Epoch 11/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0019 - mae: 0.0309 - val_loss: 0.0064 - val_mae: 0.0493\n",
      "Epoch 12/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0018 - mae: 0.0301 - val_loss: 0.0064 - val_mae: 0.0498\n",
      "Epoch 13/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0019 - mae: 0.0297 - val_loss: 0.0069 - val_mae: 0.0548\n",
      "Epoch 14/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0018 - mae: 0.0286 - val_loss: 0.0070 - val_mae: 0.0561\n",
      "Epoch 15/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0018 - mae: 0.0301 - val_loss: 0.0065 - val_mae: 0.0501\n",
      "Epoch 16/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0017 - mae: 0.0286 - val_loss: 0.0069 - val_mae: 0.0517\n",
      "Epoch 17/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0016 - mae: 0.0284 - val_loss: 0.0067 - val_mae: 0.0520\n",
      "Epoch 18/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0016 - mae: 0.0287 - val_loss: 0.0075 - val_mae: 0.0546\n",
      "Epoch 19/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.0015 - mae: 0.0269 - val_loss: 0.0090 - val_mae: 0.0631\n",
      "Epoch 20/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0017 - mae: 0.0286 - val_loss: 0.0080 - val_mae: 0.0557\n",
      "Epoch 21/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0017 - mae: 0.0281 - val_loss: 0.0092 - val_mae: 0.0577\n",
      "Epoch 22/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.0019 - mae: 0.0305 - val_loss: 0.0073 - val_mae: 0.0521\n",
      "Epoch 23/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0015 - mae: 0.0274 - val_loss: 0.0076 - val_mae: 0.0563\n",
      "Epoch 24/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0015 - mae: 0.0272 - val_loss: 0.0080 - val_mae: 0.0574\n",
      "Epoch 25/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0015 - mae: 0.0275 - val_loss: 0.0079 - val_mae: 0.0539\n",
      "Epoch 26/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0015 - mae: 0.0267 - val_loss: 0.0075 - val_mae: 0.0528\n",
      "Epoch 27/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0016 - mae: 0.0274 - val_loss: 0.0081 - val_mae: 0.0549\n",
      "Epoch 28/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0015 - mae: 0.0268 - val_loss: 0.0082 - val_mae: 0.0537\n",
      "Epoch 29/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0015 - mae: 0.0271 - val_loss: 0.0093 - val_mae: 0.0580\n",
      "Epoch 30/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0017 - mae: 0.0287 - val_loss: 0.0080 - val_mae: 0.0605\n",
      "Epoch 31/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0014 - mae: 0.0264 - val_loss: 0.0089 - val_mae: 0.0553\n",
      "Epoch 32/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0014 - mae: 0.0264 - val_loss: 0.0088 - val_mae: 0.0555\n",
      "Epoch 33/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0016 - mae: 0.0278 - val_loss: 0.0080 - val_mae: 0.0541\n",
      "Epoch 34/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.0016 - mae: 0.0267 - val_loss: 0.0080 - val_mae: 0.0561\n",
      "Epoch 35/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0016 - mae: 0.0272 - val_loss: 0.0099 - val_mae: 0.0613\n",
      "Epoch 36/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0015 - mae: 0.0263 - val_loss: 0.0077 - val_mae: 0.0531\n",
      "Epoch 37/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0016 - mae: 0.0269 - val_loss: 0.0090 - val_mae: 0.0558\n",
      "Epoch 38/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0015 - mae: 0.0261 - val_loss: 0.0096 - val_mae: 0.0600\n",
      "Epoch 39/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0016 - mae: 0.0269 - val_loss: 0.0075 - val_mae: 0.0534\n",
      "Epoch 40/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0016 - mae: 0.0285 - val_loss: 0.0108 - val_mae: 0.0646\n",
      "Epoch 41/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0014 - mae: 0.0258 - val_loss: 0.0087 - val_mae: 0.0548\n",
      "Epoch 42/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0016 - mae: 0.0271 - val_loss: 0.0088 - val_mae: 0.0565\n",
      "Epoch 43/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0016 - mae: 0.0266 - val_loss: 0.0102 - val_mae: 0.0610\n",
      "Epoch 44/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0015 - mae: 0.0265 - val_loss: 0.0082 - val_mae: 0.0544\n",
      "Epoch 45/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0016 - mae: 0.0268 - val_loss: 0.0083 - val_mae: 0.0544\n",
      "Epoch 46/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0014 - mae: 0.0259 - val_loss: 0.0098 - val_mae: 0.0602\n",
      "Epoch 47/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0015 - mae: 0.0270 - val_loss: 0.0094 - val_mae: 0.0585\n",
      "Epoch 48/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0015 - mae: 0.0265 - val_loss: 0.0128 - val_mae: 0.0708\n",
      "Epoch 49/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0015 - mae: 0.0266 - val_loss: 0.0068 - val_mae: 0.0527\n",
      "Epoch 50/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0014 - mae: 0.0255 - val_loss: 0.0087 - val_mae: 0.0584\n",
      "Epoch 51/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0016 - mae: 0.0276 - val_loss: 0.0072 - val_mae: 0.0525\n",
      "Epoch 52/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0016 - mae: 0.0272 - val_loss: 0.0105 - val_mae: 0.0627\n",
      "Epoch 53/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0016 - mae: 0.0273 - val_loss: 0.0111 - val_mae: 0.0689\n",
      "Epoch 54/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0015 - mae: 0.0261 - val_loss: 0.0072 - val_mae: 0.0525\n",
      "Epoch 55/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0014 - mae: 0.0254 - val_loss: 0.0096 - val_mae: 0.0625\n",
      "Epoch 56/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0014 - mae: 0.0258 - val_loss: 0.0072 - val_mae: 0.0524\n",
      "Epoch 57/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0014 - mae: 0.0257 - val_loss: 0.0073 - val_mae: 0.0530\n",
      "Epoch 58/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0015 - mae: 0.0268 - val_loss: 0.0086 - val_mae: 0.0572\n",
      "Epoch 59/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0014 - mae: 0.0254 - val_loss: 0.0068 - val_mae: 0.0517\n",
      "Epoch 60/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0014 - mae: 0.0253 - val_loss: 0.0068 - val_mae: 0.0521\n",
      "Epoch 61/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0013 - mae: 0.0255 - val_loss: 0.0070 - val_mae: 0.0519\n",
      "Epoch 62/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0015 - mae: 0.0260 - val_loss: 0.0067 - val_mae: 0.0526\n",
      "Epoch 63/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0016 - mae: 0.0269 - val_loss: 0.0071 - val_mae: 0.0532\n",
      "Epoch 64/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0015 - mae: 0.0264 - val_loss: 0.0111 - val_mae: 0.0710\n",
      "Epoch 65/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0016 - mae: 0.0281 - val_loss: 0.0080 - val_mae: 0.0559\n",
      "Epoch 66/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0015 - mae: 0.0266 - val_loss: 0.0062 - val_mae: 0.0515\n",
      "Epoch 67/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.0015 - mae: 0.0263 - val_loss: 0.0068 - val_mae: 0.0514\n",
      "Epoch 68/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0014 - mae: 0.0246 - val_loss: 0.0068 - val_mae: 0.0516\n",
      "Epoch 69/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0014 - mae: 0.0264 - val_loss: 0.0072 - val_mae: 0.0524\n",
      "Epoch 70/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0015 - mae: 0.0277 - val_loss: 0.0068 - val_mae: 0.0514\n",
      "Epoch 71/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0015 - mae: 0.0262 - val_loss: 0.0071 - val_mae: 0.0529\n",
      "Epoch 72/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0013 - mae: 0.0248 - val_loss: 0.0082 - val_mae: 0.0573\n",
      "Epoch 73/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0014 - mae: 0.0250 - val_loss: 0.0082 - val_mae: 0.0576\n",
      "Epoch 74/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0014 - mae: 0.0259 - val_loss: 0.0090 - val_mae: 0.0622\n",
      "Epoch 75/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0014 - mae: 0.0259 - val_loss: 0.0059 - val_mae: 0.0494\n",
      "Epoch 76/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0013 - mae: 0.0246 - val_loss: 0.0065 - val_mae: 0.0507\n",
      "Epoch 77/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0014 - mae: 0.0253 - val_loss: 0.0081 - val_mae: 0.0586\n",
      "Epoch 78/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0014 - mae: 0.0260 - val_loss: 0.0059 - val_mae: 0.0500\n",
      "Epoch 79/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0014 - mae: 0.0260 - val_loss: 0.0071 - val_mae: 0.0553\n",
      "Epoch 80/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0014 - mae: 0.0251 - val_loss: 0.0070 - val_mae: 0.0522\n",
      "Epoch 81/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0015 - mae: 0.0260 - val_loss: 0.0063 - val_mae: 0.0509\n",
      "Epoch 82/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0014 - mae: 0.0255 - val_loss: 0.0054 - val_mae: 0.0487\n",
      "Epoch 83/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0014 - mae: 0.0258 - val_loss: 0.0066 - val_mae: 0.0532\n",
      "Epoch 84/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0013 - mae: 0.0248 - val_loss: 0.0054 - val_mae: 0.0489\n",
      "Epoch 85/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0014 - mae: 0.0266 - val_loss: 0.0065 - val_mae: 0.0532\n",
      "Epoch 86/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0013 - mae: 0.0251 - val_loss: 0.0081 - val_mae: 0.0657\n",
      "Epoch 87/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0014 - mae: 0.0254 - val_loss: 0.0061 - val_mae: 0.0534\n",
      "Epoch 88/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0014 - mae: 0.0258 - val_loss: 0.0058 - val_mae: 0.0508\n",
      "Epoch 89/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0012 - mae: 0.0241 - val_loss: 0.0059 - val_mae: 0.0518\n",
      "Epoch 90/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0014 - mae: 0.0258 - val_loss: 0.0083 - val_mae: 0.0683\n",
      "Epoch 91/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0014 - mae: 0.0259 - val_loss: 0.0054 - val_mae: 0.0490\n",
      "Epoch 92/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0014 - mae: 0.0249 - val_loss: 0.0083 - val_mae: 0.0684\n",
      "Epoch 93/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0014 - mae: 0.0256 - val_loss: 0.0053 - val_mae: 0.0476\n",
      "Epoch 94/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0013 - mae: 0.0249 - val_loss: 0.0057 - val_mae: 0.0511\n",
      "Epoch 95/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0013 - mae: 0.0244 - val_loss: 0.0057 - val_mae: 0.0508\n",
      "Epoch 96/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0013 - mae: 0.0247 - val_loss: 0.0066 - val_mae: 0.0584\n",
      "Epoch 97/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0015 - mae: 0.0265 - val_loss: 0.0053 - val_mae: 0.0477\n",
      "Epoch 98/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0014 - mae: 0.0255 - val_loss: 0.0051 - val_mae: 0.0469\n",
      "Epoch 99/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0012 - mae: 0.0241 - val_loss: 0.0063 - val_mae: 0.0552\n",
      "Epoch 100/100\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0013 - mae: 0.0249 - val_loss: 0.0067 - val_mae: 0.0578\n",
      "\u001b[1m140/140\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "âœ… Done with capsicum_Davangere_daily.csv | MAE=533.77, RMSE=816.88, R2=0.78, MAPE=16.95%, Accuracy=83.05%\n",
      "\n",
      "ğŸš€ Processing: capsicum_Dharwad_daily.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\2878872630.py:115: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Average Price'].fillna(df['Average Price'].mean(), inplace=True)\n",
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\2878872630.py:120: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['MA_7'].fillna(df['MA_7'].mean(), inplace=True)\n",
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\2878872630.py:121: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['MA_30'].fillna(df['MA_30'].mean(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 56ms/step - loss: 0.6336 - mae: 0.6755 - val_loss: 0.0847 - val_mae: 0.2820\n",
      "Epoch 2/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.1571 - mae: 0.3174 - val_loss: 0.0069 - val_mae: 0.0706\n",
      "Epoch 3/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1345 - mae: 0.3024 - val_loss: 0.1050 - val_mae: 0.3159\n",
      "Epoch 4/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1106 - mae: 0.2736 - val_loss: 0.1979 - val_mae: 0.4389\n",
      "Epoch 5/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0790 - mae: 0.2283 - val_loss: 0.1464 - val_mae: 0.3756\n",
      "Epoch 6/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0952 - mae: 0.2554 - val_loss: 0.1338 - val_mae: 0.3584\n",
      "Epoch 7/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0803 - mae: 0.2315 - val_loss: 0.1508 - val_mae: 0.3813\n",
      "Epoch 8/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.1033 - mae: 0.2645 - val_loss: 0.1052 - val_mae: 0.3158\n",
      "Epoch 9/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0839 - mae: 0.2334 - val_loss: 0.0401 - val_mae: 0.1860\n",
      "Epoch 10/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.1033 - mae: 0.2698 - val_loss: 0.0735 - val_mae: 0.2606\n",
      "Epoch 11/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0702 - mae: 0.2243 - val_loss: 0.0688 - val_mae: 0.2515\n",
      "Epoch 12/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0877 - mae: 0.2492 - val_loss: 0.0781 - val_mae: 0.2693\n",
      "Epoch 13/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0724 - mae: 0.2207 - val_loss: 0.0145 - val_mae: 0.0969\n",
      "Epoch 14/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1100 - mae: 0.2789 - val_loss: 0.0116 - val_mae: 0.0842\n",
      "Epoch 15/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1054 - mae: 0.2763 - val_loss: 0.0647 - val_mae: 0.2430\n",
      "Epoch 16/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0878 - mae: 0.2484 - val_loss: 0.0158 - val_mae: 0.1022\n",
      "Epoch 17/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0917 - mae: 0.2521 - val_loss: 0.0296 - val_mae: 0.1544\n",
      "Epoch 18/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0949 - mae: 0.2580 - val_loss: 0.0490 - val_mae: 0.2078\n",
      "Epoch 19/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0761 - mae: 0.2365 - val_loss: 0.2145 - val_mae: 0.4565\n",
      "Epoch 20/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.1120 - mae: 0.2709 - val_loss: 0.0968 - val_mae: 0.3014\n",
      "Epoch 21/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0785 - mae: 0.2279 - val_loss: 0.0301 - val_mae: 0.1558\n",
      "Epoch 22/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0706 - mae: 0.2182 - val_loss: 0.1109 - val_mae: 0.3239\n",
      "Epoch 23/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0923 - mae: 0.2475 - val_loss: 0.1153 - val_mae: 0.3305\n",
      "Epoch 24/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0875 - mae: 0.2426 - val_loss: 0.0864 - val_mae: 0.2835\n",
      "Epoch 25/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0783 - mae: 0.2251 - val_loss: 0.0502 - val_mae: 0.2102\n",
      "Epoch 26/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1132 - mae: 0.2720 - val_loss: 0.0381 - val_mae: 0.1794\n",
      "Epoch 27/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0950 - mae: 0.2553 - val_loss: 0.0075 - val_mae: 0.0678\n",
      "Epoch 28/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0865 - mae: 0.2470 - val_loss: 0.0204 - val_mae: 0.1205\n",
      "Epoch 29/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0828 - mae: 0.2380 - val_loss: 0.1305 - val_mae: 0.3528\n",
      "Epoch 30/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0769 - mae: 0.2338 - val_loss: 0.0454 - val_mae: 0.1986\n",
      "Epoch 31/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0783 - mae: 0.2380 - val_loss: 0.0407 - val_mae: 0.1861\n",
      "Epoch 32/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0752 - mae: 0.2306 - val_loss: 0.0997 - val_mae: 0.3059\n",
      "Epoch 33/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0833 - mae: 0.2384 - val_loss: 0.0698 - val_mae: 0.2524\n",
      "Epoch 34/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0789 - mae: 0.2336 - val_loss: 0.0443 - val_mae: 0.1957\n",
      "Epoch 35/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0756 - mae: 0.2293 - val_loss: 0.0152 - val_mae: 0.0988\n",
      "Epoch 36/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0886 - mae: 0.2499 - val_loss: 0.0845 - val_mae: 0.2800\n",
      "Epoch 37/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0754 - mae: 0.2394 - val_loss: 0.0639 - val_mae: 0.2406\n",
      "Epoch 38/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0660 - mae: 0.2164 - val_loss: 0.0471 - val_mae: 0.2026\n",
      "Epoch 39/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0692 - mae: 0.2240 - val_loss: 0.1020 - val_mae: 0.3098\n",
      "Epoch 40/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0746 - mae: 0.2277 - val_loss: 0.0761 - val_mae: 0.2649\n",
      "Epoch 41/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0821 - mae: 0.2326 - val_loss: 0.0462 - val_mae: 0.2008\n",
      "Epoch 42/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0717 - mae: 0.2183 - val_loss: 0.0247 - val_mae: 0.1374\n",
      "Epoch 43/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0724 - mae: 0.2250 - val_loss: 0.1019 - val_mae: 0.3098\n",
      "Epoch 44/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0747 - mae: 0.2235 - val_loss: 0.0641 - val_mae: 0.2414\n",
      "Epoch 45/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0761 - mae: 0.2236 - val_loss: 0.0403 - val_mae: 0.1858\n",
      "Epoch 46/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0760 - mae: 0.2350 - val_loss: 0.0617 - val_mae: 0.2363\n",
      "Epoch 47/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0669 - mae: 0.2153 - val_loss: 0.0399 - val_mae: 0.1845\n",
      "Epoch 48/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0830 - mae: 0.2420 - val_loss: 0.0968 - val_mae: 0.3014\n",
      "Epoch 49/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0883 - mae: 0.2482 - val_loss: 0.0590 - val_mae: 0.2306\n",
      "Epoch 50/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0780 - mae: 0.2363 - val_loss: 0.0690 - val_mae: 0.2512\n",
      "Epoch 51/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0692 - mae: 0.2175 - val_loss: 0.0253 - val_mae: 0.1394\n",
      "Epoch 52/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0898 - mae: 0.2506 - val_loss: 0.0387 - val_mae: 0.1811\n",
      "Epoch 53/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0677 - mae: 0.2130 - val_loss: 0.0787 - val_mae: 0.2697\n",
      "Epoch 54/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0819 - mae: 0.2379 - val_loss: 0.0120 - val_mae: 0.0843\n",
      "Epoch 55/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0848 - mae: 0.2384 - val_loss: 0.0886 - val_mae: 0.2874\n",
      "Epoch 56/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0781 - mae: 0.2307 - val_loss: 0.0819 - val_mae: 0.2755\n",
      "Epoch 57/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0748 - mae: 0.2286 - val_loss: 0.0075 - val_mae: 0.0678\n",
      "Epoch 58/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0961 - mae: 0.2503 - val_loss: 0.0978 - val_mae: 0.3032\n",
      "Epoch 59/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0749 - mae: 0.2288 - val_loss: 0.0864 - val_mae: 0.2838\n",
      "Epoch 60/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0641 - mae: 0.2106 - val_loss: 0.0220 - val_mae: 0.1275\n",
      "Epoch 61/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0930 - mae: 0.2541 - val_loss: 0.0710 - val_mae: 0.2554\n",
      "Epoch 62/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0767 - mae: 0.2348 - val_loss: 0.0781 - val_mae: 0.2689\n",
      "Epoch 63/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0717 - mae: 0.2224 - val_loss: 0.0325 - val_mae: 0.1633\n",
      "Epoch 64/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0717 - mae: 0.2257 - val_loss: 0.0592 - val_mae: 0.2310\n",
      "Epoch 65/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0831 - mae: 0.2442 - val_loss: 0.1039 - val_mae: 0.3130\n",
      "Epoch 66/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0857 - mae: 0.2416 - val_loss: 0.0588 - val_mae: 0.2301\n",
      "Epoch 67/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0789 - mae: 0.2358 - val_loss: 0.0589 - val_mae: 0.2304\n",
      "Epoch 68/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0789 - mae: 0.2344 - val_loss: 0.0466 - val_mae: 0.2020\n",
      "Epoch 69/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0708 - mae: 0.2205 - val_loss: 0.0433 - val_mae: 0.1937\n",
      "Epoch 70/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0779 - mae: 0.2292 - val_loss: 0.0459 - val_mae: 0.2002\n",
      "Epoch 71/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0797 - mae: 0.2362 - val_loss: 0.0887 - val_mae: 0.2878\n",
      "Epoch 72/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0754 - mae: 0.2371 - val_loss: 0.0508 - val_mae: 0.2122\n",
      "Epoch 73/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0682 - mae: 0.2156 - val_loss: 0.0560 - val_mae: 0.2241\n",
      "Epoch 74/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0801 - mae: 0.2402 - val_loss: 0.0796 - val_mae: 0.2716\n",
      "Epoch 75/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0851 - mae: 0.2446 - val_loss: 0.0857 - val_mae: 0.2826\n",
      "Epoch 76/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0760 - mae: 0.2296 - val_loss: 0.0682 - val_mae: 0.2499\n",
      "Epoch 77/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0694 - mae: 0.2083 - val_loss: 0.0266 - val_mae: 0.1447\n",
      "Epoch 78/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0753 - mae: 0.2288 - val_loss: 0.1405 - val_mae: 0.3670\n",
      "Epoch 79/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0857 - mae: 0.2472 - val_loss: 0.0921 - val_mae: 0.2940\n",
      "Epoch 80/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0819 - mae: 0.2420 - val_loss: 0.0495 - val_mae: 0.2094\n",
      "Epoch 81/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0702 - mae: 0.2210 - val_loss: 0.0949 - val_mae: 0.2987\n",
      "Epoch 82/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0785 - mae: 0.2351 - val_loss: 0.0633 - val_mae: 0.2398\n",
      "Epoch 83/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0703 - mae: 0.2248 - val_loss: 0.0662 - val_mae: 0.2459\n",
      "Epoch 84/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0717 - mae: 0.2224 - val_loss: 0.0504 - val_mae: 0.2114\n",
      "Epoch 85/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0684 - mae: 0.2140 - val_loss: 0.0707 - val_mae: 0.2547\n",
      "Epoch 86/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0688 - mae: 0.2205 - val_loss: 0.0233 - val_mae: 0.1324\n",
      "Epoch 87/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0899 - mae: 0.2459 - val_loss: 0.1162 - val_mae: 0.3321\n",
      "Epoch 88/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0855 - mae: 0.2517 - val_loss: 0.0597 - val_mae: 0.2322\n",
      "Epoch 89/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0738 - mae: 0.2199 - val_loss: 0.0525 - val_mae: 0.2161\n",
      "Epoch 90/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0694 - mae: 0.2185 - val_loss: 0.0580 - val_mae: 0.2286\n",
      "Epoch 91/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0671 - mae: 0.2169 - val_loss: 0.0601 - val_mae: 0.2331\n",
      "Epoch 92/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0805 - mae: 0.2377 - val_loss: 0.0267 - val_mae: 0.1449\n",
      "Epoch 93/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0771 - mae: 0.2323 - val_loss: 0.1032 - val_mae: 0.3121\n",
      "Epoch 94/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0724 - mae: 0.2191 - val_loss: 0.0268 - val_mae: 0.1453\n",
      "Epoch 95/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0722 - mae: 0.2267 - val_loss: 0.0514 - val_mae: 0.2137\n",
      "Epoch 96/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0716 - mae: 0.2211 - val_loss: 0.1087 - val_mae: 0.3210\n",
      "Epoch 97/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0705 - mae: 0.2257 - val_loss: 0.0579 - val_mae: 0.2285\n",
      "Epoch 98/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0761 - mae: 0.2345 - val_loss: 0.0580 - val_mae: 0.2288\n",
      "Epoch 99/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0626 - mae: 0.2071 - val_loss: 0.0638 - val_mae: 0.2412\n",
      "Epoch 100/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0765 - mae: 0.2313 - val_loss: 0.0899 - val_mae: 0.2903\n",
      "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "âœ… Done with capsicum_Dharwad_daily.csv | MAE=887.05, RMSE=1023.51, R2=0.35, MAPE=50.23%, Accuracy=49.77%\n",
      "\n",
      "ğŸš€ Processing: capsicum_Hassan_daily.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\2878872630.py:115: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Average Price'].fillna(df['Average Price'].mean(), inplace=True)\n",
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\2878872630.py:120: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['MA_7'].fillna(df['MA_7'].mean(), inplace=True)\n",
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\2878872630.py:121: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['MA_30'].fillna(df['MA_30'].mean(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 12ms/step - loss: 0.2851 - mae: 0.2948 - val_loss: 0.0306 - val_mae: 0.1296\n",
      "Epoch 2/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 0.0140 - mae: 0.0930 - val_loss: 0.0415 - val_mae: 0.1402\n",
      "Epoch 3/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 0.0077 - mae: 0.0689 - val_loss: 0.0296 - val_mae: 0.1250\n",
      "Epoch 4/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 0.0050 - mae: 0.0549 - val_loss: 0.0355 - val_mae: 0.1291\n",
      "Epoch 5/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 0.0041 - mae: 0.0489 - val_loss: 0.0293 - val_mae: 0.1218\n",
      "Epoch 6/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 0.0033 - mae: 0.0443 - val_loss: 0.0262 - val_mae: 0.1224\n",
      "Epoch 7/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 0.0029 - mae: 0.0415 - val_loss: 0.0251 - val_mae: 0.1217\n",
      "Epoch 8/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0024 - mae: 0.0376 - val_loss: 0.0233 - val_mae: 0.1225\n",
      "Epoch 9/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0021 - mae: 0.0345 - val_loss: 0.0226 - val_mae: 0.1276\n",
      "Epoch 10/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0019 - mae: 0.0323 - val_loss: 0.0230 - val_mae: 0.1234\n",
      "Epoch 11/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 0.0016 - mae: 0.0291 - val_loss: 0.0265 - val_mae: 0.1294\n",
      "Epoch 12/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0013 - mae: 0.0262 - val_loss: 0.0404 - val_mae: 0.1454\n",
      "Epoch 13/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 0.0015 - mae: 0.0282 - val_loss: 0.0439 - val_mae: 0.1468\n",
      "Epoch 14/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 0.0013 - mae: 0.0262 - val_loss: 0.0333 - val_mae: 0.1335\n",
      "Epoch 15/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0011 - mae: 0.0243 - val_loss: 0.0475 - val_mae: 0.1490\n",
      "Epoch 16/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0012 - mae: 0.0251 - val_loss: 0.0360 - val_mae: 0.1357\n",
      "Epoch 17/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 0.0012 - mae: 0.0242 - val_loss: 0.0444 - val_mae: 0.1497\n",
      "Epoch 18/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 0.0013 - mae: 0.0270 - val_loss: 0.0396 - val_mae: 0.1429\n",
      "Epoch 19/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 0.0012 - mae: 0.0240 - val_loss: 0.0375 - val_mae: 0.1387\n",
      "Epoch 20/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 0.0011 - mae: 0.0233 - val_loss: 0.0332 - val_mae: 0.1348\n",
      "Epoch 21/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 0.0011 - mae: 0.0234 - val_loss: 0.0381 - val_mae: 0.1435\n",
      "Epoch 22/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0011 - mae: 0.0244 - val_loss: 0.0415 - val_mae: 0.1479\n",
      "Epoch 23/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 0.0011 - mae: 0.0238 - val_loss: 0.0305 - val_mae: 0.1340\n",
      "Epoch 24/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0011 - mae: 0.0227 - val_loss: 0.0324 - val_mae: 0.1385\n",
      "Epoch 25/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 0.0011 - mae: 0.0239 - val_loss: 0.0258 - val_mae: 0.1296\n",
      "Epoch 26/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 0.0010 - mae: 0.0221 - val_loss: 0.0299 - val_mae: 0.1354\n",
      "Epoch 27/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 9.7288e-04 - mae: 0.0212 - val_loss: 0.0336 - val_mae: 0.1418\n",
      "Epoch 28/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 0.0010 - mae: 0.0223 - val_loss: 0.0315 - val_mae: 0.1392\n",
      "Epoch 29/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 9.3176e-04 - mae: 0.0212 - val_loss: 0.0283 - val_mae: 0.1348\n",
      "Epoch 30/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 9.8661e-04 - mae: 0.0223 - val_loss: 0.0339 - val_mae: 0.1438\n",
      "Epoch 31/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 0.0010 - mae: 0.0226 - val_loss: 0.0331 - val_mae: 0.1419\n",
      "Epoch 32/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 9.4416e-04 - mae: 0.0217 - val_loss: 0.0333 - val_mae: 0.1409\n",
      "Epoch 33/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 9.7260e-04 - mae: 0.0219 - val_loss: 0.0309 - val_mae: 0.1408\n",
      "Epoch 34/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 8.9918e-04 - mae: 0.0207 - val_loss: 0.0276 - val_mae: 0.1328\n",
      "Epoch 35/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 9.1471e-04 - mae: 0.0216 - val_loss: 0.0280 - val_mae: 0.1363\n",
      "Epoch 36/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 9.6326e-04 - mae: 0.0218 - val_loss: 0.0308 - val_mae: 0.1392\n",
      "Epoch 37/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 9.0185e-04 - mae: 0.0209 - val_loss: 0.0324 - val_mae: 0.1409\n",
      "Epoch 38/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 9.3039e-04 - mae: 0.0206 - val_loss: 0.0373 - val_mae: 0.1480\n",
      "Epoch 39/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 9.7162e-04 - mae: 0.0222 - val_loss: 0.0328 - val_mae: 0.1405\n",
      "Epoch 40/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 9.3652e-04 - mae: 0.0216 - val_loss: 0.0297 - val_mae: 0.1380\n",
      "Epoch 41/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 9.2197e-04 - mae: 0.0215 - val_loss: 0.0312 - val_mae: 0.1399\n",
      "Epoch 42/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 0.0010 - mae: 0.0222 - val_loss: 0.0282 - val_mae: 0.1370\n",
      "Epoch 43/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 9.5788e-04 - mae: 0.0219 - val_loss: 0.0307 - val_mae: 0.1394\n",
      "Epoch 44/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 8.7475e-04 - mae: 0.0203 - val_loss: 0.0278 - val_mae: 0.1351\n",
      "Epoch 45/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 9.3448e-04 - mae: 0.0204 - val_loss: 0.0282 - val_mae: 0.1365\n",
      "Epoch 46/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 8.8949e-04 - mae: 0.0208 - val_loss: 0.0324 - val_mae: 0.1435\n",
      "Epoch 47/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 9.0462e-04 - mae: 0.0208 - val_loss: 0.0289 - val_mae: 0.1374\n",
      "Epoch 48/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 8.5619e-04 - mae: 0.0202 - val_loss: 0.0349 - val_mae: 0.1465\n",
      "Epoch 49/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 9.1717e-04 - mae: 0.0218 - val_loss: 0.0317 - val_mae: 0.1407\n",
      "Epoch 50/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 8.8473e-04 - mae: 0.0207 - val_loss: 0.0307 - val_mae: 0.1417\n",
      "Epoch 51/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 8.6777e-04 - mae: 0.0203 - val_loss: 0.0317 - val_mae: 0.1419\n",
      "Epoch 52/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 8.5407e-04 - mae: 0.0201 - val_loss: 0.0310 - val_mae: 0.1423\n",
      "Epoch 53/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 8.1400e-04 - mae: 0.0196 - val_loss: 0.0270 - val_mae: 0.1343\n",
      "Epoch 54/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 8.8841e-04 - mae: 0.0211 - val_loss: 0.0309 - val_mae: 0.1413\n",
      "Epoch 55/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 9.0286e-04 - mae: 0.0208 - val_loss: 0.0250 - val_mae: 0.1307\n",
      "Epoch 56/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 8.7293e-04 - mae: 0.0207 - val_loss: 0.0291 - val_mae: 0.1367\n",
      "Epoch 57/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 9.0138e-04 - mae: 0.0212 - val_loss: 0.0295 - val_mae: 0.1405\n",
      "Epoch 58/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 8.7752e-04 - mae: 0.0203 - val_loss: 0.0309 - val_mae: 0.1403\n",
      "Epoch 59/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 8.8657e-04 - mae: 0.0209 - val_loss: 0.0315 - val_mae: 0.1422\n",
      "Epoch 60/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 8.4645e-04 - mae: 0.0198 - val_loss: 0.0322 - val_mae: 0.1435\n",
      "Epoch 61/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 8.8416e-04 - mae: 0.0209 - val_loss: 0.0293 - val_mae: 0.1399\n",
      "Epoch 62/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 8.6531e-04 - mae: 0.0198 - val_loss: 0.0290 - val_mae: 0.1382\n",
      "Epoch 63/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 8.5785e-04 - mae: 0.0201 - val_loss: 0.0282 - val_mae: 0.1383\n",
      "Epoch 64/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 8.4112e-04 - mae: 0.0196 - val_loss: 0.0306 - val_mae: 0.1418\n",
      "Epoch 65/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 8.6884e-04 - mae: 0.0202 - val_loss: 0.0305 - val_mae: 0.1411\n",
      "Epoch 66/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 9.3311e-04 - mae: 0.0209 - val_loss: 0.0263 - val_mae: 0.1349\n",
      "Epoch 67/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 9.0754e-04 - mae: 0.0208 - val_loss: 0.0330 - val_mae: 0.1444\n",
      "Epoch 68/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 8.8393e-04 - mae: 0.0205 - val_loss: 0.0303 - val_mae: 0.1413\n",
      "Epoch 69/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 8.4307e-04 - mae: 0.0200 - val_loss: 0.0319 - val_mae: 0.1435\n",
      "Epoch 70/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 8.5885e-04 - mae: 0.0198 - val_loss: 0.0294 - val_mae: 0.1391\n",
      "Epoch 71/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 8.4230e-04 - mae: 0.0200 - val_loss: 0.0283 - val_mae: 0.1360\n",
      "Epoch 72/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 8.9614e-04 - mae: 0.0207 - val_loss: 0.0292 - val_mae: 0.1400\n",
      "Epoch 73/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 8.6770e-04 - mae: 0.0204 - val_loss: 0.0315 - val_mae: 0.1443\n",
      "Epoch 74/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 8.3547e-04 - mae: 0.0199 - val_loss: 0.0252 - val_mae: 0.1334\n",
      "Epoch 75/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 7.8100e-04 - mae: 0.0189 - val_loss: 0.0286 - val_mae: 0.1387\n",
      "Epoch 76/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 8.5703e-04 - mae: 0.0197 - val_loss: 0.0302 - val_mae: 0.1431\n",
      "Epoch 77/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 8.9079e-04 - mae: 0.0206 - val_loss: 0.0307 - val_mae: 0.1428\n",
      "Epoch 78/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 8.1252e-04 - mae: 0.0191 - val_loss: 0.0274 - val_mae: 0.1364\n",
      "Epoch 79/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 8.6277e-04 - mae: 0.0205 - val_loss: 0.0311 - val_mae: 0.1411\n",
      "Epoch 80/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 8.3467e-04 - mae: 0.0196 - val_loss: 0.0270 - val_mae: 0.1370\n",
      "Epoch 81/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 8.2293e-04 - mae: 0.0199 - val_loss: 0.0321 - val_mae: 0.1435\n",
      "Epoch 82/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 8.8010e-04 - mae: 0.0205 - val_loss: 0.0302 - val_mae: 0.1405\n",
      "Epoch 83/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 8.5094e-04 - mae: 0.0203 - val_loss: 0.0300 - val_mae: 0.1413\n",
      "Epoch 84/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 8.0161e-04 - mae: 0.0195 - val_loss: 0.0276 - val_mae: 0.1368\n",
      "Epoch 85/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 8.9749e-04 - mae: 0.0204 - val_loss: 0.0300 - val_mae: 0.1405\n",
      "Epoch 86/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 8.0917e-04 - mae: 0.0193 - val_loss: 0.0302 - val_mae: 0.1409\n",
      "Epoch 87/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 7.8318e-04 - mae: 0.0188 - val_loss: 0.0249 - val_mae: 0.1316\n",
      "Epoch 88/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 8.7650e-04 - mae: 0.0201 - val_loss: 0.0284 - val_mae: 0.1387\n",
      "Epoch 89/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 7.8445e-04 - mae: 0.0187 - val_loss: 0.0265 - val_mae: 0.1351\n",
      "Epoch 90/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 8.3796e-04 - mae: 0.0202 - val_loss: 0.0303 - val_mae: 0.1413\n",
      "Epoch 91/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 8.6410e-04 - mae: 0.0200 - val_loss: 0.0289 - val_mae: 0.1386\n",
      "Epoch 92/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 8.5811e-04 - mae: 0.0199 - val_loss: 0.0327 - val_mae: 0.1442\n",
      "Epoch 93/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 8.4534e-04 - mae: 0.0202 - val_loss: 0.0310 - val_mae: 0.1420\n",
      "Epoch 94/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 8.0474e-04 - mae: 0.0188 - val_loss: 0.0306 - val_mae: 0.1415\n",
      "Epoch 95/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 8.3254e-04 - mae: 0.0192 - val_loss: 0.0291 - val_mae: 0.1388\n",
      "Epoch 96/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 8.6735e-04 - mae: 0.0202 - val_loss: 0.0292 - val_mae: 0.1394\n",
      "Epoch 97/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 8.1863e-04 - mae: 0.0194 - val_loss: 0.0329 - val_mae: 0.1445\n",
      "Epoch 98/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 8.5575e-04 - mae: 0.0199 - val_loss: 0.0254 - val_mae: 0.1317\n",
      "Epoch 99/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 8.2352e-04 - mae: 0.0195 - val_loss: 0.0308 - val_mae: 0.1411\n",
      "Epoch 100/100\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 8.8860e-04 - mae: 0.0200 - val_loss: 0.0260 - val_mae: 0.1338\n",
      "\u001b[1m217/217\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "âœ… Done with capsicum_Hassan_daily.csv | MAE=275.05, RMSE=476.32, R2=0.78, MAPE=12.54%, Accuracy=87.46%\n",
      "\n",
      "ğŸš€ Processing: capsicum_Kalburgi_daily.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\2878872630.py:115: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Average Price'].fillna(df['Average Price'].mean(), inplace=True)\n",
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\2878872630.py:120: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['MA_7'].fillna(df['MA_7'].mean(), inplace=True)\n",
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\2878872630.py:121: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['MA_30'].fillna(df['MA_30'].mean(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - loss: 0.8029 - mae: 0.6300 - val_loss: 0.0172 - val_mae: 0.0790\n",
      "Epoch 2/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0261 - mae: 0.1232 - val_loss: 0.0186 - val_mae: 0.0850\n",
      "Epoch 3/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0244 - mae: 0.1226 - val_loss: 0.0129 - val_mae: 0.0723\n",
      "Epoch 4/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0211 - mae: 0.1118 - val_loss: 0.0135 - val_mae: 0.0698\n",
      "Epoch 5/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0217 - mae: 0.1199 - val_loss: 0.0133 - val_mae: 0.0700\n",
      "Epoch 6/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0212 - mae: 0.1117 - val_loss: 0.0227 - val_mae: 0.1041\n",
      "Epoch 7/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0242 - mae: 0.1213 - val_loss: 0.0230 - val_mae: 0.1059\n",
      "Epoch 8/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0196 - mae: 0.1102 - val_loss: 0.0126 - val_mae: 0.0726\n",
      "Epoch 9/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0204 - mae: 0.1082 - val_loss: 0.0165 - val_mae: 0.0765\n",
      "Epoch 10/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0177 - mae: 0.1044 - val_loss: 0.0137 - val_mae: 0.0689\n",
      "Epoch 11/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0175 - mae: 0.1052 - val_loss: 0.0135 - val_mae: 0.0687\n",
      "Epoch 12/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0190 - mae: 0.1052 - val_loss: 0.0188 - val_mae: 0.0875\n",
      "Epoch 13/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0167 - mae: 0.1004 - val_loss: 0.0137 - val_mae: 0.0683\n",
      "Epoch 14/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0159 - mae: 0.0984 - val_loss: 0.0169 - val_mae: 0.0790\n",
      "Epoch 15/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0161 - mae: 0.0970 - val_loss: 0.0196 - val_mae: 0.0915\n",
      "Epoch 16/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0153 - mae: 0.0957 - val_loss: 0.0149 - val_mae: 0.0709\n",
      "Epoch 17/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0156 - mae: 0.0963 - val_loss: 0.0145 - val_mae: 0.0695\n",
      "Epoch 18/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0137 - mae: 0.0891 - val_loss: 0.0199 - val_mae: 0.0939\n",
      "Epoch 19/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0146 - mae: 0.0943 - val_loss: 0.0116 - val_mae: 0.0767\n",
      "Epoch 20/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0158 - mae: 0.0948 - val_loss: 0.0199 - val_mae: 0.0943\n",
      "Epoch 21/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0145 - mae: 0.0940 - val_loss: 0.0177 - val_mae: 0.0842\n",
      "Epoch 22/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0132 - mae: 0.0890 - val_loss: 0.0127 - val_mae: 0.0663\n",
      "Epoch 23/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0145 - mae: 0.0891 - val_loss: 0.0115 - val_mae: 0.0761\n",
      "Epoch 24/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0126 - mae: 0.0857 - val_loss: 0.0116 - val_mae: 0.0738\n",
      "Epoch 25/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0156 - mae: 0.0922 - val_loss: 0.0271 - val_mae: 0.1262\n",
      "Epoch 26/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0147 - mae: 0.0898 - val_loss: 0.0205 - val_mae: 0.0985\n",
      "Epoch 27/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0122 - mae: 0.0849 - val_loss: 0.0122 - val_mae: 0.0661\n",
      "Epoch 28/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0120 - mae: 0.0830 - val_loss: 0.0169 - val_mae: 0.0820\n",
      "Epoch 29/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0129 - mae: 0.0853 - val_loss: 0.0235 - val_mae: 0.1123\n",
      "Epoch 30/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0130 - mae: 0.0861 - val_loss: 0.0235 - val_mae: 0.1124\n",
      "Epoch 31/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0111 - mae: 0.0816 - val_loss: 0.0120 - val_mae: 0.0658\n",
      "Epoch 32/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0120 - mae: 0.0773 - val_loss: 0.0138 - val_mae: 0.0697\n",
      "Epoch 33/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0111 - mae: 0.0794 - val_loss: 0.0119 - val_mae: 0.0658\n",
      "Epoch 34/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0117 - mae: 0.0796 - val_loss: 0.0175 - val_mae: 0.0867\n",
      "Epoch 35/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0117 - mae: 0.0813 - val_loss: 0.0117 - val_mae: 0.0900\n",
      "Epoch 36/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0130 - mae: 0.0859 - val_loss: 0.0171 - val_mae: 0.0857\n",
      "Epoch 37/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0117 - mae: 0.0797 - val_loss: 0.0111 - val_mae: 0.0700\n",
      "Epoch 38/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0112 - mae: 0.0754 - val_loss: 0.0127 - val_mae: 0.0977\n",
      "Epoch 39/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0112 - mae: 0.0782 - val_loss: 0.0178 - val_mae: 0.0890\n",
      "Epoch 40/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0106 - mae: 0.0754 - val_loss: 0.0113 - val_mae: 0.0844\n",
      "Epoch 41/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0117 - mae: 0.0792 - val_loss: 0.0149 - val_mae: 0.0769\n",
      "Epoch 42/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0107 - mae: 0.0747 - val_loss: 0.0131 - val_mae: 0.0697\n",
      "Epoch 43/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0098 - mae: 0.0736 - val_loss: 0.0110 - val_mae: 0.0787\n",
      "Epoch 44/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0115 - mae: 0.0782 - val_loss: 0.0111 - val_mae: 0.0687\n",
      "Epoch 45/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0105 - mae: 0.0741 - val_loss: 0.0132 - val_mae: 0.0703\n",
      "Epoch 46/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0098 - mae: 0.0723 - val_loss: 0.0112 - val_mae: 0.0826\n",
      "Epoch 47/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0086 - mae: 0.0669 - val_loss: 0.0110 - val_mae: 0.0790\n",
      "Epoch 48/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0101 - mae: 0.0727 - val_loss: 0.0121 - val_mae: 0.0909\n",
      "Epoch 49/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0117 - mae: 0.0744 - val_loss: 0.0117 - val_mae: 0.0670\n",
      "Epoch 50/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0096 - mae: 0.0710 - val_loss: 0.0114 - val_mae: 0.0840\n",
      "Epoch 51/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0098 - mae: 0.0712 - val_loss: 0.0113 - val_mae: 0.0684\n",
      "Epoch 52/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0084 - mae: 0.0654 - val_loss: 0.0124 - val_mae: 0.0929\n",
      "Epoch 53/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0102 - mae: 0.0728 - val_loss: 0.0119 - val_mae: 0.0681\n",
      "Epoch 54/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0108 - mae: 0.0781 - val_loss: 0.0112 - val_mae: 0.0696\n",
      "Epoch 55/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0111 - mae: 0.0755 - val_loss: 0.0113 - val_mae: 0.0823\n",
      "Epoch 56/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0108 - mae: 0.0749 - val_loss: 0.0112 - val_mae: 0.0805\n",
      "Epoch 57/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0112 - mae: 0.0774 - val_loss: 0.0121 - val_mae: 0.0892\n",
      "Epoch 58/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0099 - mae: 0.0718 - val_loss: 0.0116 - val_mae: 0.0852\n",
      "Epoch 59/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0102 - mae: 0.0720 - val_loss: 0.0139 - val_mae: 0.1020\n",
      "Epoch 60/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0095 - mae: 0.0695 - val_loss: 0.0116 - val_mae: 0.0849\n",
      "Epoch 61/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0097 - mae: 0.0690 - val_loss: 0.0114 - val_mae: 0.0808\n",
      "Epoch 62/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0093 - mae: 0.0694 - val_loss: 0.0120 - val_mae: 0.0670\n",
      "Epoch 63/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0133 - mae: 0.0863 - val_loss: 0.0118 - val_mae: 0.0859\n",
      "Epoch 64/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0095 - mae: 0.0698 - val_loss: 0.0141 - val_mae: 0.1022\n",
      "Epoch 65/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0113 - mae: 0.0785 - val_loss: 0.0113 - val_mae: 0.0699\n",
      "Epoch 66/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0101 - mae: 0.0712 - val_loss: 0.0121 - val_mae: 0.0887\n",
      "Epoch 67/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0101 - mae: 0.0721 - val_loss: 0.0127 - val_mae: 0.0934\n",
      "Epoch 68/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0114 - mae: 0.0753 - val_loss: 0.0112 - val_mae: 0.0723\n",
      "Epoch 69/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0094 - mae: 0.0713 - val_loss: 0.0127 - val_mae: 0.0920\n",
      "Epoch 70/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0090 - mae: 0.0677 - val_loss: 0.0198 - val_mae: 0.1275\n",
      "Epoch 71/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0105 - mae: 0.0747 - val_loss: 0.0121 - val_mae: 0.0891\n",
      "Epoch 72/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0106 - mae: 0.0705 - val_loss: 0.0113 - val_mae: 0.0751\n",
      "Epoch 73/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0092 - mae: 0.0712 - val_loss: 0.0165 - val_mae: 0.1143\n",
      "Epoch 74/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0100 - mae: 0.0700 - val_loss: 0.0118 - val_mae: 0.0689\n",
      "Epoch 75/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0095 - mae: 0.0710 - val_loss: 0.0118 - val_mae: 0.0846\n",
      "Epoch 76/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0091 - mae: 0.0672 - val_loss: 0.0144 - val_mae: 0.1036\n",
      "Epoch 77/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0099 - mae: 0.0720 - val_loss: 0.0113 - val_mae: 0.0739\n",
      "Epoch 78/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0105 - mae: 0.0734 - val_loss: 0.0124 - val_mae: 0.0914\n",
      "Epoch 79/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0088 - mae: 0.0672 - val_loss: 0.0116 - val_mae: 0.0834\n",
      "Epoch 80/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0089 - mae: 0.0661 - val_loss: 0.0118 - val_mae: 0.0847\n",
      "Epoch 81/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0095 - mae: 0.0696 - val_loss: 0.0151 - val_mae: 0.1075\n",
      "Epoch 82/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0105 - mae: 0.0751 - val_loss: 0.0154 - val_mae: 0.1084\n",
      "Epoch 83/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0101 - mae: 0.0724 - val_loss: 0.0135 - val_mae: 0.0986\n",
      "Epoch 84/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0116 - mae: 0.0764 - val_loss: 0.0159 - val_mae: 0.1111\n",
      "Epoch 85/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0096 - mae: 0.0716 - val_loss: 0.0125 - val_mae: 0.0910\n",
      "Epoch 86/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0091 - mae: 0.0693 - val_loss: 0.0114 - val_mae: 0.0704\n",
      "Epoch 87/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0100 - mae: 0.0714 - val_loss: 0.0158 - val_mae: 0.1100\n",
      "Epoch 88/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0088 - mae: 0.0656 - val_loss: 0.0157 - val_mae: 0.1096\n",
      "Epoch 89/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0117 - mae: 0.0805 - val_loss: 0.0115 - val_mae: 0.0719\n",
      "Epoch 90/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0099 - mae: 0.0732 - val_loss: 0.0117 - val_mae: 0.0843\n",
      "Epoch 91/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0093 - mae: 0.0672 - val_loss: 0.0121 - val_mae: 0.0708\n",
      "Epoch 92/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0097 - mae: 0.0717 - val_loss: 0.0139 - val_mae: 0.1005\n",
      "Epoch 93/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0095 - mae: 0.0720 - val_loss: 0.0124 - val_mae: 0.0690\n",
      "Epoch 94/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0099 - mae: 0.0717 - val_loss: 0.0114 - val_mae: 0.0742\n",
      "Epoch 95/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0101 - mae: 0.0736 - val_loss: 0.0134 - val_mae: 0.0980\n",
      "Epoch 96/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0114 - mae: 0.0760 - val_loss: 0.0114 - val_mae: 0.0716\n",
      "Epoch 97/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0085 - mae: 0.0653 - val_loss: 0.0176 - val_mae: 0.1195\n",
      "Epoch 98/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0104 - mae: 0.0744 - val_loss: 0.0114 - val_mae: 0.0801\n",
      "Epoch 99/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0114 - mae: 0.0757 - val_loss: 0.0113 - val_mae: 0.0757\n",
      "Epoch 100/100\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0110 - mae: 0.0757 - val_loss: 0.0125 - val_mae: 0.0908\n",
      "\u001b[1m36/36\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step\n",
      "âœ… Done with capsicum_Kalburgi_daily.csv | MAE=529.9, RMSE=702.05, R2=0.67, MAPE=26.25%, Accuracy=73.75%\n",
      "\n",
      "ğŸš€ Processing: capsicum_Kolar_daily.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\2878872630.py:115: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Average Price'].fillna(df['Average Price'].mean(), inplace=True)\n",
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\2878872630.py:120: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['MA_7'].fillna(df['MA_7'].mean(), inplace=True)\n",
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\2878872630.py:121: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['MA_30'].fillna(df['MA_30'].mean(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 7ms/step - loss: 0.0319 - mae: 0.1120 - val_loss: 0.0021 - val_mae: 0.0330\n",
      "Epoch 2/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - loss: 0.0086 - mae: 0.0682 - val_loss: 0.0032 - val_mae: 0.0444\n",
      "Epoch 3/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9ms/step - loss: 0.0081 - mae: 0.0653 - val_loss: 0.0043 - val_mae: 0.0559\n",
      "Epoch 4/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - loss: 0.0081 - mae: 0.0656 - val_loss: 0.0022 - val_mae: 0.0363\n",
      "Epoch 5/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9ms/step - loss: 0.0076 - mae: 0.0627 - val_loss: 0.0020 - val_mae: 0.0325\n",
      "Epoch 6/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 0.0077 - mae: 0.0627 - val_loss: 0.0021 - val_mae: 0.0361\n",
      "Epoch 7/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - loss: 0.0076 - mae: 0.0623 - val_loss: 0.0028 - val_mae: 0.0433\n",
      "Epoch 8/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - loss: 0.0076 - mae: 0.0618 - val_loss: 0.0021 - val_mae: 0.0361\n",
      "Epoch 9/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - loss: 0.0075 - mae: 0.0616 - val_loss: 0.0018 - val_mae: 0.0322\n",
      "Epoch 10/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - loss: 0.0077 - mae: 0.0626 - val_loss: 0.0017 - val_mae: 0.0310\n",
      "Epoch 11/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 0.0074 - mae: 0.0612 - val_loss: 0.0017 - val_mae: 0.0309\n",
      "Epoch 12/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - loss: 0.0076 - mae: 0.0614 - val_loss: 0.0019 - val_mae: 0.0341\n",
      "Epoch 13/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - loss: 0.0073 - mae: 0.0605 - val_loss: 0.0017 - val_mae: 0.0314\n",
      "Epoch 14/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - loss: 0.0073 - mae: 0.0599 - val_loss: 0.0026 - val_mae: 0.0411\n",
      "Epoch 15/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9ms/step - loss: 0.0074 - mae: 0.0603 - val_loss: 0.0017 - val_mae: 0.0309\n",
      "Epoch 16/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - loss: 0.0072 - mae: 0.0600 - val_loss: 0.0018 - val_mae: 0.0313\n",
      "Epoch 17/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - loss: 0.0073 - mae: 0.0602 - val_loss: 0.0020 - val_mae: 0.0340\n",
      "Epoch 18/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - loss: 0.0074 - mae: 0.0608 - val_loss: 0.0019 - val_mae: 0.0331\n",
      "Epoch 19/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - loss: 0.0072 - mae: 0.0593 - val_loss: 0.0017 - val_mae: 0.0310\n",
      "Epoch 20/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9ms/step - loss: 0.0073 - mae: 0.0599 - val_loss: 0.0026 - val_mae: 0.0405\n",
      "Epoch 21/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 0.0070 - mae: 0.0591 - val_loss: 0.0020 - val_mae: 0.0338\n",
      "Epoch 22/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - loss: 0.0071 - mae: 0.0584 - val_loss: 0.0018 - val_mae: 0.0320\n",
      "Epoch 23/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - loss: 0.0072 - mae: 0.0593 - val_loss: 0.0019 - val_mae: 0.0340\n",
      "Epoch 24/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - loss: 0.0073 - mae: 0.0598 - val_loss: 0.0017 - val_mae: 0.0312\n",
      "Epoch 25/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9ms/step - loss: 0.0072 - mae: 0.0596 - val_loss: 0.0022 - val_mae: 0.0368\n",
      "Epoch 26/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - loss: 0.0071 - mae: 0.0587 - val_loss: 0.0017 - val_mae: 0.0309\n",
      "Epoch 27/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - loss: 0.0072 - mae: 0.0588 - val_loss: 0.0017 - val_mae: 0.0311\n",
      "Epoch 28/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - loss: 0.0072 - mae: 0.0590 - val_loss: 0.0017 - val_mae: 0.0309\n",
      "Epoch 29/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - loss: 0.0073 - mae: 0.0592 - val_loss: 0.0017 - val_mae: 0.0308\n",
      "Epoch 30/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - loss: 0.0071 - mae: 0.0585 - val_loss: 0.0030 - val_mae: 0.0441\n",
      "Epoch 31/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - loss: 0.0074 - mae: 0.0597 - val_loss: 0.0017 - val_mae: 0.0310\n",
      "Epoch 32/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 0.0072 - mae: 0.0593 - val_loss: 0.0017 - val_mae: 0.0311\n",
      "Epoch 33/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9ms/step - loss: 0.0073 - mae: 0.0597 - val_loss: 0.0017 - val_mae: 0.0311\n",
      "Epoch 34/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9ms/step - loss: 0.0071 - mae: 0.0589 - val_loss: 0.0017 - val_mae: 0.0316\n",
      "Epoch 35/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - loss: 0.0070 - mae: 0.0585 - val_loss: 0.0017 - val_mae: 0.0309\n",
      "Epoch 36/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - loss: 0.0073 - mae: 0.0590 - val_loss: 0.0019 - val_mae: 0.0331\n",
      "Epoch 37/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - loss: 0.0073 - mae: 0.0603 - val_loss: 0.0017 - val_mae: 0.0309\n",
      "Epoch 38/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - loss: 0.0072 - mae: 0.0598 - val_loss: 0.0017 - val_mae: 0.0309\n",
      "Epoch 39/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9ms/step - loss: 0.0071 - mae: 0.0587 - val_loss: 0.0018 - val_mae: 0.0315\n",
      "Epoch 40/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - loss: 0.0074 - mae: 0.0600 - val_loss: 0.0017 - val_mae: 0.0311\n",
      "Epoch 41/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - loss: 0.0072 - mae: 0.0594 - val_loss: 0.0018 - val_mae: 0.0317\n",
      "Epoch 42/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0070 - mae: 0.0584 - val_loss: 0.0017 - val_mae: 0.0311\n",
      "Epoch 43/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0072 - mae: 0.0593 - val_loss: 0.0018 - val_mae: 0.0320\n",
      "Epoch 44/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - loss: 0.0071 - mae: 0.0585 - val_loss: 0.0017 - val_mae: 0.0309\n",
      "Epoch 45/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0071 - mae: 0.0591 - val_loss: 0.0018 - val_mae: 0.0317\n",
      "Epoch 46/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - loss: 0.0071 - mae: 0.0585 - val_loss: 0.0017 - val_mae: 0.0311\n",
      "Epoch 47/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - loss: 0.0070 - mae: 0.0583 - val_loss: 0.0017 - val_mae: 0.0315\n",
      "Epoch 48/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0072 - mae: 0.0595 - val_loss: 0.0018 - val_mae: 0.0317\n",
      "Epoch 49/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - loss: 0.0072 - mae: 0.0595 - val_loss: 0.0017 - val_mae: 0.0309\n",
      "Epoch 50/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - loss: 0.0071 - mae: 0.0588 - val_loss: 0.0017 - val_mae: 0.0310\n",
      "Epoch 51/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - loss: 0.0073 - mae: 0.0594 - val_loss: 0.0021 - val_mae: 0.0350\n",
      "Epoch 52/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - loss: 0.0074 - mae: 0.0600 - val_loss: 0.0017 - val_mae: 0.0314\n",
      "Epoch 53/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - loss: 0.0072 - mae: 0.0594 - val_loss: 0.0017 - val_mae: 0.0310\n",
      "Epoch 54/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - loss: 0.0069 - mae: 0.0577 - val_loss: 0.0017 - val_mae: 0.0314\n",
      "Epoch 55/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0071 - mae: 0.0586 - val_loss: 0.0018 - val_mae: 0.0319\n",
      "Epoch 56/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - loss: 0.0071 - mae: 0.0586 - val_loss: 0.0018 - val_mae: 0.0319\n",
      "Epoch 57/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - loss: 0.0073 - mae: 0.0601 - val_loss: 0.0018 - val_mae: 0.0325\n",
      "Epoch 58/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0072 - mae: 0.0593 - val_loss: 0.0018 - val_mae: 0.0318\n",
      "Epoch 59/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0071 - mae: 0.0584 - val_loss: 0.0017 - val_mae: 0.0316\n",
      "Epoch 60/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - loss: 0.0072 - mae: 0.0593 - val_loss: 0.0018 - val_mae: 0.0320\n",
      "Epoch 61/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - loss: 0.0071 - mae: 0.0592 - val_loss: 0.0017 - val_mae: 0.0312\n",
      "Epoch 62/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9ms/step - loss: 0.0071 - mae: 0.0589 - val_loss: 0.0017 - val_mae: 0.0315\n",
      "Epoch 63/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 0.0072 - mae: 0.0585 - val_loss: 0.0017 - val_mae: 0.0313\n",
      "Epoch 64/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 0.0072 - mae: 0.0598 - val_loss: 0.0018 - val_mae: 0.0326\n",
      "Epoch 65/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 0.0075 - mae: 0.0604 - val_loss: 0.0017 - val_mae: 0.0313\n",
      "Epoch 66/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - loss: 0.0073 - mae: 0.0600 - val_loss: 0.0017 - val_mae: 0.0308\n",
      "Epoch 67/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 0.0074 - mae: 0.0598 - val_loss: 0.0017 - val_mae: 0.0310\n",
      "Epoch 68/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 0.0070 - mae: 0.0588 - val_loss: 0.0017 - val_mae: 0.0315\n",
      "Epoch 69/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - loss: 0.0072 - mae: 0.0596 - val_loss: 0.0017 - val_mae: 0.0312\n",
      "Epoch 70/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9ms/step - loss: 0.0072 - mae: 0.0590 - val_loss: 0.0018 - val_mae: 0.0319\n",
      "Epoch 71/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 0.0072 - mae: 0.0593 - val_loss: 0.0017 - val_mae: 0.0310\n",
      "Epoch 72/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 0.0072 - mae: 0.0597 - val_loss: 0.0017 - val_mae: 0.0312\n",
      "Epoch 73/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - loss: 0.0073 - mae: 0.0600 - val_loss: 0.0019 - val_mae: 0.0332\n",
      "Epoch 74/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - loss: 0.0070 - mae: 0.0587 - val_loss: 0.0018 - val_mae: 0.0322\n",
      "Epoch 75/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - loss: 0.0073 - mae: 0.0594 - val_loss: 0.0018 - val_mae: 0.0325\n",
      "Epoch 76/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 0.0072 - mae: 0.0593 - val_loss: 0.0017 - val_mae: 0.0316\n",
      "Epoch 77/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - loss: 0.0074 - mae: 0.0600 - val_loss: 0.0019 - val_mae: 0.0338\n",
      "Epoch 78/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 0.0071 - mae: 0.0592 - val_loss: 0.0017 - val_mae: 0.0316\n",
      "Epoch 79/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - loss: 0.0071 - mae: 0.0587 - val_loss: 0.0019 - val_mae: 0.0334\n",
      "Epoch 80/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - loss: 0.0073 - mae: 0.0598 - val_loss: 0.0017 - val_mae: 0.0316\n",
      "Epoch 81/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 0.0075 - mae: 0.0595 - val_loss: 0.0020 - val_mae: 0.0340\n",
      "Epoch 82/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 0.0072 - mae: 0.0588 - val_loss: 0.0017 - val_mae: 0.0315\n",
      "Epoch 83/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - loss: 0.0072 - mae: 0.0590 - val_loss: 0.0017 - val_mae: 0.0310\n",
      "Epoch 84/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 0.0072 - mae: 0.0596 - val_loss: 0.0017 - val_mae: 0.0315\n",
      "Epoch 85/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 0.0072 - mae: 0.0597 - val_loss: 0.0017 - val_mae: 0.0314\n",
      "Epoch 86/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 0.0075 - mae: 0.0602 - val_loss: 0.0018 - val_mae: 0.0317\n",
      "Epoch 87/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 0.0072 - mae: 0.0593 - val_loss: 0.0019 - val_mae: 0.0334\n",
      "Epoch 88/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 0.0072 - mae: 0.0591 - val_loss: 0.0018 - val_mae: 0.0317\n",
      "Epoch 89/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 0.0073 - mae: 0.0597 - val_loss: 0.0017 - val_mae: 0.0315\n",
      "Epoch 90/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 0.0072 - mae: 0.0594 - val_loss: 0.0020 - val_mae: 0.0344\n",
      "Epoch 91/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 0.0072 - mae: 0.0594 - val_loss: 0.0017 - val_mae: 0.0315\n",
      "Epoch 92/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 0.0071 - mae: 0.0585 - val_loss: 0.0017 - val_mae: 0.0314\n",
      "Epoch 93/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 0.0070 - mae: 0.0588 - val_loss: 0.0017 - val_mae: 0.0315\n",
      "Epoch 94/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 0.0072 - mae: 0.0593 - val_loss: 0.0019 - val_mae: 0.0329\n",
      "Epoch 95/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9ms/step - loss: 0.0070 - mae: 0.0581 - val_loss: 0.0018 - val_mae: 0.0323\n",
      "Epoch 96/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 0.0074 - mae: 0.0602 - val_loss: 0.0018 - val_mae: 0.0322\n",
      "Epoch 97/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 0.0070 - mae: 0.0587 - val_loss: 0.0018 - val_mae: 0.0317\n",
      "Epoch 98/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 0.0071 - mae: 0.0586 - val_loss: 0.0018 - val_mae: 0.0318\n",
      "Epoch 99/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 0.0072 - mae: 0.0596 - val_loss: 0.0018 - val_mae: 0.0324\n",
      "Epoch 100/100\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 0.0070 - mae: 0.0594 - val_loss: 0.0020 - val_mae: 0.0347\n",
      "\u001b[1m672/672\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step\n",
      "âœ… Done with capsicum_Kolar_daily.csv | MAE=999.8, RMSE=1440.74, R2=0.21, MAPE=44.98%, Accuracy=55.02%\n",
      "\n",
      "ğŸš€ Processing: capsicum_Mandya_daily.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\2878872630.py:115: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Average Price'].fillna(df['Average Price'].mean(), inplace=True)\n",
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\2878872630.py:120: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['MA_7'].fillna(df['MA_7'].mean(), inplace=True)\n",
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\2878872630.py:121: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['MA_30'].fillna(df['MA_30'].mean(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - loss: 0.1859 - mae: 0.2680 - val_loss: 0.0290 - val_mae: 0.1492\n",
      "Epoch 2/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0159 - mae: 0.0930 - val_loss: 0.0392 - val_mae: 0.1779\n",
      "Epoch 3/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0122 - mae: 0.0808 - val_loss: 0.0327 - val_mae: 0.1608\n",
      "Epoch 4/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0092 - mae: 0.0679 - val_loss: 0.0303 - val_mae: 0.1537\n",
      "Epoch 5/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.0083 - mae: 0.0635 - val_loss: 0.0402 - val_mae: 0.1811\n",
      "Epoch 6/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0077 - mae: 0.0609 - val_loss: 0.0177 - val_mae: 0.1124\n",
      "Epoch 7/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0060 - mae: 0.0516 - val_loss: 0.0267 - val_mae: 0.1454\n",
      "Epoch 8/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0058 - mae: 0.0500 - val_loss: 0.0143 - val_mae: 0.1014\n",
      "Epoch 9/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0066 - mae: 0.0522 - val_loss: 0.0142 - val_mae: 0.1020\n",
      "Epoch 10/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0063 - mae: 0.0506 - val_loss: 0.0103 - val_mae: 0.0848\n",
      "Epoch 11/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0054 - mae: 0.0465 - val_loss: 0.0082 - val_mae: 0.0732\n",
      "Epoch 12/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.0052 - mae: 0.0444 - val_loss: 0.0093 - val_mae: 0.0705\n",
      "Epoch 13/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0052 - mae: 0.0436 - val_loss: 0.0096 - val_mae: 0.0815\n",
      "Epoch 14/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0056 - mae: 0.0462 - val_loss: 0.0085 - val_mae: 0.0671\n",
      "Epoch 15/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.0052 - mae: 0.0430 - val_loss: 0.0085 - val_mae: 0.0691\n",
      "Epoch 16/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0050 - mae: 0.0422 - val_loss: 0.0074 - val_mae: 0.0647\n",
      "Epoch 17/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.0054 - mae: 0.0424 - val_loss: 0.0072 - val_mae: 0.0660\n",
      "Epoch 18/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0049 - mae: 0.0402 - val_loss: 0.0099 - val_mae: 0.0827\n",
      "Epoch 19/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0048 - mae: 0.0400 - val_loss: 0.0074 - val_mae: 0.0641\n",
      "Epoch 20/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0047 - mae: 0.0398 - val_loss: 0.0101 - val_mae: 0.0838\n",
      "Epoch 21/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0050 - mae: 0.0422 - val_loss: 0.0077 - val_mae: 0.0703\n",
      "Epoch 22/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0052 - mae: 0.0435 - val_loss: 0.0075 - val_mae: 0.0646\n",
      "Epoch 23/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0052 - mae: 0.0407 - val_loss: 0.0072 - val_mae: 0.0664\n",
      "Epoch 24/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.0047 - mae: 0.0405 - val_loss: 0.0078 - val_mae: 0.0673\n",
      "Epoch 25/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0050 - mae: 0.0402 - val_loss: 0.0071 - val_mae: 0.0637\n",
      "Epoch 26/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0047 - mae: 0.0401 - val_loss: 0.0071 - val_mae: 0.0660\n",
      "Epoch 27/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0044 - mae: 0.0374 - val_loss: 0.0095 - val_mae: 0.0802\n",
      "Epoch 28/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0044 - mae: 0.0385 - val_loss: 0.0072 - val_mae: 0.0671\n",
      "Epoch 29/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0043 - mae: 0.0359 - val_loss: 0.0105 - val_mae: 0.0826\n",
      "Epoch 30/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0046 - mae: 0.0380 - val_loss: 0.0080 - val_mae: 0.0704\n",
      "Epoch 31/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0047 - mae: 0.0393 - val_loss: 0.0083 - val_mae: 0.0746\n",
      "Epoch 32/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0046 - mae: 0.0388 - val_loss: 0.0072 - val_mae: 0.0650\n",
      "Epoch 33/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0051 - mae: 0.0407 - val_loss: 0.0071 - val_mae: 0.0659\n",
      "Epoch 34/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0044 - mae: 0.0362 - val_loss: 0.0089 - val_mae: 0.0783\n",
      "Epoch 35/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0048 - mae: 0.0387 - val_loss: 0.0093 - val_mae: 0.0798\n",
      "Epoch 36/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0044 - mae: 0.0373 - val_loss: 0.0100 - val_mae: 0.0832\n",
      "Epoch 37/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0046 - mae: 0.0397 - val_loss: 0.0073 - val_mae: 0.0646\n",
      "Epoch 38/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0049 - mae: 0.0380 - val_loss: 0.0072 - val_mae: 0.0641\n",
      "Epoch 39/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0044 - mae: 0.0362 - val_loss: 0.0122 - val_mae: 0.0949\n",
      "Epoch 40/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0049 - mae: 0.0397 - val_loss: 0.0096 - val_mae: 0.0816\n",
      "Epoch 41/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0050 - mae: 0.0425 - val_loss: 0.0079 - val_mae: 0.0716\n",
      "Epoch 42/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0049 - mae: 0.0397 - val_loss: 0.0086 - val_mae: 0.0778\n",
      "Epoch 43/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0042 - mae: 0.0343 - val_loss: 0.0075 - val_mae: 0.0691\n",
      "Epoch 44/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0044 - mae: 0.0371 - val_loss: 0.0075 - val_mae: 0.0684\n",
      "Epoch 45/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0046 - mae: 0.0371 - val_loss: 0.0072 - val_mae: 0.0639\n",
      "Epoch 46/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0043 - mae: 0.0368 - val_loss: 0.0162 - val_mae: 0.1106\n",
      "Epoch 47/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0049 - mae: 0.0389 - val_loss: 0.0088 - val_mae: 0.0791\n",
      "Epoch 48/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0046 - mae: 0.0376 - val_loss: 0.0118 - val_mae: 0.0923\n",
      "Epoch 49/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0046 - mae: 0.0379 - val_loss: 0.0106 - val_mae: 0.0888\n",
      "Epoch 50/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0044 - mae: 0.0367 - val_loss: 0.0111 - val_mae: 0.0890\n",
      "Epoch 51/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0049 - mae: 0.0403 - val_loss: 0.0072 - val_mae: 0.0644\n",
      "Epoch 52/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0044 - mae: 0.0347 - val_loss: 0.0206 - val_mae: 0.1257\n",
      "Epoch 53/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0046 - mae: 0.0378 - val_loss: 0.0073 - val_mae: 0.0670\n",
      "Epoch 54/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0042 - mae: 0.0344 - val_loss: 0.0134 - val_mae: 0.1006\n",
      "Epoch 55/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0045 - mae: 0.0371 - val_loss: 0.0079 - val_mae: 0.0726\n",
      "Epoch 56/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0046 - mae: 0.0367 - val_loss: 0.0089 - val_mae: 0.0803\n",
      "Epoch 57/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0048 - mae: 0.0384 - val_loss: 0.0072 - val_mae: 0.0647\n",
      "Epoch 58/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0042 - mae: 0.0353 - val_loss: 0.0144 - val_mae: 0.1051\n",
      "Epoch 59/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0045 - mae: 0.0376 - val_loss: 0.0129 - val_mae: 0.0989\n",
      "Epoch 60/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0044 - mae: 0.0370 - val_loss: 0.0095 - val_mae: 0.0844\n",
      "Epoch 61/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0042 - mae: 0.0343 - val_loss: 0.0092 - val_mae: 0.0826\n",
      "Epoch 62/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0046 - mae: 0.0374 - val_loss: 0.0083 - val_mae: 0.0754\n",
      "Epoch 63/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0049 - mae: 0.0386 - val_loss: 0.0076 - val_mae: 0.0680\n",
      "Epoch 64/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0045 - mae: 0.0364 - val_loss: 0.0073 - val_mae: 0.0658\n",
      "Epoch 65/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0044 - mae: 0.0357 - val_loss: 0.0085 - val_mae: 0.0742\n",
      "Epoch 66/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0044 - mae: 0.0373 - val_loss: 0.0091 - val_mae: 0.0822\n",
      "Epoch 67/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0045 - mae: 0.0381 - val_loss: 0.0093 - val_mae: 0.0829\n",
      "Epoch 68/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0042 - mae: 0.0355 - val_loss: 0.0073 - val_mae: 0.0655\n",
      "Epoch 69/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0045 - mae: 0.0348 - val_loss: 0.0089 - val_mae: 0.0807\n",
      "Epoch 70/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0044 - mae: 0.0340 - val_loss: 0.0101 - val_mae: 0.0879\n",
      "Epoch 71/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0046 - mae: 0.0387 - val_loss: 0.0081 - val_mae: 0.0755\n",
      "Epoch 72/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0042 - mae: 0.0347 - val_loss: 0.0078 - val_mae: 0.0730\n",
      "Epoch 73/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0047 - mae: 0.0376 - val_loss: 0.0092 - val_mae: 0.0824\n",
      "Epoch 74/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0041 - mae: 0.0351 - val_loss: 0.0084 - val_mae: 0.0779\n",
      "Epoch 75/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0042 - mae: 0.0339 - val_loss: 0.0074 - val_mae: 0.0672\n",
      "Epoch 76/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0042 - mae: 0.0362 - val_loss: 0.0076 - val_mae: 0.0704\n",
      "Epoch 77/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0045 - mae: 0.0363 - val_loss: 0.0080 - val_mae: 0.0743\n",
      "Epoch 78/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0047 - mae: 0.0364 - val_loss: 0.0105 - val_mae: 0.0895\n",
      "Epoch 79/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0043 - mae: 0.0347 - val_loss: 0.0082 - val_mae: 0.0763\n",
      "Epoch 80/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0044 - mae: 0.0352 - val_loss: 0.0082 - val_mae: 0.0765\n",
      "Epoch 81/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0047 - mae: 0.0369 - val_loss: 0.0104 - val_mae: 0.0892\n",
      "Epoch 82/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0045 - mae: 0.0358 - val_loss: 0.0084 - val_mae: 0.0775\n",
      "Epoch 83/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0048 - mae: 0.0373 - val_loss: 0.0087 - val_mae: 0.0798\n",
      "Epoch 84/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0041 - mae: 0.0348 - val_loss: 0.0125 - val_mae: 0.0990\n",
      "Epoch 85/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0041 - mae: 0.0340 - val_loss: 0.0078 - val_mae: 0.0733\n",
      "Epoch 86/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0045 - mae: 0.0364 - val_loss: 0.0078 - val_mae: 0.0733\n",
      "Epoch 87/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0043 - mae: 0.0363 - val_loss: 0.0097 - val_mae: 0.0859\n",
      "Epoch 88/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0044 - mae: 0.0358 - val_loss: 0.0099 - val_mae: 0.0869\n",
      "Epoch 89/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0042 - mae: 0.0344 - val_loss: 0.0075 - val_mae: 0.0689\n",
      "Epoch 90/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0040 - mae: 0.0328 - val_loss: 0.0102 - val_mae: 0.0887\n",
      "Epoch 91/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0042 - mae: 0.0338 - val_loss: 0.0124 - val_mae: 0.0979\n",
      "Epoch 92/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0047 - mae: 0.0360 - val_loss: 0.0085 - val_mae: 0.0787\n",
      "Epoch 93/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0040 - mae: 0.0329 - val_loss: 0.0121 - val_mae: 0.0965\n",
      "Epoch 94/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0043 - mae: 0.0348 - val_loss: 0.0092 - val_mae: 0.0826\n",
      "Epoch 95/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0044 - mae: 0.0358 - val_loss: 0.0076 - val_mae: 0.0710\n",
      "Epoch 96/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0047 - mae: 0.0358 - val_loss: 0.0074 - val_mae: 0.0670\n",
      "Epoch 97/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0047 - mae: 0.0379 - val_loss: 0.0089 - val_mae: 0.0815\n",
      "Epoch 98/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0042 - mae: 0.0349 - val_loss: 0.0095 - val_mae: 0.0847\n",
      "Epoch 99/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0037 - mae: 0.0317 - val_loss: 0.0107 - val_mae: 0.0910\n",
      "Epoch 100/100\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0044 - mae: 0.0348 - val_loss: 0.0107 - val_mae: 0.0906\n",
      "\u001b[1m178/178\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "âœ… Done with capsicum_Mandya_daily.csv | MAE=274.03, RMSE=441.55, R2=0.85, MAPE=12.89%, Accuracy=87.11%\n",
      "\n",
      "ğŸš€ Processing: capsicum_Mysore_daily.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\2878872630.py:115: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Average Price'].fillna(df['Average Price'].mean(), inplace=True)\n",
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\2878872630.py:120: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['MA_7'].fillna(df['MA_7'].mean(), inplace=True)\n",
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\2878872630.py:121: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['MA_30'].fillna(df['MA_30'].mean(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - loss: 0.0702 - mae: 0.1456 - val_loss: 0.0030 - val_mae: 0.0439\n",
      "Epoch 2/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0035 - mae: 0.0460 - val_loss: 0.0030 - val_mae: 0.0443\n",
      "Epoch 3/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - loss: 0.0023 - mae: 0.0367 - val_loss: 0.0020 - val_mae: 0.0317\n",
      "Epoch 4/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0017 - mae: 0.0316 - val_loss: 0.0020 - val_mae: 0.0321\n",
      "Epoch 5/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0015 - mae: 0.0290 - val_loss: 0.0020 - val_mae: 0.0315\n",
      "Epoch 6/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - loss: 0.0014 - mae: 0.0279 - val_loss: 0.0020 - val_mae: 0.0327\n",
      "Epoch 7/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - loss: 0.0014 - mae: 0.0271 - val_loss: 0.0021 - val_mae: 0.0327\n",
      "Epoch 8/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0015 - mae: 0.0282 - val_loss: 0.0022 - val_mae: 0.0325\n",
      "Epoch 9/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0015 - mae: 0.0280 - val_loss: 0.0020 - val_mae: 0.0317\n",
      "Epoch 10/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0013 - mae: 0.0269 - val_loss: 0.0024 - val_mae: 0.0369\n",
      "Epoch 11/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0012 - mae: 0.0258 - val_loss: 0.0029 - val_mae: 0.0429\n",
      "Epoch 12/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0013 - mae: 0.0268 - val_loss: 0.0058 - val_mae: 0.0672\n",
      "Epoch 13/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0014 - mae: 0.0277 - val_loss: 0.0021 - val_mae: 0.0329\n",
      "Epoch 14/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0012 - mae: 0.0254 - val_loss: 0.0020 - val_mae: 0.0313\n",
      "Epoch 15/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0013 - mae: 0.0268 - val_loss: 0.0020 - val_mae: 0.0313\n",
      "Epoch 16/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0011 - mae: 0.0246 - val_loss: 0.0020 - val_mae: 0.0314\n",
      "Epoch 17/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0012 - mae: 0.0253 - val_loss: 0.0027 - val_mae: 0.0402\n",
      "Epoch 18/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0012 - mae: 0.0247 - val_loss: 0.0020 - val_mae: 0.0314\n",
      "Epoch 19/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0011 - mae: 0.0240 - val_loss: 0.0021 - val_mae: 0.0329\n",
      "Epoch 20/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - loss: 0.0012 - mae: 0.0244 - val_loss: 0.0021 - val_mae: 0.0320\n",
      "Epoch 21/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0011 - mae: 0.0237 - val_loss: 0.0021 - val_mae: 0.0328\n",
      "Epoch 22/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0011 - mae: 0.0243 - val_loss: 0.0020 - val_mae: 0.0327\n",
      "Epoch 23/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0012 - mae: 0.0241 - val_loss: 0.0030 - val_mae: 0.0387\n",
      "Epoch 24/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0012 - mae: 0.0256 - val_loss: 0.0024 - val_mae: 0.0366\n",
      "Epoch 25/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0011 - mae: 0.0244 - val_loss: 0.0020 - val_mae: 0.0314\n",
      "Epoch 26/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0012 - mae: 0.0246 - val_loss: 0.0023 - val_mae: 0.0353\n",
      "Epoch 27/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0011 - mae: 0.0236 - val_loss: 0.0020 - val_mae: 0.0318\n",
      "Epoch 28/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0012 - mae: 0.0247 - val_loss: 0.0020 - val_mae: 0.0315\n",
      "Epoch 29/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0012 - mae: 0.0243 - val_loss: 0.0020 - val_mae: 0.0318\n",
      "Epoch 30/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0011 - mae: 0.0240 - val_loss: 0.0020 - val_mae: 0.0314\n",
      "Epoch 31/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0012 - mae: 0.0245 - val_loss: 0.0023 - val_mae: 0.0340\n",
      "Epoch 32/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0011 - mae: 0.0240 - val_loss: 0.0021 - val_mae: 0.0335\n",
      "Epoch 33/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0011 - mae: 0.0239 - val_loss: 0.0024 - val_mae: 0.0362\n",
      "Epoch 34/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0011 - mae: 0.0233 - val_loss: 0.0023 - val_mae: 0.0353\n",
      "Epoch 35/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0011 - mae: 0.0237 - val_loss: 0.0020 - val_mae: 0.0315\n",
      "Epoch 36/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0012 - mae: 0.0245 - val_loss: 0.0021 - val_mae: 0.0338\n",
      "Epoch 37/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0012 - mae: 0.0243 - val_loss: 0.0020 - val_mae: 0.0315\n",
      "Epoch 38/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0011 - mae: 0.0237 - val_loss: 0.0020 - val_mae: 0.0324\n",
      "Epoch 39/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0011 - mae: 0.0239 - val_loss: 0.0020 - val_mae: 0.0314\n",
      "Epoch 40/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - loss: 0.0011 - mae: 0.0239 - val_loss: 0.0020 - val_mae: 0.0315\n",
      "Epoch 41/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0011 - mae: 0.0235 - val_loss: 0.0020 - val_mae: 0.0315\n",
      "Epoch 42/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0011 - mae: 0.0240 - val_loss: 0.0024 - val_mae: 0.0365\n",
      "Epoch 43/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0010 - mae: 0.0232 - val_loss: 0.0020 - val_mae: 0.0322\n",
      "Epoch 44/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0011 - mae: 0.0238 - val_loss: 0.0021 - val_mae: 0.0333\n",
      "Epoch 45/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0012 - mae: 0.0241 - val_loss: 0.0020 - val_mae: 0.0321\n",
      "Epoch 46/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0010 - mae: 0.0230 - val_loss: 0.0020 - val_mae: 0.0319\n",
      "Epoch 47/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0011 - mae: 0.0239 - val_loss: 0.0020 - val_mae: 0.0318\n",
      "Epoch 48/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0011 - mae: 0.0242 - val_loss: 0.0024 - val_mae: 0.0362\n",
      "Epoch 49/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0011 - mae: 0.0236 - val_loss: 0.0020 - val_mae: 0.0315\n",
      "Epoch 50/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0011 - mae: 0.0238 - val_loss: 0.0024 - val_mae: 0.0360\n",
      "Epoch 51/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0011 - mae: 0.0236 - val_loss: 0.0020 - val_mae: 0.0325\n",
      "Epoch 52/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0011 - mae: 0.0236 - val_loss: 0.0022 - val_mae: 0.0340\n",
      "Epoch 53/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0011 - mae: 0.0238 - val_loss: 0.0020 - val_mae: 0.0317\n",
      "Epoch 54/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0010 - mae: 0.0233 - val_loss: 0.0021 - val_mae: 0.0332\n",
      "Epoch 55/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0011 - mae: 0.0234 - val_loss: 0.0025 - val_mae: 0.0381\n",
      "Epoch 56/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0011 - mae: 0.0234 - val_loss: 0.0020 - val_mae: 0.0317\n",
      "Epoch 57/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0011 - mae: 0.0234 - val_loss: 0.0023 - val_mae: 0.0356\n",
      "Epoch 58/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0011 - mae: 0.0234 - val_loss: 0.0020 - val_mae: 0.0315\n",
      "Epoch 59/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0011 - mae: 0.0238 - val_loss: 0.0020 - val_mae: 0.0314\n",
      "Epoch 60/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0011 - mae: 0.0239 - val_loss: 0.0022 - val_mae: 0.0342\n",
      "Epoch 61/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0011 - mae: 0.0239 - val_loss: 0.0024 - val_mae: 0.0366\n",
      "Epoch 62/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0011 - mae: 0.0233 - val_loss: 0.0020 - val_mae: 0.0314\n",
      "Epoch 63/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0011 - mae: 0.0236 - val_loss: 0.0020 - val_mae: 0.0317\n",
      "Epoch 64/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0011 - mae: 0.0233 - val_loss: 0.0022 - val_mae: 0.0350\n",
      "Epoch 65/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0011 - mae: 0.0231 - val_loss: 0.0021 - val_mae: 0.0335\n",
      "Epoch 66/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0011 - mae: 0.0235 - val_loss: 0.0020 - val_mae: 0.0321\n",
      "Epoch 67/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0011 - mae: 0.0239 - val_loss: 0.0022 - val_mae: 0.0341\n",
      "Epoch 68/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0011 - mae: 0.0232 - val_loss: 0.0020 - val_mae: 0.0318\n",
      "Epoch 69/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0011 - mae: 0.0238 - val_loss: 0.0026 - val_mae: 0.0383\n",
      "Epoch 70/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0011 - mae: 0.0236 - val_loss: 0.0025 - val_mae: 0.0369\n",
      "Epoch 71/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0011 - mae: 0.0230 - val_loss: 0.0025 - val_mae: 0.0377\n",
      "Epoch 72/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0010 - mae: 0.0231 - val_loss: 0.0020 - val_mae: 0.0322\n",
      "Epoch 73/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0011 - mae: 0.0230 - val_loss: 0.0026 - val_mae: 0.0390\n",
      "Epoch 74/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0011 - mae: 0.0237 - val_loss: 0.0021 - val_mae: 0.0329\n",
      "Epoch 75/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0011 - mae: 0.0234 - val_loss: 0.0023 - val_mae: 0.0349\n",
      "Epoch 76/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0011 - mae: 0.0228 - val_loss: 0.0020 - val_mae: 0.0326\n",
      "Epoch 77/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0011 - mae: 0.0234 - val_loss: 0.0028 - val_mae: 0.0406\n",
      "Epoch 78/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0011 - mae: 0.0232 - val_loss: 0.0020 - val_mae: 0.0321\n",
      "Epoch 79/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0011 - mae: 0.0235 - val_loss: 0.0021 - val_mae: 0.0334\n",
      "Epoch 80/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0010 - mae: 0.0230 - val_loss: 0.0021 - val_mae: 0.0328\n",
      "Epoch 81/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0010 - mae: 0.0228 - val_loss: 0.0020 - val_mae: 0.0327\n",
      "Epoch 82/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0011 - mae: 0.0229 - val_loss: 0.0022 - val_mae: 0.0339\n",
      "Epoch 83/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0011 - mae: 0.0229 - val_loss: 0.0021 - val_mae: 0.0333\n",
      "Epoch 84/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0011 - mae: 0.0233 - val_loss: 0.0021 - val_mae: 0.0335\n",
      "Epoch 85/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0011 - mae: 0.0232 - val_loss: 0.0020 - val_mae: 0.0314\n",
      "Epoch 86/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0011 - mae: 0.0231 - val_loss: 0.0021 - val_mae: 0.0319\n",
      "Epoch 87/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0011 - mae: 0.0233 - val_loss: 0.0021 - val_mae: 0.0335\n",
      "Epoch 88/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0011 - mae: 0.0234 - val_loss: 0.0020 - val_mae: 0.0325\n",
      "Epoch 89/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0011 - mae: 0.0232 - val_loss: 0.0020 - val_mae: 0.0316\n",
      "Epoch 90/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0011 - mae: 0.0229 - val_loss: 0.0020 - val_mae: 0.0318\n",
      "Epoch 91/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0011 - mae: 0.0232 - val_loss: 0.0020 - val_mae: 0.0323\n",
      "Epoch 92/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0011 - mae: 0.0231 - val_loss: 0.0022 - val_mae: 0.0346\n",
      "Epoch 93/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0011 - mae: 0.0229 - val_loss: 0.0020 - val_mae: 0.0317\n",
      "Epoch 94/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0011 - mae: 0.0232 - val_loss: 0.0021 - val_mae: 0.0332\n",
      "Epoch 95/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0010 - mae: 0.0228 - val_loss: 0.0020 - val_mae: 0.0327\n",
      "Epoch 96/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0011 - mae: 0.0230 - val_loss: 0.0020 - val_mae: 0.0325\n",
      "Epoch 97/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0011 - mae: 0.0234 - val_loss: 0.0021 - val_mae: 0.0333\n",
      "Epoch 98/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0011 - mae: 0.0230 - val_loss: 0.0020 - val_mae: 0.0323\n",
      "Epoch 99/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0011 - mae: 0.0230 - val_loss: 0.0025 - val_mae: 0.0373\n",
      "Epoch 100/100\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0011 - mae: 0.0231 - val_loss: 0.0020 - val_mae: 0.0323\n",
      "\u001b[1m319/319\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "âœ… Done with capsicum_Mysore_daily.csv | MAE=589.06, RMSE=839.76, R2=0.55, MAPE=23.29%, Accuracy=76.71%\n",
      "\n",
      "ğŸš€ Processing: capsicum_Shimoga_daily.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\2878872630.py:115: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Average Price'].fillna(df['Average Price'].mean(), inplace=True)\n",
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\2878872630.py:120: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['MA_7'].fillna(df['MA_7'].mean(), inplace=True)\n",
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\2878872630.py:121: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['MA_30'].fillna(df['MA_30'].mean(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 181ms/step - loss: 0.5979 - mae: 0.6911 - val_loss: 0.0607 - val_mae: 0.2417\n",
      "Epoch 2/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.1285 - mae: 0.2904 - val_loss: 0.3662 - val_mae: 0.6037\n",
      "Epoch 3/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0994 - mae: 0.2596 - val_loss: 0.0104 - val_mae: 0.0934\n",
      "Epoch 4/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0889 - mae: 0.2594 - val_loss: 0.0988 - val_mae: 0.3122\n",
      "Epoch 5/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0556 - mae: 0.1947 - val_loss: 0.0183 - val_mae: 0.1305\n",
      "Epoch 6/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0218 - mae: 0.1120 - val_loss: 0.0061 - val_mae: 0.0700\n",
      "Epoch 7/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0264 - mae: 0.1341 - val_loss: 0.0250 - val_mae: 0.1546\n",
      "Epoch 8/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0308 - mae: 0.1382 - val_loss: 0.0022 - val_mae: 0.0378\n",
      "Epoch 9/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0261 - mae: 0.1277 - val_loss: 0.0020 - val_mae: 0.0364\n",
      "Epoch 10/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0198 - mae: 0.1101 - val_loss: 0.0307 - val_mae: 0.1721\n",
      "Epoch 11/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0286 - mae: 0.1252 - val_loss: 0.0025 - val_mae: 0.0405\n",
      "Epoch 12/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0261 - mae: 0.1360 - val_loss: 0.0100 - val_mae: 0.0940\n",
      "Epoch 13/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0213 - mae: 0.1160 - val_loss: 0.0235 - val_mae: 0.1494\n",
      "Epoch 14/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0213 - mae: 0.1155 - val_loss: 0.0036 - val_mae: 0.0498\n",
      "Epoch 15/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0204 - mae: 0.1166 - val_loss: 0.0246 - val_mae: 0.1527\n",
      "Epoch 16/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0107 - mae: 0.0836 - val_loss: 0.0177 - val_mae: 0.1283\n",
      "Epoch 17/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0161 - mae: 0.1055 - val_loss: 0.0098 - val_mae: 0.0922\n",
      "Epoch 18/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0123 - mae: 0.0902 - val_loss: 0.0217 - val_mae: 0.1428\n",
      "Epoch 19/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.0102 - mae: 0.0837 - val_loss: 0.0162 - val_mae: 0.1222\n",
      "Epoch 20/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0143 - mae: 0.0948 - val_loss: 0.0144 - val_mae: 0.1146\n",
      "Epoch 21/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.0132 - mae: 0.0826 - val_loss: 0.0082 - val_mae: 0.0834\n",
      "Epoch 22/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0153 - mae: 0.0925 - val_loss: 0.0282 - val_mae: 0.1644\n",
      "Epoch 23/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0208 - mae: 0.1170 - val_loss: 0.0080 - val_mae: 0.0822\n",
      "Epoch 24/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.0174 - mae: 0.1006 - val_loss: 0.0059 - val_mae: 0.0678\n",
      "Epoch 25/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0106 - mae: 0.0818 - val_loss: 0.0169 - val_mae: 0.1251\n",
      "Epoch 26/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0111 - mae: 0.0827 - val_loss: 0.0176 - val_mae: 0.1278\n",
      "Epoch 27/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0151 - mae: 0.0999 - val_loss: 0.0254 - val_mae: 0.1553\n",
      "Epoch 28/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.0148 - mae: 0.0955 - val_loss: 0.0093 - val_mae: 0.0899\n",
      "Epoch 29/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0097 - mae: 0.0764 - val_loss: 0.0108 - val_mae: 0.0978\n",
      "Epoch 30/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.0108 - mae: 0.0796 - val_loss: 0.0170 - val_mae: 0.1260\n",
      "Epoch 31/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0144 - mae: 0.0945 - val_loss: 0.0048 - val_mae: 0.0601\n",
      "Epoch 32/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.0118 - mae: 0.0875 - val_loss: 0.0224 - val_mae: 0.1456\n",
      "Epoch 33/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0163 - mae: 0.1032 - val_loss: 0.0018 - val_mae: 0.0355\n",
      "Epoch 34/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0230 - mae: 0.1295 - val_loss: 0.0332 - val_mae: 0.1790\n",
      "Epoch 35/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.0245 - mae: 0.1337 - val_loss: 0.0074 - val_mae: 0.0788\n",
      "Epoch 36/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0094 - mae: 0.0798 - val_loss: 0.0197 - val_mae: 0.1362\n",
      "Epoch 37/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0111 - mae: 0.0888 - val_loss: 0.0075 - val_mae: 0.0792\n",
      "Epoch 38/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0158 - mae: 0.0983 - val_loss: 0.0069 - val_mae: 0.0759\n",
      "Epoch 39/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0217 - mae: 0.1150 - val_loss: 0.0164 - val_mae: 0.1233\n",
      "Epoch 40/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0149 - mae: 0.0995 - val_loss: 0.0047 - val_mae: 0.0595\n",
      "Epoch 41/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0097 - mae: 0.0805 - val_loss: 0.0183 - val_mae: 0.1309\n",
      "Epoch 42/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.0135 - mae: 0.0930 - val_loss: 0.0039 - val_mae: 0.0519\n",
      "Epoch 43/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0183 - mae: 0.1120 - val_loss: 0.0500 - val_mae: 0.2209\n",
      "Epoch 44/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.0129 - mae: 0.0991 - val_loss: 0.0043 - val_mae: 0.0547\n",
      "Epoch 45/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.0143 - mae: 0.1026 - val_loss: 0.0428 - val_mae: 0.2038\n",
      "Epoch 46/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0195 - mae: 0.1035 - val_loss: 0.0131 - val_mae: 0.1089\n",
      "Epoch 47/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0163 - mae: 0.1002 - val_loss: 0.0134 - val_mae: 0.1103\n",
      "Epoch 48/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0148 - mae: 0.0994 - val_loss: 0.0131 - val_mae: 0.1092\n",
      "Epoch 49/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0105 - mae: 0.0843 - val_loss: 0.0054 - val_mae: 0.0654\n",
      "Epoch 50/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0109 - mae: 0.0874 - val_loss: 0.0108 - val_mae: 0.0984\n",
      "Epoch 51/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0105 - mae: 0.0871 - val_loss: 0.0034 - val_mae: 0.0478\n",
      "Epoch 52/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0125 - mae: 0.0858 - val_loss: 0.0094 - val_mae: 0.0906\n",
      "Epoch 53/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0146 - mae: 0.0904 - val_loss: 0.0046 - val_mae: 0.0583\n",
      "Epoch 54/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.0110 - mae: 0.0849 - val_loss: 0.0214 - val_mae: 0.1421\n",
      "Epoch 55/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0110 - mae: 0.0849 - val_loss: 0.0036 - val_mae: 0.0501\n",
      "Epoch 56/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0084 - mae: 0.0726 - val_loss: 0.0135 - val_mae: 0.1107\n",
      "Epoch 57/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0097 - mae: 0.0849 - val_loss: 0.0128 - val_mae: 0.1074\n",
      "Epoch 58/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0087 - mae: 0.0761 - val_loss: 0.0207 - val_mae: 0.1398\n",
      "Epoch 59/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0127 - mae: 0.0911 - val_loss: 0.0111 - val_mae: 0.0994\n",
      "Epoch 60/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0107 - mae: 0.0806 - val_loss: 0.0148 - val_mae: 0.1167\n",
      "Epoch 61/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0155 - mae: 0.0968 - val_loss: 0.0049 - val_mae: 0.0609\n",
      "Epoch 62/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0108 - mae: 0.0807 - val_loss: 0.0246 - val_mae: 0.1532\n",
      "Epoch 63/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.0073 - mae: 0.0660 - val_loss: 0.0013 - val_mae: 0.0307\n",
      "Epoch 64/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.0159 - mae: 0.1020 - val_loss: 0.0431 - val_mae: 0.2048\n",
      "Epoch 65/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0168 - mae: 0.1077 - val_loss: 0.0033 - val_mae: 0.0468\n",
      "Epoch 66/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 0.0133 - mae: 0.0951 - val_loss: 0.0221 - val_mae: 0.1444\n",
      "Epoch 67/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0123 - mae: 0.0905 - val_loss: 0.0171 - val_mae: 0.1258\n",
      "Epoch 68/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0135 - mae: 0.0880 - val_loss: 0.0111 - val_mae: 0.0993\n",
      "Epoch 69/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0159 - mae: 0.0972 - val_loss: 0.0343 - val_mae: 0.1821\n",
      "Epoch 70/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0077 - mae: 0.0686 - val_loss: 0.0022 - val_mae: 0.0382\n",
      "Epoch 71/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.0114 - mae: 0.0866 - val_loss: 0.0156 - val_mae: 0.1203\n",
      "Epoch 72/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0098 - mae: 0.0799 - val_loss: 0.0040 - val_mae: 0.0538\n",
      "Epoch 73/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0085 - mae: 0.0746 - val_loss: 0.0069 - val_mae: 0.0760\n",
      "Epoch 74/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0101 - mae: 0.0846 - val_loss: 0.0143 - val_mae: 0.1149\n",
      "Epoch 75/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0077 - mae: 0.0710 - val_loss: 0.0069 - val_mae: 0.0756\n",
      "Epoch 76/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0093 - mae: 0.0819 - val_loss: 0.0133 - val_mae: 0.1100\n",
      "Epoch 77/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0088 - mae: 0.0764 - val_loss: 0.0185 - val_mae: 0.1315\n",
      "Epoch 78/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0089 - mae: 0.0691 - val_loss: 0.0145 - val_mae: 0.1154\n",
      "Epoch 79/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0083 - mae: 0.0746 - val_loss: 0.0158 - val_mae: 0.1209\n",
      "Epoch 80/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0089 - mae: 0.0748 - val_loss: 0.0116 - val_mae: 0.1022\n",
      "Epoch 81/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.0123 - mae: 0.0917 - val_loss: 0.0053 - val_mae: 0.0644\n",
      "Epoch 82/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0067 - mae: 0.0647 - val_loss: 0.0050 - val_mae: 0.0622\n",
      "Epoch 83/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0124 - mae: 0.0884 - val_loss: 0.0133 - val_mae: 0.1103\n",
      "Epoch 84/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.0082 - mae: 0.0699 - val_loss: 0.0073 - val_mae: 0.0788\n",
      "Epoch 85/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0098 - mae: 0.0788 - val_loss: 0.0113 - val_mae: 0.1008\n",
      "Epoch 86/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.0086 - mae: 0.0706 - val_loss: 0.0061 - val_mae: 0.0711\n",
      "Epoch 87/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0091 - mae: 0.0813 - val_loss: 0.0066 - val_mae: 0.0739\n",
      "Epoch 88/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0079 - mae: 0.0754 - val_loss: 0.0073 - val_mae: 0.0787\n",
      "Epoch 89/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0080 - mae: 0.0731 - val_loss: 0.0138 - val_mae: 0.1125\n",
      "Epoch 90/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0086 - mae: 0.0705 - val_loss: 0.0097 - val_mae: 0.0928\n",
      "Epoch 91/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.0073 - mae: 0.0647 - val_loss: 0.0117 - val_mae: 0.1026\n",
      "Epoch 92/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0056 - mae: 0.0569 - val_loss: 0.0066 - val_mae: 0.0737\n",
      "Epoch 93/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0067 - mae: 0.0657 - val_loss: 0.0158 - val_mae: 0.1213\n",
      "Epoch 94/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0082 - mae: 0.0763 - val_loss: 0.0139 - val_mae: 0.1130\n",
      "Epoch 95/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0065 - mae: 0.0699 - val_loss: 0.0213 - val_mae: 0.1419\n",
      "Epoch 96/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0077 - mae: 0.0720 - val_loss: 0.0126 - val_mae: 0.1069\n",
      "Epoch 97/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0072 - mae: 0.0697 - val_loss: 0.0157 - val_mae: 0.1205\n",
      "Epoch 98/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 0.0105 - mae: 0.0874 - val_loss: 0.0139 - val_mae: 0.1127\n",
      "Epoch 99/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.0075 - mae: 0.0699 - val_loss: 0.0147 - val_mae: 0.1165\n",
      "Epoch 100/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0064 - mae: 0.0627 - val_loss: 0.0117 - val_mae: 0.1026\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step\n",
      "âœ… Done with capsicum_Shimoga_daily.csv | MAE=50.18, RMSE=56.65, R2=0.83, MAPE=3.03%, Accuracy=96.97%\n",
      "\n",
      "ğŸš€ Processing: capsicum_Udupi_daily.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\2878872630.py:115: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Average Price'].fillna(df['Average Price'].mean(), inplace=True)\n",
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\2878872630.py:120: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['MA_7'].fillna(df['MA_7'].mean(), inplace=True)\n",
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\2878872630.py:121: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['MA_30'].fillna(df['MA_30'].mean(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - loss: 0.1455 - mae: 0.2342 - val_loss: 0.0170 - val_mae: 0.0992\n",
      "Epoch 2/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0118 - mae: 0.0837 - val_loss: 0.0161 - val_mae: 0.0954\n",
      "Epoch 3/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0089 - mae: 0.0724 - val_loss: 0.0156 - val_mae: 0.0946\n",
      "Epoch 4/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0066 - mae: 0.0627 - val_loss: 0.0158 - val_mae: 0.0938\n",
      "Epoch 5/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0056 - mae: 0.0560 - val_loss: 0.0150 - val_mae: 0.0912\n",
      "Epoch 6/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0046 - mae: 0.0505 - val_loss: 0.0154 - val_mae: 0.0905\n",
      "Epoch 7/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0033 - mae: 0.0421 - val_loss: 0.0155 - val_mae: 0.0895\n",
      "Epoch 8/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0031 - mae: 0.0405 - val_loss: 0.0165 - val_mae: 0.0911\n",
      "Epoch 9/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0029 - mae: 0.0379 - val_loss: 0.0154 - val_mae: 0.0883\n",
      "Epoch 10/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0023 - mae: 0.0325 - val_loss: 0.0166 - val_mae: 0.0903\n",
      "Epoch 11/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0022 - mae: 0.0312 - val_loss: 0.0164 - val_mae: 0.0899\n",
      "Epoch 12/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0025 - mae: 0.0347 - val_loss: 0.0151 - val_mae: 0.0887\n",
      "Epoch 13/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0020 - mae: 0.0297 - val_loss: 0.0175 - val_mae: 0.0932\n",
      "Epoch 14/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0018 - mae: 0.0286 - val_loss: 0.0151 - val_mae: 0.0895\n",
      "Epoch 15/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0019 - mae: 0.0295 - val_loss: 0.0139 - val_mae: 0.0869\n",
      "Epoch 16/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0022 - mae: 0.0323 - val_loss: 0.0152 - val_mae: 0.0890\n",
      "Epoch 17/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0022 - mae: 0.0307 - val_loss: 0.0145 - val_mae: 0.0878\n",
      "Epoch 18/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0019 - mae: 0.0294 - val_loss: 0.0148 - val_mae: 0.0887\n",
      "Epoch 19/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0018 - mae: 0.0281 - val_loss: 0.0138 - val_mae: 0.0876\n",
      "Epoch 20/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0018 - mae: 0.0274 - val_loss: 0.0143 - val_mae: 0.0910\n",
      "Epoch 21/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0018 - mae: 0.0282 - val_loss: 0.0136 - val_mae: 0.0873\n",
      "Epoch 22/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0017 - mae: 0.0269 - val_loss: 0.0143 - val_mae: 0.0894\n",
      "Epoch 23/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0018 - mae: 0.0285 - val_loss: 0.0164 - val_mae: 0.0946\n",
      "Epoch 24/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0017 - mae: 0.0269 - val_loss: 0.0169 - val_mae: 0.0962\n",
      "Epoch 25/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0017 - mae: 0.0270 - val_loss: 0.0149 - val_mae: 0.0913\n",
      "Epoch 26/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0017 - mae: 0.0263 - val_loss: 0.0148 - val_mae: 0.0912\n",
      "Epoch 27/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0016 - mae: 0.0252 - val_loss: 0.0192 - val_mae: 0.1063\n",
      "Epoch 28/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0017 - mae: 0.0275 - val_loss: 0.0141 - val_mae: 0.0913\n",
      "Epoch 29/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0019 - mae: 0.0297 - val_loss: 0.0135 - val_mae: 0.0885\n",
      "Epoch 30/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0016 - mae: 0.0255 - val_loss: 0.0154 - val_mae: 0.0929\n",
      "Epoch 31/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0015 - mae: 0.0245 - val_loss: 0.0146 - val_mae: 0.0916\n",
      "Epoch 32/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0016 - mae: 0.0249 - val_loss: 0.0148 - val_mae: 0.0921\n",
      "Epoch 33/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0015 - mae: 0.0249 - val_loss: 0.0152 - val_mae: 0.0925\n",
      "Epoch 34/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0017 - mae: 0.0278 - val_loss: 0.0140 - val_mae: 0.0903\n",
      "Epoch 35/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0015 - mae: 0.0257 - val_loss: 0.0151 - val_mae: 0.0927\n",
      "Epoch 36/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0015 - mae: 0.0246 - val_loss: 0.0140 - val_mae: 0.0901\n",
      "Epoch 37/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0016 - mae: 0.0246 - val_loss: 0.0170 - val_mae: 0.0980\n",
      "Epoch 38/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0015 - mae: 0.0246 - val_loss: 0.0143 - val_mae: 0.0913\n",
      "Epoch 39/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0016 - mae: 0.0258 - val_loss: 0.0136 - val_mae: 0.0919\n",
      "Epoch 40/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0016 - mae: 0.0255 - val_loss: 0.0135 - val_mae: 0.0902\n",
      "Epoch 41/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0016 - mae: 0.0241 - val_loss: 0.0138 - val_mae: 0.0906\n",
      "Epoch 42/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0015 - mae: 0.0246 - val_loss: 0.0137 - val_mae: 0.0903\n",
      "Epoch 43/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0016 - mae: 0.0249 - val_loss: 0.0145 - val_mae: 0.0909\n",
      "Epoch 44/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0015 - mae: 0.0249 - val_loss: 0.0144 - val_mae: 0.0904\n",
      "Epoch 45/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0015 - mae: 0.0251 - val_loss: 0.0140 - val_mae: 0.0899\n",
      "Epoch 46/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0014 - mae: 0.0224 - val_loss: 0.0171 - val_mae: 0.0975\n",
      "Epoch 47/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0015 - mae: 0.0249 - val_loss: 0.0139 - val_mae: 0.0905\n",
      "Epoch 48/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0015 - mae: 0.0247 - val_loss: 0.0142 - val_mae: 0.0899\n",
      "Epoch 49/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0016 - mae: 0.0267 - val_loss: 0.0148 - val_mae: 0.0913\n",
      "Epoch 50/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0015 - mae: 0.0234 - val_loss: 0.0147 - val_mae: 0.0913\n",
      "Epoch 51/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0016 - mae: 0.0263 - val_loss: 0.0141 - val_mae: 0.0903\n",
      "Epoch 52/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0013 - mae: 0.0223 - val_loss: 0.0151 - val_mae: 0.0923\n",
      "Epoch 53/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0016 - mae: 0.0240 - val_loss: 0.0152 - val_mae: 0.0928\n",
      "Epoch 54/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0013 - mae: 0.0228 - val_loss: 0.0147 - val_mae: 0.0910\n",
      "Epoch 55/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0014 - mae: 0.0224 - val_loss: 0.0149 - val_mae: 0.0917\n",
      "Epoch 56/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0016 - mae: 0.0255 - val_loss: 0.0149 - val_mae: 0.0923\n",
      "Epoch 57/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0015 - mae: 0.0232 - val_loss: 0.0157 - val_mae: 0.0933\n",
      "Epoch 58/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0016 - mae: 0.0247 - val_loss: 0.0143 - val_mae: 0.0904\n",
      "Epoch 59/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0014 - mae: 0.0229 - val_loss: 0.0143 - val_mae: 0.0910\n",
      "Epoch 60/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0014 - mae: 0.0229 - val_loss: 0.0183 - val_mae: 0.1001\n",
      "Epoch 61/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0015 - mae: 0.0247 - val_loss: 0.0191 - val_mae: 0.1025\n",
      "Epoch 62/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0016 - mae: 0.0246 - val_loss: 0.0146 - val_mae: 0.0911\n",
      "Epoch 63/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0016 - mae: 0.0239 - val_loss: 0.0157 - val_mae: 0.0939\n",
      "Epoch 64/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0014 - mae: 0.0228 - val_loss: 0.0181 - val_mae: 0.1003\n",
      "Epoch 65/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0015 - mae: 0.0230 - val_loss: 0.0148 - val_mae: 0.0918\n",
      "Epoch 66/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0015 - mae: 0.0234 - val_loss: 0.0173 - val_mae: 0.0973\n",
      "Epoch 67/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0015 - mae: 0.0246 - val_loss: 0.0145 - val_mae: 0.0909\n",
      "Epoch 68/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0014 - mae: 0.0231 - val_loss: 0.0146 - val_mae: 0.0913\n",
      "Epoch 69/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0014 - mae: 0.0226 - val_loss: 0.0163 - val_mae: 0.0953\n",
      "Epoch 70/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0015 - mae: 0.0233 - val_loss: 0.0153 - val_mae: 0.0927\n",
      "Epoch 71/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0016 - mae: 0.0253 - val_loss: 0.0152 - val_mae: 0.0926\n",
      "Epoch 72/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0014 - mae: 0.0230 - val_loss: 0.0151 - val_mae: 0.0923\n",
      "Epoch 73/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0015 - mae: 0.0236 - val_loss: 0.0164 - val_mae: 0.0949\n",
      "Epoch 74/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0015 - mae: 0.0231 - val_loss: 0.0154 - val_mae: 0.0928\n",
      "Epoch 75/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0014 - mae: 0.0227 - val_loss: 0.0169 - val_mae: 0.0964\n",
      "Epoch 76/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0014 - mae: 0.0218 - val_loss: 0.0169 - val_mae: 0.0962\n",
      "Epoch 77/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0015 - mae: 0.0234 - val_loss: 0.0155 - val_mae: 0.0927\n",
      "Epoch 78/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0014 - mae: 0.0233 - val_loss: 0.0172 - val_mae: 0.0972\n",
      "Epoch 79/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0014 - mae: 0.0228 - val_loss: 0.0140 - val_mae: 0.0892\n",
      "Epoch 80/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0014 - mae: 0.0229 - val_loss: 0.0186 - val_mae: 0.1009\n",
      "Epoch 81/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0014 - mae: 0.0226 - val_loss: 0.0151 - val_mae: 0.0923\n",
      "Epoch 82/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0014 - mae: 0.0236 - val_loss: 0.0135 - val_mae: 0.0889\n",
      "Epoch 83/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0016 - mae: 0.0247 - val_loss: 0.0152 - val_mae: 0.0924\n",
      "Epoch 84/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0014 - mae: 0.0226 - val_loss: 0.0148 - val_mae: 0.0914\n",
      "Epoch 85/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0014 - mae: 0.0229 - val_loss: 0.0152 - val_mae: 0.0927\n",
      "Epoch 86/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0014 - mae: 0.0232 - val_loss: 0.0174 - val_mae: 0.0972\n",
      "Epoch 87/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0013 - mae: 0.0226 - val_loss: 0.0158 - val_mae: 0.0939\n",
      "Epoch 88/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0015 - mae: 0.0231 - val_loss: 0.0145 - val_mae: 0.0912\n",
      "Epoch 89/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0015 - mae: 0.0240 - val_loss: 0.0150 - val_mae: 0.0920\n",
      "Epoch 90/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0014 - mae: 0.0227 - val_loss: 0.0136 - val_mae: 0.0887\n",
      "Epoch 91/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0014 - mae: 0.0222 - val_loss: 0.0157 - val_mae: 0.0934\n",
      "Epoch 92/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0015 - mae: 0.0238 - val_loss: 0.0158 - val_mae: 0.0936\n",
      "Epoch 93/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0014 - mae: 0.0224 - val_loss: 0.0148 - val_mae: 0.0934\n",
      "Epoch 94/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0014 - mae: 0.0243 - val_loss: 0.0159 - val_mae: 0.0937\n",
      "Epoch 95/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0014 - mae: 0.0224 - val_loss: 0.0141 - val_mae: 0.0900\n",
      "Epoch 96/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0014 - mae: 0.0226 - val_loss: 0.0155 - val_mae: 0.0927\n",
      "Epoch 97/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0014 - mae: 0.0234 - val_loss: 0.0157 - val_mae: 0.0934\n",
      "Epoch 98/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0013 - mae: 0.0220 - val_loss: 0.0164 - val_mae: 0.0946\n",
      "Epoch 99/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0016 - mae: 0.0231 - val_loss: 0.0155 - val_mae: 0.0930\n",
      "Epoch 100/100\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0015 - mae: 0.0234 - val_loss: 0.0168 - val_mae: 0.0959\n",
      "\u001b[1m155/155\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "âœ… Done with capsicum_Udupi_daily.csv | MAE=420.96, RMSE=638.35, R2=0.82, MAPE=11.65%, Accuracy=88.35%\n",
      "\n",
      "ğŸ“Š Metrics saved to tat_gqa_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "import joblib  # For saving models as .pkl\n",
    "\n",
    "# -----------------------------\n",
    "# Output directories\n",
    "# -----------------------------\n",
    "input_folder = \"dataset\"\n",
    "output_models = \"tat_gqa_output_models\"\n",
    "output_csv = \"tat_gqa_output_csv\"\n",
    "output_graphs = \"tat_gqa_output_graphs\"\n",
    "output_logs = \"tat_gqa_output_logs\"\n",
    "metrics_file = \"tat_gqa_metrics.csv\"\n",
    "\n",
    "os.makedirs(output_models, exist_ok=True)\n",
    "os.makedirs(output_csv, exist_ok=True)\n",
    "os.makedirs(output_graphs, exist_ok=True)\n",
    "os.makedirs(output_logs, exist_ok=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Function to create dataset\n",
    "# -----------------------------\n",
    "def create_dataset(data, look_back=30):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - look_back):\n",
    "        X.append(data[i:i+look_back, 0])\n",
    "        y.append(data[i+look_back, 0])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# -----------------------------\n",
    "# Grouped Query Attention Layer\n",
    "# -----------------------------\n",
    "class GroupedQueryAttention(layers.Layer):\n",
    "    def __init__(self, num_groups=2, key_dim=64, dropout_rate=0.1, **kwargs):\n",
    "        super(GroupedQueryAttention, self).__init__(**kwargs)\n",
    "        self.num_groups = num_groups\n",
    "        self.key_dim = key_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.k_dense = layers.Dense(key_dim)\n",
    "        self.v_dense = layers.Dense(key_dim)\n",
    "        self.q_dense_groups = [layers.Dense(key_dim) for _ in range(num_groups)]\n",
    "        self.dropout = layers.Dropout(dropout_rate)\n",
    "        self.output_dense = layers.Dense(key_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        K = self.k_dense(x)\n",
    "        V = self.v_dense(x)\n",
    "        group_outputs = []\n",
    "        for q_layer in self.q_dense_groups:\n",
    "            Q = q_layer(x)\n",
    "            scores = tf.matmul(Q, K, transpose_b=True) / tf.math.sqrt(tf.cast(self.key_dim, tf.float32))\n",
    "            attention_weights = tf.nn.softmax(scores, axis=-1)\n",
    "            attn_output = tf.matmul(attention_weights, V)\n",
    "            group_outputs.append(attn_output)\n",
    "        concat = tf.concat(group_outputs, axis=-1)\n",
    "        output = self.output_dense(concat)\n",
    "        output = self.dropout(output)\n",
    "        return output\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(GroupedQueryAttention, self).get_config()\n",
    "        config.update({\n",
    "            \"num_groups\": self.num_groups,\n",
    "            \"key_dim\": self.key_dim,\n",
    "            \"dropout_rate\": self.dropout_rate\n",
    "        })\n",
    "        return config\n",
    "\n",
    "# -----------------------------\n",
    "# TAT-GQA Model\n",
    "# -----------------------------\n",
    "def build_tat_gqa_model(input_shape, d_model=64, num_groups=2, ff_dim=128, dropout_rate=0.2):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = GroupedQueryAttention(num_groups=num_groups, key_dim=d_model, dropout_rate=dropout_rate)(inputs)\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x + inputs)\n",
    "    ff = layers.Dense(ff_dim, activation='relu')(x)\n",
    "    ff = layers.Dense(input_shape[1])(ff)\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x + ff)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    outputs = layers.Dense(1)(x)\n",
    "    model = models.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "                  loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# -----------------------------\n",
    "# Metrics storage\n",
    "# -----------------------------\n",
    "metrics_list = []\n",
    "\n",
    "# -----------------------------\n",
    "# Process each CSV file\n",
    "# -----------------------------\n",
    "look_back = 30\n",
    "for file in os.listdir(input_folder):\n",
    "    if file.endswith(\".csv\"):\n",
    "        file_path = os.path.join(input_folder, file)\n",
    "        print(f\"ğŸš€ Processing: {file}\")\n",
    "\n",
    "        # Load CSV\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Date parsing\n",
    "        df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "        df = df.dropna(subset=['Date'])\n",
    "        df = df.sort_values('Date')\n",
    "\n",
    "        # Fill NaNs\n",
    "        df['Average Price'].fillna(df['Average Price'].mean(), inplace=True)\n",
    "\n",
    "        # Moving averages\n",
    "        df['MA_7'] = df['Average Price'].rolling(window=7).mean()\n",
    "        df['MA_30'] = df['Average Price'].rolling(window=30).mean()\n",
    "        df['MA_7'].fillna(df['MA_7'].mean(), inplace=True)\n",
    "        df['MA_30'].fillna(df['MA_30'].mean(), inplace=True)\n",
    "\n",
    "        # Prepare data\n",
    "        values = df[['Average Price']].values.astype('float32')\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        scaled_values = scaler.fit_transform(values)\n",
    "        X, y = create_dataset(scaled_values, look_back)\n",
    "\n",
    "        # -------------------------\n",
    "        # FIX 1: Prevent IndexError\n",
    "        # -------------------------\n",
    "        if X.size == 0:\n",
    "            print(f\"âŒ Skipped {file}: Not enough rows (need > {look_back})\")\n",
    "            continue\n",
    "\n",
    "        X = np.reshape(X, (X.shape[0], X.shape[1], 1))\n",
    "\n",
    "        # Build model\n",
    "        model = build_tat_gqa_model(input_shape=(look_back,1), num_groups=2)\n",
    "\n",
    "        # Train model\n",
    "        history = model.fit(X, y, epochs=100, batch_size=16, validation_split=0.2, verbose=1)\n",
    "\n",
    "        # Save training logs\n",
    "        log_file = os.path.join(output_logs, file.replace(\".csv\", \"_tat_gqa_training.txt\"))\n",
    "        with open(log_file, \"w\") as f:\n",
    "            f.write(\"Training Loss per Epoch:\\n\")\n",
    "            for i, loss in enumerate(history.history['loss']):\n",
    "                f.write(f\"Epoch {i+1}: Loss={loss}, Val_Loss={history.history['val_loss'][i]}\\n\")\n",
    "\n",
    "        # Predictions\n",
    "        predictions = model.predict(X)\n",
    "        predictions_rescaled = scaler.inverse_transform(predictions)\n",
    "        df['Predicted'] = [np.nan]*look_back + list(predictions_rescaled.flatten())\n",
    "\n",
    "        df['Average Price'] = df['Average Price'].round(2)\n",
    "        df['Predicted'] = df['Predicted'].round(2)\n",
    "\n",
    "        # Metrics\n",
    "        y_true = df['Average Price'].values[look_back:]\n",
    "        y_pred = predictions_rescaled.flatten()\n",
    "        mae = round(mean_absolute_error(y_true, y_pred), 2)\n",
    "        rmse = round(np.sqrt(mean_squared_error(y_true, y_pred)), 2)\n",
    "        r2 = round(r2_score(y_true, y_pred), 2)\n",
    "        mape = round(np.mean(np.abs((y_true - y_pred) / y_true)) * 100, 2)\n",
    "        accuracy = round(100 - mape, 2)\n",
    "\n",
    "        metrics_list.append([file.replace(\".csv\",\"\"), mae, rmse, r2, mape, accuracy])\n",
    "\n",
    "        # Save model\n",
    "        model_file = os.path.join(output_models, file.replace(\".csv\", \"_tat_gqa_model.pkl\"))\n",
    "        joblib.dump(model, model_file)\n",
    "\n",
    "        # Save updated CSV with Actual + Predicted\n",
    "        save_df = df[['Date','Average Price','Predicted']].rename(columns={'Average Price':'Actual'})\n",
    "        updated_csv_path = os.path.join(output_csv, file.replace(\".csv\", \"_tat_gqa_updated.csv\"))\n",
    "        save_df.to_csv(updated_csv_path, index=False)\n",
    "\n",
    "        # -------------------------\n",
    "        # FIX 2: Prevent KeyError\n",
    "        # -------------------------\n",
    "        plt.figure(figsize=(12,6))\n",
    "        plt.plot(df['Date'], df['Average Price'], label='Actual', color='blue')\n",
    "        plt.plot(df['Date'], df['MA_7'], label='MA 7', color='orange')\n",
    "        plt.plot(df['Date'], df['MA_30'], label='MA 30', color='green')\n",
    "        plt.plot(df['Date'], df['Predicted'], label='Predicted (TAT-GQA)', color='red', linestyle='dashed')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Price')\n",
    "        plt.title(f'Price Prediction (TAT-GQA) - {file}')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        graph_file = os.path.join(output_graphs, file.replace(\".csv\", \"_tat_gqa_graph.png\"))\n",
    "        plt.savefig(graph_file)\n",
    "        plt.close()\n",
    "\n",
    "        print(f\"âœ… Done with {file} | MAE={mae}, RMSE={rmse}, R2={r2}, MAPE={mape}%, Accuracy={accuracy}%\\n\")\n",
    "\n",
    "# Save metrics\n",
    "metrics_df = pd.DataFrame(metrics_list, columns=['District','MAE','RMSE','R2','MAPE(%)','Accuracy(%)'])\n",
    "metrics_df.to_csv(metrics_file, index=False)\n",
    "print(f\"ğŸ“Š Metrics saved to {metrics_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a519a99-747f-4e22-9ed3-a1b4b645522e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TAT+HA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba79a3c6-d8e0-4953-8188-849f8ffa3310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Processing: capsicum_Bangalore_daily.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\2714984772.py:101: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Average Price'].fillna(df['Average Price'].mean(), inplace=True)\n",
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\2714984772.py:106: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['MA_7'].fillna(df['MA_7'].mean(), inplace=True)\n",
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\2714984772.py:107: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['MA_30'].fillna(df['MA_30'].mean(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ravik\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:219: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "Epoch 1/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - loss: 0.0424 - mae: 0.1320 - val_loss: 0.0120 - val_mae: 0.0822\n",
      "Epoch 2/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 15ms/step - loss: 0.0093 - mae: 0.0696 - val_loss: 0.0123 - val_mae: 0.0879\n",
      "Epoch 3/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 15ms/step - loss: 0.0068 - mae: 0.0594 - val_loss: 0.0115 - val_mae: 0.0799\n",
      "Epoch 4/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 15ms/step - loss: 0.0068 - mae: 0.0577 - val_loss: 0.0188 - val_mae: 0.1153\n",
      "Epoch 5/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 15ms/step - loss: 0.0072 - mae: 0.0613 - val_loss: 0.0138 - val_mae: 0.0948\n",
      "Epoch 6/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - loss: 0.0067 - mae: 0.0585 - val_loss: 0.0157 - val_mae: 0.1023\n",
      "Epoch 7/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 15ms/step - loss: 0.0066 - mae: 0.0576 - val_loss: 0.0147 - val_mae: 0.0993\n",
      "Epoch 8/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - loss: 0.0063 - mae: 0.0567 - val_loss: 0.0153 - val_mae: 0.1015\n",
      "Epoch 9/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - loss: 0.0062 - mae: 0.0552 - val_loss: 0.0176 - val_mae: 0.1070\n",
      "Epoch 10/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - loss: 0.0060 - mae: 0.0544 - val_loss: 0.0162 - val_mae: 0.1046\n",
      "Epoch 11/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - loss: 0.0066 - mae: 0.0567 - val_loss: 0.0115 - val_mae: 0.0805\n",
      "Epoch 12/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 15ms/step - loss: 0.0062 - mae: 0.0551 - val_loss: 0.0126 - val_mae: 0.0885\n",
      "Epoch 13/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 15ms/step - loss: 0.0059 - mae: 0.0534 - val_loss: 0.0116 - val_mae: 0.0830\n",
      "Epoch 14/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 16ms/step - loss: 0.0061 - mae: 0.0547 - val_loss: 0.0116 - val_mae: 0.0823\n",
      "Epoch 15/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 15ms/step - loss: 0.0063 - mae: 0.0548 - val_loss: 0.0145 - val_mae: 0.0976\n",
      "Epoch 16/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 15ms/step - loss: 0.0059 - mae: 0.0535 - val_loss: 0.0147 - val_mae: 0.0968\n",
      "Epoch 17/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 15ms/step - loss: 0.0057 - mae: 0.0524 - val_loss: 0.0152 - val_mae: 0.0984\n",
      "Epoch 18/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - loss: 0.0064 - mae: 0.0553 - val_loss: 0.0156 - val_mae: 0.0987\n",
      "Epoch 19/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 15ms/step - loss: 0.0061 - mae: 0.0540 - val_loss: 0.0146 - val_mae: 0.0934\n",
      "Epoch 20/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - loss: 0.0061 - mae: 0.0543 - val_loss: 0.0145 - val_mae: 0.0971\n",
      "Epoch 21/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 15ms/step - loss: 0.0062 - mae: 0.0548 - val_loss: 0.0119 - val_mae: 0.0847\n",
      "Epoch 22/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - loss: 0.0060 - mae: 0.0534 - val_loss: 0.0132 - val_mae: 0.0890\n",
      "Epoch 23/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - loss: 0.0061 - mae: 0.0537 - val_loss: 0.0148 - val_mae: 0.0959\n",
      "Epoch 24/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 15ms/step - loss: 0.0061 - mae: 0.0533 - val_loss: 0.0121 - val_mae: 0.0841\n",
      "Epoch 25/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - loss: 0.0058 - mae: 0.0525 - val_loss: 0.0126 - val_mae: 0.0892\n",
      "Epoch 26/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 14ms/step - loss: 0.0060 - mae: 0.0539 - val_loss: 0.0122 - val_mae: 0.0859\n",
      "Epoch 27/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 15ms/step - loss: 0.0060 - mae: 0.0530 - val_loss: 0.0123 - val_mae: 0.0870\n",
      "Epoch 28/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - loss: 0.0059 - mae: 0.0526 - val_loss: 0.0119 - val_mae: 0.0841\n",
      "Epoch 29/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 15ms/step - loss: 0.0058 - mae: 0.0529 - val_loss: 0.0147 - val_mae: 0.0965\n",
      "Epoch 30/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 14ms/step - loss: 0.0057 - mae: 0.0511 - val_loss: 0.0121 - val_mae: 0.0841\n",
      "Epoch 31/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 15ms/step - loss: 0.0058 - mae: 0.0528 - val_loss: 0.0169 - val_mae: 0.1025\n",
      "Epoch 32/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 14ms/step - loss: 0.0057 - mae: 0.0520 - val_loss: 0.0119 - val_mae: 0.0827\n",
      "Epoch 33/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - loss: 0.0060 - mae: 0.0527 - val_loss: 0.0125 - val_mae: 0.0847\n",
      "Epoch 34/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - loss: 0.0058 - mae: 0.0524 - val_loss: 0.0118 - val_mae: 0.0809\n",
      "Epoch 35/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - loss: 0.0057 - mae: 0.0525 - val_loss: 0.0135 - val_mae: 0.0884\n",
      "Epoch 36/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - loss: 0.0057 - mae: 0.0520 - val_loss: 0.0122 - val_mae: 0.0864\n",
      "Epoch 37/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 14ms/step - loss: 0.0059 - mae: 0.0525 - val_loss: 0.0117 - val_mae: 0.0817\n",
      "Epoch 38/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 15ms/step - loss: 0.0056 - mae: 0.0515 - val_loss: 0.0120 - val_mae: 0.0830\n",
      "Epoch 39/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 14ms/step - loss: 0.0058 - mae: 0.0527 - val_loss: 0.0155 - val_mae: 0.0989\n",
      "Epoch 40/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 14ms/step - loss: 0.0060 - mae: 0.0539 - val_loss: 0.0132 - val_mae: 0.0896\n",
      "Epoch 41/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - loss: 0.0057 - mae: 0.0516 - val_loss: 0.0127 - val_mae: 0.0872\n",
      "Epoch 42/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 15ms/step - loss: 0.0052 - mae: 0.0496 - val_loss: 0.0121 - val_mae: 0.0846\n",
      "Epoch 43/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 14ms/step - loss: 0.0060 - mae: 0.0523 - val_loss: 0.0125 - val_mae: 0.0855\n",
      "Epoch 44/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 15ms/step - loss: 0.0059 - mae: 0.0526 - val_loss: 0.0122 - val_mae: 0.0842\n",
      "Epoch 45/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 15ms/step - loss: 0.0056 - mae: 0.0511 - val_loss: 0.0122 - val_mae: 0.0860\n",
      "Epoch 46/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 15ms/step - loss: 0.0058 - mae: 0.0525 - val_loss: 0.0117 - val_mae: 0.0812\n",
      "Epoch 47/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - loss: 0.0060 - mae: 0.0532 - val_loss: 0.0118 - val_mae: 0.0816\n",
      "Epoch 48/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 12ms/step - loss: 0.0057 - mae: 0.0525 - val_loss: 0.0125 - val_mae: 0.0857\n",
      "Epoch 49/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - loss: 0.0057 - mae: 0.0515 - val_loss: 0.0130 - val_mae: 0.0886\n",
      "Epoch 50/50\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 14ms/step - loss: 0.0062 - mae: 0.0534 - val_loss: 0.0156 - val_mae: 0.0982\n",
      "\u001b[1m290/290\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step\n",
      "âœ… Done with capsicum_Bangalore_daily.csv | MAE=621.75, RMSE=896.75, R2=0.49, MAPE=21.25%, Accuracy=78.75%\n",
      "\n",
      "ğŸš€ Processing: capsicum_Belgaum_daily.csv\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\2714984772.py:101: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Average Price'].fillna(df['Average Price'].mean(), inplace=True)\n",
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\2714984772.py:106: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['MA_7'].fillna(df['MA_7'].mean(), inplace=True)\n",
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\2714984772.py:107: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['MA_30'].fillna(df['MA_30'].mean(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 20ms/step - loss: 0.2671 - mae: 0.3698 - val_loss: 0.0050 - val_mae: 0.0643\n",
      "Epoch 2/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0168 - mae: 0.0980 - val_loss: 0.0043 - val_mae: 0.0598\n",
      "Epoch 3/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0168 - mae: 0.1006 - val_loss: 0.0062 - val_mae: 0.0717\n",
      "Epoch 4/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0166 - mae: 0.0997 - val_loss: 0.0043 - val_mae: 0.0600\n",
      "Epoch 5/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0159 - mae: 0.0904 - val_loss: 0.0032 - val_mae: 0.0506\n",
      "Epoch 6/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0124 - mae: 0.0813 - val_loss: 0.0089 - val_mae: 0.0859\n",
      "Epoch 7/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0139 - mae: 0.0886 - val_loss: 0.0034 - val_mae: 0.0388\n",
      "Epoch 8/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0131 - mae: 0.0859 - val_loss: 0.0039 - val_mae: 0.0568\n",
      "Epoch 9/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0108 - mae: 0.0724 - val_loss: 0.0038 - val_mae: 0.0399\n",
      "Epoch 10/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0090 - mae: 0.0667 - val_loss: 0.0036 - val_mae: 0.0542\n",
      "Epoch 11/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0083 - mae: 0.0682 - val_loss: 0.0060 - val_mae: 0.0705\n",
      "Epoch 12/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0109 - mae: 0.0746 - val_loss: 0.0068 - val_mae: 0.0649\n",
      "Epoch 13/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0109 - mae: 0.0739 - val_loss: 0.0028 - val_mae: 0.0453\n",
      "Epoch 14/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0093 - mae: 0.0709 - val_loss: 0.0079 - val_mae: 0.0729\n",
      "Epoch 15/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0138 - mae: 0.0843 - val_loss: 0.0027 - val_mae: 0.0434\n",
      "Epoch 16/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0087 - mae: 0.0633 - val_loss: 0.0032 - val_mae: 0.0506\n",
      "Epoch 17/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0100 - mae: 0.0657 - val_loss: 0.0028 - val_mae: 0.0384\n",
      "Epoch 18/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0103 - mae: 0.0701 - val_loss: 0.0028 - val_mae: 0.0387\n",
      "Epoch 19/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0147 - mae: 0.0860 - val_loss: 0.0044 - val_mae: 0.0604\n",
      "Epoch 20/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0100 - mae: 0.0686 - val_loss: 0.0029 - val_mae: 0.0476\n",
      "Epoch 21/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0090 - mae: 0.0666 - val_loss: 0.0028 - val_mae: 0.0465\n",
      "Epoch 22/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0091 - mae: 0.0655 - val_loss: 0.0027 - val_mae: 0.0400\n",
      "Epoch 23/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0091 - mae: 0.0610 - val_loss: 0.0028 - val_mae: 0.0387\n",
      "Epoch 24/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0093 - mae: 0.0638 - val_loss: 0.0037 - val_mae: 0.0398\n",
      "Epoch 25/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0071 - mae: 0.0578 - val_loss: 0.0028 - val_mae: 0.0385\n",
      "Epoch 26/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0075 - mae: 0.0582 - val_loss: 0.0037 - val_mae: 0.0552\n",
      "Epoch 27/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0088 - mae: 0.0651 - val_loss: 0.0026 - val_mae: 0.0428\n",
      "Epoch 28/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0099 - mae: 0.0652 - val_loss: 0.0029 - val_mae: 0.0480\n",
      "Epoch 29/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0079 - mae: 0.0632 - val_loss: 0.0026 - val_mae: 0.0415\n",
      "Epoch 30/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0082 - mae: 0.0612 - val_loss: 0.0051 - val_mae: 0.0524\n",
      "Epoch 31/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0104 - mae: 0.0667 - val_loss: 0.0038 - val_mae: 0.0408\n",
      "Epoch 32/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0096 - mae: 0.0695 - val_loss: 0.0032 - val_mae: 0.0511\n",
      "Epoch 33/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0092 - mae: 0.0624 - val_loss: 0.0074 - val_mae: 0.0779\n",
      "Epoch 34/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0092 - mae: 0.0679 - val_loss: 0.0026 - val_mae: 0.0400\n",
      "Epoch 35/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0078 - mae: 0.0605 - val_loss: 0.0034 - val_mae: 0.0387\n",
      "Epoch 36/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0086 - mae: 0.0599 - val_loss: 0.0052 - val_mae: 0.0533\n",
      "Epoch 37/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0090 - mae: 0.0652 - val_loss: 0.0053 - val_mae: 0.0540\n",
      "Epoch 38/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0089 - mae: 0.0609 - val_loss: 0.0026 - val_mae: 0.0382\n",
      "Epoch 39/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0062 - mae: 0.0541 - val_loss: 0.0027 - val_mae: 0.0456\n",
      "Epoch 40/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0072 - mae: 0.0562 - val_loss: 0.0034 - val_mae: 0.0524\n",
      "Epoch 41/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0099 - mae: 0.0669 - val_loss: 0.0025 - val_mae: 0.0393\n",
      "Epoch 42/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0081 - mae: 0.0569 - val_loss: 0.0036 - val_mae: 0.0541\n",
      "Epoch 43/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0079 - mae: 0.0596 - val_loss: 0.0025 - val_mae: 0.0419\n",
      "Epoch 44/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0083 - mae: 0.0621 - val_loss: 0.0036 - val_mae: 0.0544\n",
      "Epoch 45/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0071 - mae: 0.0591 - val_loss: 0.0025 - val_mae: 0.0417\n",
      "Epoch 46/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0075 - mae: 0.0593 - val_loss: 0.0030 - val_mae: 0.0487\n",
      "Epoch 47/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0087 - mae: 0.0625 - val_loss: 0.0026 - val_mae: 0.0387\n",
      "Epoch 48/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0067 - mae: 0.0555 - val_loss: 0.0031 - val_mae: 0.0378\n",
      "Epoch 49/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0078 - mae: 0.0592 - val_loss: 0.0026 - val_mae: 0.0439\n",
      "Epoch 50/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0084 - mae: 0.0619 - val_loss: 0.0025 - val_mae: 0.0399\n",
      "\u001b[1m38/38\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
      "âœ… Done with capsicum_Belgaum_daily.csv | MAE=362.93, RMSE=555.07, R2=0.51, MAPE=13.06%, Accuracy=86.94%\n",
      "\n",
      "ğŸš€ Processing: capsicum_Bellary_daily.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\2714984772.py:101: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Average Price'].fillna(df['Average Price'].mean(), inplace=True)\n",
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\2714984772.py:106: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['MA_7'].fillna(df['MA_7'].mean(), inplace=True)\n",
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\2714984772.py:107: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['MA_30'].fillna(df['MA_30'].mean(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 133ms/step - loss: 1.1233 - mae: 0.9218 - val_loss: 0.1682 - val_mae: 0.3495\n",
      "Epoch 2/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.2179 - mae: 0.3964 - val_loss: 0.5094 - val_mae: 0.6691\n",
      "Epoch 3/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.1231 - mae: 0.2916 - val_loss: 0.0663 - val_mae: 0.2168\n",
      "Epoch 4/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.1380 - mae: 0.3173 - val_loss: 0.4528 - val_mae: 0.6258\n",
      "Epoch 5/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.1126 - mae: 0.2808 - val_loss: 0.1957 - val_mae: 0.3876\n",
      "Epoch 6/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0548 - mae: 0.1994 - val_loss: 0.0936 - val_mae: 0.2575\n",
      "Epoch 7/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0602 - mae: 0.2053 - val_loss: 0.3050 - val_mae: 0.4968\n",
      "Epoch 8/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0443 - mae: 0.1731 - val_loss: 0.1493 - val_mae: 0.3416\n",
      "Epoch 9/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0359 - mae: 0.1583 - val_loss: 0.1579 - val_mae: 0.3530\n",
      "Epoch 10/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0381 - mae: 0.1669 - val_loss: 0.1865 - val_mae: 0.3840\n",
      "Epoch 11/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0392 - mae: 0.1508 - val_loss: 0.1511 - val_mae: 0.3479\n",
      "Epoch 12/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0338 - mae: 0.1545 - val_loss: 0.1862 - val_mae: 0.3863\n",
      "Epoch 13/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0334 - mae: 0.1532 - val_loss: 0.1478 - val_mae: 0.3461\n",
      "Epoch 14/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0336 - mae: 0.1488 - val_loss: 0.1644 - val_mae: 0.3653\n",
      "Epoch 15/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0321 - mae: 0.1462 - val_loss: 0.1734 - val_mae: 0.3754\n",
      "Epoch 16/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0298 - mae: 0.1357 - val_loss: 0.1524 - val_mae: 0.3529\n",
      "Epoch 17/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0379 - mae: 0.1587 - val_loss: 0.2012 - val_mae: 0.4030\n",
      "Epoch 18/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0352 - mae: 0.1571 - val_loss: 0.1457 - val_mae: 0.3449\n",
      "Epoch 19/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0375 - mae: 0.1625 - val_loss: 0.1613 - val_mae: 0.3629\n",
      "Epoch 20/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0356 - mae: 0.1514 - val_loss: 0.1964 - val_mae: 0.3985\n",
      "Epoch 21/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0326 - mae: 0.1498 - val_loss: 0.1477 - val_mae: 0.3477\n",
      "Epoch 22/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0384 - mae: 0.1672 - val_loss: 0.1640 - val_mae: 0.3662\n",
      "Epoch 23/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0324 - mae: 0.1489 - val_loss: 0.1448 - val_mae: 0.3446\n",
      "Epoch 24/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0339 - mae: 0.1518 - val_loss: 0.1736 - val_mae: 0.3768\n",
      "Epoch 25/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0335 - mae: 0.1582 - val_loss: 0.1517 - val_mae: 0.3531\n",
      "Epoch 26/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0330 - mae: 0.1454 - val_loss: 0.1520 - val_mae: 0.3534\n",
      "Epoch 27/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0287 - mae: 0.1409 - val_loss: 0.1582 - val_mae: 0.3607\n",
      "Epoch 28/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0364 - mae: 0.1534 - val_loss: 0.1582 - val_mae: 0.3608\n",
      "Epoch 29/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0364 - mae: 0.1574 - val_loss: 0.1519 - val_mae: 0.3536\n",
      "Epoch 30/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0363 - mae: 0.1576 - val_loss: 0.1589 - val_mae: 0.3615\n",
      "Epoch 31/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0367 - mae: 0.1625 - val_loss: 0.2012 - val_mae: 0.4041\n",
      "Epoch 32/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0265 - mae: 0.1342 - val_loss: 0.1372 - val_mae: 0.3360\n",
      "Epoch 33/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0332 - mae: 0.1531 - val_loss: 0.1962 - val_mae: 0.3994\n",
      "Epoch 34/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0407 - mae: 0.1703 - val_loss: 0.1832 - val_mae: 0.3869\n",
      "Epoch 35/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0336 - mae: 0.1583 - val_loss: 0.1549 - val_mae: 0.3570\n",
      "Epoch 36/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0378 - mae: 0.1612 - val_loss: 0.1494 - val_mae: 0.3508\n",
      "Epoch 37/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0277 - mae: 0.1347 - val_loss: 0.1862 - val_mae: 0.3902\n",
      "Epoch 38/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0378 - mae: 0.1660 - val_loss: 0.1599 - val_mae: 0.3629\n",
      "Epoch 39/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0277 - mae: 0.1307 - val_loss: 0.1854 - val_mae: 0.3892\n",
      "Epoch 40/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0409 - mae: 0.1671 - val_loss: 0.1591 - val_mae: 0.3617\n",
      "Epoch 41/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0339 - mae: 0.1496 - val_loss: 0.1244 - val_mae: 0.3191\n",
      "Epoch 42/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0321 - mae: 0.1524 - val_loss: 0.1733 - val_mae: 0.3769\n",
      "Epoch 43/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0350 - mae: 0.1567 - val_loss: 0.1489 - val_mae: 0.3500\n",
      "Epoch 44/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0317 - mae: 0.1497 - val_loss: 0.1818 - val_mae: 0.3855\n",
      "Epoch 45/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0337 - mae: 0.1494 - val_loss: 0.1778 - val_mae: 0.3817\n",
      "Epoch 46/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0323 - mae: 0.1427 - val_loss: 0.1412 - val_mae: 0.3410\n",
      "Epoch 47/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0278 - mae: 0.1356 - val_loss: 0.1788 - val_mae: 0.3825\n",
      "Epoch 48/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0347 - mae: 0.1466 - val_loss: 0.1692 - val_mae: 0.3726\n",
      "Epoch 49/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0292 - mae: 0.1450 - val_loss: 0.1597 - val_mae: 0.3624\n",
      "Epoch 50/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0415 - mae: 0.1619 - val_loss: 0.2161 - val_mae: 0.4179\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 117ms/step\n",
      "âœ… Done with capsicum_Bellary_daily.csv | MAE=425.26, RMSE=551.25, R2=-0.17, MAPE=16.47%, Accuracy=83.53%\n",
      "\n",
      "ğŸš€ Processing: capsicum_Chikmagalur_daily.csv\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\2714984772.py:101: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Average Price'].fillna(df['Average Price'].mean(), inplace=True)\n",
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\2714984772.py:106: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['MA_7'].fillna(df['MA_7'].mean(), inplace=True)\n",
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\2714984772.py:107: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['MA_30'].fillna(df['MA_30'].mean(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 17ms/step - loss: 0.3215 - mae: 0.3711 - val_loss: 0.0392 - val_mae: 0.1616\n",
      "Epoch 2/50\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0135 - mae: 0.0941 - val_loss: 0.0563 - val_mae: 0.1991\n",
      "Epoch 3/50\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - loss: 0.0143 - mae: 0.0958 - val_loss: 0.0419 - val_mae: 0.1706\n",
      "Epoch 4/50\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - loss: 0.0106 - mae: 0.0825 - val_loss: 0.0487 - val_mae: 0.1838\n",
      "Epoch 5/50\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - loss: 0.0100 - mae: 0.0809 - val_loss: 0.0341 - val_mae: 0.1538\n",
      "Epoch 6/50\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - loss: 0.0088 - mae: 0.0758 - val_loss: 0.0315 - val_mae: 0.1464\n",
      "Epoch 7/50\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - loss: 0.0093 - mae: 0.0780 - val_loss: 0.0310 - val_mae: 0.1473\n",
      "Epoch 8/50\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - loss: 0.0077 - mae: 0.0707 - val_loss: 0.0434 - val_mae: 0.1789\n",
      "Epoch 9/50\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - loss: 0.0075 - mae: 0.0704 - val_loss: 0.0357 - val_mae: 0.1659\n",
      "Epoch 10/50\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - loss: 0.0072 - mae: 0.0694 - val_loss: 0.0333 - val_mae: 0.1592\n",
      "Epoch 11/50\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - loss: 0.0061 - mae: 0.0635 - val_loss: 0.0294 - val_mae: 0.1499\n",
      "Epoch 12/50\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - loss: 0.0062 - mae: 0.0638 - val_loss: 0.0344 - val_mae: 0.1634\n",
      "Epoch 13/50\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - loss: 0.0071 - mae: 0.0673 - val_loss: 0.0279 - val_mae: 0.1283\n",
      "Epoch 14/50\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - loss: 0.0067 - mae: 0.0654 - val_loss: 0.0299 - val_mae: 0.1313\n",
      "Epoch 15/50\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - loss: 0.0063 - mae: 0.0630 - val_loss: 0.0279 - val_mae: 0.1346\n",
      "Epoch 16/50\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - loss: 0.0053 - mae: 0.0572 - val_loss: 0.0270 - val_mae: 0.1263\n",
      "Epoch 17/50\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - loss: 0.0051 - mae: 0.0563 - val_loss: 0.0264 - val_mae: 0.1251\n",
      "Epoch 18/50\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - loss: 0.0046 - mae: 0.0532 - val_loss: 0.0262 - val_mae: 0.1251\n",
      "Epoch 19/50\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - loss: 0.0064 - mae: 0.0640 - val_loss: 0.0251 - val_mae: 0.1312\n",
      "Epoch 20/50\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - loss: 0.0052 - mae: 0.0567 - val_loss: 0.0248 - val_mae: 0.1352\n",
      "Epoch 21/50\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0050 - mae: 0.0554 - val_loss: 0.0232 - val_mae: 0.1215\n",
      "Epoch 22/50\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - loss: 0.0052 - mae: 0.0568 - val_loss: 0.0249 - val_mae: 0.1201\n",
      "Epoch 23/50\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - loss: 0.0062 - mae: 0.0628 - val_loss: 0.0266 - val_mae: 0.1250\n",
      "Epoch 24/50\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - loss: 0.0052 - mae: 0.0567 - val_loss: 0.0228 - val_mae: 0.1261\n",
      "Epoch 25/50\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - loss: 0.0045 - mae: 0.0520 - val_loss: 0.0222 - val_mae: 0.1151\n",
      "Epoch 26/50\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - loss: 0.0052 - mae: 0.0568 - val_loss: 0.0208 - val_mae: 0.1180\n",
      "Epoch 27/50\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - loss: 0.0055 - mae: 0.0591 - val_loss: 0.0211 - val_mae: 0.1226\n",
      "Epoch 28/50\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - loss: 0.0046 - mae: 0.0519 - val_loss: 0.0214 - val_mae: 0.1135\n",
      "Epoch 29/50\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.0048 - mae: 0.0545 - val_loss: 0.0222 - val_mae: 0.1270\n",
      "Epoch 30/50\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.0056 - mae: 0.0587 - val_loss: 0.0199 - val_mae: 0.1145\n",
      "Epoch 31/50\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.0044 - mae: 0.0513 - val_loss: 0.0199 - val_mae: 0.1086\n",
      "Epoch 32/50\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0048 - mae: 0.0545 - val_loss: 0.0201 - val_mae: 0.1092\n",
      "Epoch 33/50\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.0046 - mae: 0.0529 - val_loss: 0.0193 - val_mae: 0.1095\n",
      "Epoch 34/50\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0048 - mae: 0.0536 - val_loss: 0.0192 - val_mae: 0.1101\n",
      "Epoch 35/50\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.0045 - mae: 0.0525 - val_loss: 0.0192 - val_mae: 0.1119\n",
      "Epoch 36/50\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.0049 - mae: 0.0551 - val_loss: 0.0191 - val_mae: 0.1116\n",
      "Epoch 37/50\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - loss: 0.0045 - mae: 0.0518 - val_loss: 0.0199 - val_mae: 0.1169\n",
      "Epoch 38/50\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - loss: 0.0046 - mae: 0.0526 - val_loss: 0.0248 - val_mae: 0.1247\n",
      "Epoch 39/50\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - loss: 0.0053 - mae: 0.0568 - val_loss: 0.0193 - val_mae: 0.1056\n",
      "Epoch 40/50\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - loss: 0.0049 - mae: 0.0548 - val_loss: 0.0188 - val_mae: 0.1039\n",
      "Epoch 41/50\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - loss: 0.0044 - mae: 0.0508 - val_loss: 0.0202 - val_mae: 0.1130\n",
      "Epoch 42/50\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - loss: 0.0046 - mae: 0.0528 - val_loss: 0.0188 - val_mae: 0.1042\n",
      "Epoch 43/50\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0048 - mae: 0.0529 - val_loss: 0.0197 - val_mae: 0.1176\n",
      "Epoch 44/50\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - loss: 0.0052 - mae: 0.0568 - val_loss: 0.0187 - val_mae: 0.1087\n",
      "Epoch 45/50\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0050 - mae: 0.0557 - val_loss: 0.0204 - val_mae: 0.1214\n",
      "Epoch 46/50\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0046 - mae: 0.0523 - val_loss: 0.0186 - val_mae: 0.1042\n",
      "Epoch 47/50\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.0045 - mae: 0.0520 - val_loss: 0.0185 - val_mae: 0.1060\n",
      "Epoch 48/50\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0042 - mae: 0.0495 - val_loss: 0.0186 - val_mae: 0.1041\n",
      "Epoch 49/50\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - loss: 0.0044 - mae: 0.0505 - val_loss: 0.0213 - val_mae: 0.1148\n",
      "Epoch 50/50\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0046 - mae: 0.0528 - val_loss: 0.0186 - val_mae: 0.1052\n",
      "WARNING:tensorflow:5 out of the last 42 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000020A672D0AE0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step\n",
      "âœ… Done with capsicum_Chikmagalur_daily.csv | MAE=215.54, RMSE=294.35, R2=0.75, MAPE=13.16%, Accuracy=86.84%\n",
      "\n",
      "ğŸš€ Processing: capsicum_Davangere_daily.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\2714984772.py:101: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Average Price'].fillna(df['Average Price'].mean(), inplace=True)\n",
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\2714984772.py:106: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['MA_7'].fillna(df['MA_7'].mean(), inplace=True)\n",
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\2714984772.py:107: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['MA_30'].fillna(df['MA_30'].mean(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 18ms/step - loss: 0.1566 - mae: 0.2487 - val_loss: 0.0166 - val_mae: 0.0927\n",
      "Epoch 2/50\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 18ms/step - loss: 0.0081 - mae: 0.0708 - val_loss: 0.0135 - val_mae: 0.0791\n",
      "Epoch 3/50\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0055 - mae: 0.0574 - val_loss: 0.0149 - val_mae: 0.0871\n",
      "Epoch 4/50\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 18ms/step - loss: 0.0035 - mae: 0.0452 - val_loss: 0.0114 - val_mae: 0.0723\n",
      "Epoch 5/50\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 18ms/step - loss: 0.0028 - mae: 0.0403 - val_loss: 0.0078 - val_mae: 0.0540\n",
      "Epoch 6/50\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0026 - mae: 0.0380 - val_loss: 0.0076 - val_mae: 0.0568\n",
      "Epoch 7/50\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 18ms/step - loss: 0.0020 - mae: 0.0331 - val_loss: 0.0072 - val_mae: 0.0537\n",
      "Epoch 8/50\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0020 - mae: 0.0317 - val_loss: 0.0072 - val_mae: 0.0550\n",
      "Epoch 9/50\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0019 - mae: 0.0310 - val_loss: 0.0076 - val_mae: 0.0525\n",
      "Epoch 10/50\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 0.0019 - mae: 0.0315 - val_loss: 0.0075 - val_mae: 0.0599\n",
      "Epoch 11/50\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 18ms/step - loss: 0.0018 - mae: 0.0304 - val_loss: 0.0082 - val_mae: 0.0669\n",
      "Epoch 12/50\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 18ms/step - loss: 0.0023 - mae: 0.0350 - val_loss: 0.0070 - val_mae: 0.0533\n",
      "Epoch 13/50\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 18ms/step - loss: 0.0017 - mae: 0.0289 - val_loss: 0.0068 - val_mae: 0.0523\n",
      "Epoch 14/50\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 0.0018 - mae: 0.0296 - val_loss: 0.0066 - val_mae: 0.0517\n",
      "Epoch 15/50\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 18ms/step - loss: 0.0018 - mae: 0.0311 - val_loss: 0.0070 - val_mae: 0.0565\n",
      "Epoch 16/50\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 18ms/step - loss: 0.0017 - mae: 0.0292 - val_loss: 0.0076 - val_mae: 0.0562\n",
      "Epoch 17/50\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0019 - mae: 0.0307 - val_loss: 0.0066 - val_mae: 0.0533\n",
      "Epoch 18/50\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 0.0017 - mae: 0.0289 - val_loss: 0.0071 - val_mae: 0.0596\n",
      "Epoch 19/50\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 0.0016 - mae: 0.0285 - val_loss: 0.0062 - val_mae: 0.0487\n",
      "Epoch 20/50\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 0.0018 - mae: 0.0305 - val_loss: 0.0061 - val_mae: 0.0488\n",
      "Epoch 21/50\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 18ms/step - loss: 0.0016 - mae: 0.0278 - val_loss: 0.0066 - val_mae: 0.0545\n",
      "Epoch 22/50\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 18ms/step - loss: 0.0018 - mae: 0.0296 - val_loss: 0.0065 - val_mae: 0.0516\n",
      "Epoch 23/50\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0016 - mae: 0.0283 - val_loss: 0.0061 - val_mae: 0.0494\n",
      "Epoch 24/50\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 18ms/step - loss: 0.0016 - mae: 0.0276 - val_loss: 0.0061 - val_mae: 0.0494\n",
      "Epoch 25/50\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 0.0017 - mae: 0.0282 - val_loss: 0.0073 - val_mae: 0.0600\n",
      "Epoch 26/50\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 18ms/step - loss: 0.0017 - mae: 0.0283 - val_loss: 0.0063 - val_mae: 0.0511\n",
      "Epoch 27/50\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 18ms/step - loss: 0.0016 - mae: 0.0282 - val_loss: 0.0075 - val_mae: 0.0588\n",
      "Epoch 28/50\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 18ms/step - loss: 0.0017 - mae: 0.0280 - val_loss: 0.0101 - val_mae: 0.0727\n",
      "Epoch 29/50\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 18ms/step - loss: 0.0017 - mae: 0.0286 - val_loss: 0.0066 - val_mae: 0.0514\n",
      "Epoch 30/50\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 0.0016 - mae: 0.0275 - val_loss: 0.0075 - val_mae: 0.0568\n",
      "Epoch 31/50\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 18ms/step - loss: 0.0015 - mae: 0.0267 - val_loss: 0.0063 - val_mae: 0.0503\n",
      "Epoch 32/50\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 0.0018 - mae: 0.0290 - val_loss: 0.0080 - val_mae: 0.0563\n",
      "Epoch 33/50\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 0.0015 - mae: 0.0271 - val_loss: 0.0085 - val_mae: 0.0613\n",
      "Epoch 34/50\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 0.0016 - mae: 0.0273 - val_loss: 0.0094 - val_mae: 0.0672\n",
      "Epoch 35/50\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 18ms/step - loss: 0.0016 - mae: 0.0281 - val_loss: 0.0069 - val_mae: 0.0516\n",
      "Epoch 36/50\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 0.0017 - mae: 0.0292 - val_loss: 0.0079 - val_mae: 0.0578\n",
      "Epoch 37/50\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 0.0016 - mae: 0.0276 - val_loss: 0.0063 - val_mae: 0.0503\n",
      "Epoch 38/50\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 0.0015 - mae: 0.0263 - val_loss: 0.0152 - val_mae: 0.0877\n",
      "Epoch 39/50\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 0.0015 - mae: 0.0264 - val_loss: 0.0076 - val_mae: 0.0532\n",
      "Epoch 40/50\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0017 - mae: 0.0279 - val_loss: 0.0092 - val_mae: 0.0589\n",
      "Epoch 41/50\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 18ms/step - loss: 0.0016 - mae: 0.0262 - val_loss: 0.0106 - val_mae: 0.0656\n",
      "Epoch 42/50\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - loss: 0.0015 - mae: 0.0272 - val_loss: 0.0080 - val_mae: 0.0558\n",
      "Epoch 43/50\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 0.0015 - mae: 0.0268 - val_loss: 0.0108 - val_mae: 0.0678\n",
      "Epoch 44/50\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.0015 - mae: 0.0265 - val_loss: 0.0084 - val_mae: 0.0582\n",
      "Epoch 45/50\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.0015 - mae: 0.0266 - val_loss: 0.0070 - val_mae: 0.0536\n",
      "Epoch 46/50\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.0016 - mae: 0.0278 - val_loss: 0.0064 - val_mae: 0.0509\n",
      "Epoch 47/50\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.0016 - mae: 0.0273 - val_loss: 0.0077 - val_mae: 0.0552\n",
      "Epoch 48/50\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.0015 - mae: 0.0262 - val_loss: 0.0069 - val_mae: 0.0518\n",
      "Epoch 49/50\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.0015 - mae: 0.0271 - val_loss: 0.0096 - val_mae: 0.0648\n",
      "Epoch 50/50\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.0017 - mae: 0.0280 - val_loss: 0.0118 - val_mae: 0.0648\n",
      "\u001b[1m140/140\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n",
      "âœ… Done with capsicum_Davangere_daily.csv | MAE=584.64, RMSE=975.84, R2=0.68, MAPE=18.93%, Accuracy=81.07%\n",
      "\n",
      "ğŸš€ Processing: capsicum_Dharwad_daily.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\2714984772.py:101: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Average Price'].fillna(df['Average Price'].mean(), inplace=True)\n",
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\2714984772.py:106: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['MA_7'].fillna(df['MA_7'].mean(), inplace=True)\n",
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\2714984772.py:107: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['MA_30'].fillna(df['MA_30'].mean(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 65ms/step - loss: 0.6285 - mae: 0.6527 - val_loss: 0.0277 - val_mae: 0.1519\n",
      "Epoch 2/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.1304 - mae: 0.2995 - val_loss: 0.1218 - val_mae: 0.3422\n",
      "Epoch 3/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0990 - mae: 0.2582 - val_loss: 0.2577 - val_mae: 0.5029\n",
      "Epoch 4/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0844 - mae: 0.2460 - val_loss: 0.2805 - val_mae: 0.5250\n",
      "Epoch 5/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1037 - mae: 0.2591 - val_loss: 0.2231 - val_mae: 0.4672\n",
      "Epoch 6/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0900 - mae: 0.2392 - val_loss: 0.1438 - val_mae: 0.3727\n",
      "Epoch 7/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0937 - mae: 0.2466 - val_loss: 0.1892 - val_mae: 0.4291\n",
      "Epoch 8/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0928 - mae: 0.2488 - val_loss: 0.2019 - val_mae: 0.4436\n",
      "Epoch 9/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0860 - mae: 0.2437 - val_loss: 0.3629 - val_mae: 0.5981\n",
      "Epoch 10/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1151 - mae: 0.2716 - val_loss: 0.2285 - val_mae: 0.4726\n",
      "Epoch 11/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.1248 - mae: 0.2828 - val_loss: 0.2131 - val_mae: 0.4559\n",
      "Epoch 12/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0943 - mae: 0.2538 - val_loss: 0.2077 - val_mae: 0.4499\n",
      "Epoch 13/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0892 - mae: 0.2444 - val_loss: 0.0666 - val_mae: 0.2475\n",
      "Epoch 14/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0861 - mae: 0.2465 - val_loss: 0.0587 - val_mae: 0.2310\n",
      "Epoch 15/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1034 - mae: 0.2696 - val_loss: 0.0950 - val_mae: 0.2992\n",
      "Epoch 16/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0852 - mae: 0.2449 - val_loss: 0.1900 - val_mae: 0.4296\n",
      "Epoch 17/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0801 - mae: 0.2340 - val_loss: 0.1509 - val_mae: 0.3814\n",
      "Epoch 18/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0730 - mae: 0.2287 - val_loss: 0.1011 - val_mae: 0.3093\n",
      "Epoch 19/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0921 - mae: 0.2558 - val_loss: 0.0677 - val_mae: 0.2496\n",
      "Epoch 20/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0794 - mae: 0.2332 - val_loss: 0.1177 - val_mae: 0.3351\n",
      "Epoch 21/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0850 - mae: 0.2406 - val_loss: 0.0933 - val_mae: 0.2965\n",
      "Epoch 22/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0778 - mae: 0.2365 - val_loss: 0.1017 - val_mae: 0.3103\n",
      "Epoch 23/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0827 - mae: 0.2412 - val_loss: 0.1461 - val_mae: 0.3750\n",
      "Epoch 24/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0741 - mae: 0.2189 - val_loss: 0.1455 - val_mae: 0.3742\n",
      "Epoch 25/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.1191 - mae: 0.2717 - val_loss: 0.1047 - val_mae: 0.3151\n",
      "Epoch 26/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0761 - mae: 0.2246 - val_loss: 0.0331 - val_mae: 0.1665\n",
      "Epoch 27/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0811 - mae: 0.2334 - val_loss: 0.0577 - val_mae: 0.2287\n",
      "Epoch 28/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0935 - mae: 0.2554 - val_loss: 0.1420 - val_mae: 0.3696\n",
      "Epoch 29/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0855 - mae: 0.2389 - val_loss: 0.1628 - val_mae: 0.3967\n",
      "Epoch 30/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0644 - mae: 0.2068 - val_loss: 0.1367 - val_mae: 0.3623\n",
      "Epoch 31/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0747 - mae: 0.2189 - val_loss: 0.1335 - val_mae: 0.3579\n",
      "Epoch 32/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0764 - mae: 0.2297 - val_loss: 0.0490 - val_mae: 0.2088\n",
      "Epoch 33/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0761 - mae: 0.2298 - val_loss: 0.0958 - val_mae: 0.3005\n",
      "Epoch 34/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0795 - mae: 0.2395 - val_loss: 0.0268 - val_mae: 0.1462\n",
      "Epoch 35/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0963 - mae: 0.2533 - val_loss: 0.0564 - val_mae: 0.2257\n",
      "Epoch 36/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0813 - mae: 0.2374 - val_loss: 0.0357 - val_mae: 0.1739\n",
      "Epoch 37/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0873 - mae: 0.2458 - val_loss: 0.1429 - val_mae: 0.3706\n",
      "Epoch 38/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0843 - mae: 0.2373 - val_loss: 0.0921 - val_mae: 0.2943\n",
      "Epoch 39/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0796 - mae: 0.2361 - val_loss: 0.1198 - val_mae: 0.3380\n",
      "Epoch 40/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0795 - mae: 0.2347 - val_loss: 0.1606 - val_mae: 0.3938\n",
      "Epoch 41/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0976 - mae: 0.2537 - val_loss: 0.1987 - val_mae: 0.4395\n",
      "Epoch 42/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0934 - mae: 0.2486 - val_loss: 0.1412 - val_mae: 0.3685\n",
      "Epoch 43/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1072 - mae: 0.2671 - val_loss: 0.0889 - val_mae: 0.2889\n",
      "Epoch 44/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0745 - mae: 0.2215 - val_loss: 0.0082 - val_mae: 0.0737\n",
      "Epoch 45/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1048 - mae: 0.2706 - val_loss: 0.2377 - val_mae: 0.4820\n",
      "Epoch 46/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0913 - mae: 0.2323 - val_loss: 0.0966 - val_mae: 0.3022\n",
      "Epoch 47/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0768 - mae: 0.2266 - val_loss: 0.1131 - val_mae: 0.3283\n",
      "Epoch 48/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0782 - mae: 0.2357 - val_loss: 0.1124 - val_mae: 0.3272\n",
      "Epoch 49/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0721 - mae: 0.2289 - val_loss: 0.1036 - val_mae: 0.3135\n",
      "Epoch 50/50\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0844 - mae: 0.2438 - val_loss: 0.0886 - val_mae: 0.2886\n",
      "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step\n",
      "âœ… Done with capsicum_Dharwad_daily.csv | MAE=898.24, RMSE=1032.53, R2=0.34, MAPE=56.36%, Accuracy=43.64%\n",
      "\n",
      "ğŸš€ Processing: capsicum_Hassan_daily.csv\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\2714984772.py:101: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Average Price'].fillna(df['Average Price'].mean(), inplace=True)\n",
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\2714984772.py:106: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['MA_7'].fillna(df['MA_7'].mean(), inplace=True)\n",
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\2714984772.py:107: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['MA_30'].fillna(df['MA_30'].mean(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 17ms/step - loss: 0.1182 - mae: 0.2083 - val_loss: 0.0437 - val_mae: 0.1445\n",
      "Epoch 2/50\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - loss: 0.0077 - mae: 0.0678 - val_loss: 0.0246 - val_mae: 0.1305\n",
      "Epoch 3/50\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - loss: 0.0035 - mae: 0.0454 - val_loss: 0.0265 - val_mae: 0.1482\n",
      "Epoch 4/50\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - loss: 0.0023 - mae: 0.0362 - val_loss: 0.0266 - val_mae: 0.1473\n",
      "Epoch 5/50\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - loss: 0.0017 - mae: 0.0316 - val_loss: 0.0256 - val_mae: 0.1437\n",
      "Epoch 6/50\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 0.0018 - mae: 0.0318 - val_loss: 0.0254 - val_mae: 0.1421\n",
      "Epoch 7/50\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 0.0015 - mae: 0.0283 - val_loss: 0.0257 - val_mae: 0.1424\n",
      "Epoch 8/50\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - loss: 0.0013 - mae: 0.0258 - val_loss: 0.0296 - val_mae: 0.1486\n",
      "Epoch 9/50\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - loss: 0.0016 - mae: 0.0301 - val_loss: 0.0306 - val_mae: 0.1471\n",
      "Epoch 10/50\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 0.0014 - mae: 0.0275 - val_loss: 0.0317 - val_mae: 0.1469\n",
      "Epoch 11/50\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - loss: 0.0012 - mae: 0.0246 - val_loss: 0.0279 - val_mae: 0.1385\n",
      "Epoch 12/50\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - loss: 0.0012 - mae: 0.0254 - val_loss: 0.0361 - val_mae: 0.1516\n",
      "Epoch 13/50\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - loss: 0.0012 - mae: 0.0248 - val_loss: 0.0302 - val_mae: 0.1410\n",
      "Epoch 14/50\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - loss: 0.0013 - mae: 0.0255 - val_loss: 0.0349 - val_mae: 0.1475\n",
      "Epoch 15/50\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - loss: 0.0013 - mae: 0.0263 - val_loss: 0.0324 - val_mae: 0.1429\n",
      "Epoch 16/50\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - loss: 0.0012 - mae: 0.0243 - val_loss: 0.0372 - val_mae: 0.1476\n",
      "Epoch 17/50\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - loss: 0.0014 - mae: 0.0266 - val_loss: 0.0360 - val_mae: 0.1471\n",
      "Epoch 18/50\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - loss: 0.0012 - mae: 0.0246 - val_loss: 0.0363 - val_mae: 0.1474\n",
      "Epoch 19/50\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - loss: 0.0011 - mae: 0.0237 - val_loss: 0.0333 - val_mae: 0.1434\n",
      "Epoch 20/50\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - loss: 0.0010 - mae: 0.0226 - val_loss: 0.0341 - val_mae: 0.1421\n",
      "Epoch 21/50\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - loss: 0.0012 - mae: 0.0242 - val_loss: 0.0443 - val_mae: 0.1553\n",
      "Epoch 22/50\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - loss: 0.0012 - mae: 0.0239 - val_loss: 0.0482 - val_mae: 0.1580\n",
      "Epoch 23/50\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - loss: 0.0011 - mae: 0.0229 - val_loss: 0.0435 - val_mae: 0.1547\n",
      "Epoch 24/50\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - loss: 0.0012 - mae: 0.0239 - val_loss: 0.0446 - val_mae: 0.1544\n",
      "Epoch 25/50\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - loss: 0.0011 - mae: 0.0234 - val_loss: 0.0440 - val_mae: 0.1517\n",
      "Epoch 26/50\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 0.0011 - mae: 0.0231 - val_loss: 0.0432 - val_mae: 0.1483\n",
      "Epoch 27/50\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - loss: 0.0012 - mae: 0.0248 - val_loss: 0.0537 - val_mae: 0.1670\n",
      "Epoch 28/50\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - loss: 0.0012 - mae: 0.0248 - val_loss: 0.0489 - val_mae: 0.1557\n",
      "Epoch 29/50\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - loss: 0.0011 - mae: 0.0225 - val_loss: 0.0465 - val_mae: 0.1542\n",
      "Epoch 30/50\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - loss: 0.0010 - mae: 0.0223 - val_loss: 0.0518 - val_mae: 0.1617\n",
      "Epoch 31/50\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - loss: 0.0011 - mae: 0.0228 - val_loss: 0.0522 - val_mae: 0.1602\n",
      "Epoch 32/50\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - loss: 0.0010 - mae: 0.0225 - val_loss: 0.0573 - val_mae: 0.1732\n",
      "Epoch 33/50\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - loss: 0.0011 - mae: 0.0243 - val_loss: 0.0514 - val_mae: 0.1575\n",
      "Epoch 34/50\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - loss: 9.8988e-04 - mae: 0.0214 - val_loss: 0.0462 - val_mae: 0.1540\n",
      "Epoch 35/50\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - loss: 9.7397e-04 - mae: 0.0214 - val_loss: 0.0535 - val_mae: 0.1626\n",
      "Epoch 36/50\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - loss: 9.5720e-04 - mae: 0.0212 - val_loss: 0.0336 - val_mae: 0.1398\n",
      "Epoch 37/50\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - loss: 0.0010 - mae: 0.0229 - val_loss: 0.0556 - val_mae: 0.1622\n",
      "Epoch 38/50\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - loss: 0.0011 - mae: 0.0233 - val_loss: 0.0578 - val_mae: 0.1714\n",
      "Epoch 39/50\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - loss: 0.0010 - mae: 0.0220 - val_loss: 0.0428 - val_mae: 0.1462\n",
      "Epoch 40/50\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - loss: 0.0011 - mae: 0.0232 - val_loss: 0.0585 - val_mae: 0.1732\n",
      "Epoch 41/50\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - loss: 9.8400e-04 - mae: 0.0216 - val_loss: 0.0522 - val_mae: 0.1563\n",
      "Epoch 42/50\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - loss: 9.7783e-04 - mae: 0.0210 - val_loss: 0.0613 - val_mae: 0.1781\n",
      "Epoch 43/50\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - loss: 0.0010 - mae: 0.0217 - val_loss: 0.0593 - val_mae: 0.1684\n",
      "Epoch 44/50\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - loss: 0.0011 - mae: 0.0232 - val_loss: 0.0491 - val_mae: 0.1547\n",
      "Epoch 45/50\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - loss: 9.7030e-04 - mae: 0.0209 - val_loss: 0.0585 - val_mae: 0.1708\n",
      "Epoch 46/50\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - loss: 8.9476e-04 - mae: 0.0206 - val_loss: 0.0366 - val_mae: 0.1414\n",
      "Epoch 47/50\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - loss: 9.9049e-04 - mae: 0.0217 - val_loss: 0.0591 - val_mae: 0.1745\n",
      "Epoch 48/50\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - loss: 9.3836e-04 - mae: 0.0209 - val_loss: 0.0443 - val_mae: 0.1504\n",
      "Epoch 49/50\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - loss: 9.6608e-04 - mae: 0.0213 - val_loss: 0.0479 - val_mae: 0.1579\n",
      "Epoch 50/50\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - loss: 9.9552e-04 - mae: 0.0219 - val_loss: 0.0429 - val_mae: 0.1507\n",
      "WARNING:tensorflow:5 out of the last 148 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000020A7BBA1BC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m217/217\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step\n",
      "âœ… Done with capsicum_Hassan_daily.csv | MAE=370.86, RMSE=628.12, R2=0.61, MAPE=18.24%, Accuracy=81.76%\n",
      "\n",
      "ğŸš€ Processing: capsicum_Haveri_daily.csv\n",
      "âŒ Skipped capsicum_Haveri_daily.csv: Not enough rows to form a sequence (need > 30)\n",
      "ğŸš€ Processing: capsicum_Kalburgi_daily.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\2714984772.py:101: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Average Price'].fillna(df['Average Price'].mean(), inplace=True)\n",
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\2714984772.py:106: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['MA_7'].fillna(df['MA_7'].mean(), inplace=True)\n",
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\2714984772.py:107: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['MA_30'].fillna(df['MA_30'].mean(), inplace=True)\n",
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\2714984772.py:101: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Average Price'].fillna(df['Average Price'].mean(), inplace=True)\n",
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\2714984772.py:106: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['MA_7'].fillna(df['MA_7'].mean(), inplace=True)\n",
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\2714984772.py:107: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['MA_30'].fillna(df['MA_30'].mean(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 23ms/step - loss: 0.2650 - mae: 0.3626 - val_loss: 0.0207 - val_mae: 0.0926\n",
      "Epoch 2/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0271 - mae: 0.1281 - val_loss: 0.0201 - val_mae: 0.0905\n",
      "Epoch 3/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0258 - mae: 0.1241 - val_loss: 0.0160 - val_mae: 0.0759\n",
      "Epoch 4/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0279 - mae: 0.1336 - val_loss: 0.0228 - val_mae: 0.1023\n",
      "Epoch 5/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0246 - mae: 0.1227 - val_loss: 0.0199 - val_mae: 0.0904\n",
      "Epoch 6/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0238 - mae: 0.1188 - val_loss: 0.0299 - val_mae: 0.1326\n",
      "Epoch 7/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0194 - mae: 0.1089 - val_loss: 0.0191 - val_mae: 0.0877\n",
      "Epoch 8/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0163 - mae: 0.0985 - val_loss: 0.0193 - val_mae: 0.0887\n",
      "Epoch 9/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0199 - mae: 0.1110 - val_loss: 0.0149 - val_mae: 0.1088\n",
      "Epoch 10/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0205 - mae: 0.1107 - val_loss: 0.0120 - val_mae: 0.0764\n",
      "Epoch 11/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0136 - mae: 0.0899 - val_loss: 0.0116 - val_mae: 0.0812\n",
      "Epoch 12/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0134 - mae: 0.0888 - val_loss: 0.0114 - val_mae: 0.0739\n",
      "Epoch 13/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0132 - mae: 0.0830 - val_loss: 0.0128 - val_mae: 0.0655\n",
      "Epoch 14/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0114 - mae: 0.0775 - val_loss: 0.0136 - val_mae: 0.0687\n",
      "Epoch 15/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0127 - mae: 0.0862 - val_loss: 0.0112 - val_mae: 0.0797\n",
      "Epoch 16/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0132 - mae: 0.0832 - val_loss: 0.0117 - val_mae: 0.0891\n",
      "Epoch 17/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0110 - mae: 0.0786 - val_loss: 0.0159 - val_mae: 0.1154\n",
      "Epoch 18/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0132 - mae: 0.0857 - val_loss: 0.0158 - val_mae: 0.0785\n",
      "Epoch 19/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0114 - mae: 0.0773 - val_loss: 0.0135 - val_mae: 0.1027\n",
      "Epoch 20/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0114 - mae: 0.0792 - val_loss: 0.0112 - val_mae: 0.0832\n",
      "Epoch 21/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0107 - mae: 0.0749 - val_loss: 0.0138 - val_mae: 0.0707\n",
      "Epoch 22/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0141 - mae: 0.0881 - val_loss: 0.0123 - val_mae: 0.0652\n",
      "Epoch 23/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0102 - mae: 0.0742 - val_loss: 0.0119 - val_mae: 0.0913\n",
      "Epoch 24/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0115 - mae: 0.0814 - val_loss: 0.0126 - val_mae: 0.0663\n",
      "Epoch 25/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0124 - mae: 0.0807 - val_loss: 0.0112 - val_mae: 0.0826\n",
      "Epoch 26/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0115 - mae: 0.0784 - val_loss: 0.0111 - val_mae: 0.0784\n",
      "Epoch 27/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0103 - mae: 0.0751 - val_loss: 0.0110 - val_mae: 0.0783\n",
      "Epoch 28/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0105 - mae: 0.0750 - val_loss: 0.0114 - val_mae: 0.0864\n",
      "Epoch 29/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0097 - mae: 0.0713 - val_loss: 0.0113 - val_mae: 0.0846\n",
      "Epoch 30/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0105 - mae: 0.0727 - val_loss: 0.0111 - val_mae: 0.0813\n",
      "Epoch 31/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0103 - mae: 0.0732 - val_loss: 0.0116 - val_mae: 0.0879\n",
      "Epoch 32/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0107 - mae: 0.0734 - val_loss: 0.0130 - val_mae: 0.0993\n",
      "Epoch 33/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0097 - mae: 0.0718 - val_loss: 0.0119 - val_mae: 0.0662\n",
      "Epoch 34/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0109 - mae: 0.0779 - val_loss: 0.0129 - val_mae: 0.0986\n",
      "Epoch 35/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0107 - mae: 0.0753 - val_loss: 0.0127 - val_mae: 0.0675\n",
      "Epoch 36/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0108 - mae: 0.0740 - val_loss: 0.0123 - val_mae: 0.0947\n",
      "Epoch 37/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0111 - mae: 0.0769 - val_loss: 0.0114 - val_mae: 0.0861\n",
      "Epoch 38/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0113 - mae: 0.0773 - val_loss: 0.0113 - val_mae: 0.0844\n",
      "Epoch 39/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0108 - mae: 0.0763 - val_loss: 0.0111 - val_mae: 0.0736\n",
      "Epoch 40/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0096 - mae: 0.0706 - val_loss: 0.0217 - val_mae: 0.1371\n",
      "Epoch 41/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0119 - mae: 0.0801 - val_loss: 0.0114 - val_mae: 0.0682\n",
      "Epoch 42/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0104 - mae: 0.0746 - val_loss: 0.0129 - val_mae: 0.0984\n",
      "Epoch 43/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0109 - mae: 0.0734 - val_loss: 0.0131 - val_mae: 0.0996\n",
      "Epoch 44/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0106 - mae: 0.0751 - val_loss: 0.0120 - val_mae: 0.0916\n",
      "Epoch 45/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0100 - mae: 0.0726 - val_loss: 0.0130 - val_mae: 0.0992\n",
      "Epoch 46/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0087 - mae: 0.0673 - val_loss: 0.0171 - val_mae: 0.1193\n",
      "Epoch 47/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0109 - mae: 0.0735 - val_loss: 0.0132 - val_mae: 0.1003\n",
      "Epoch 48/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0115 - mae: 0.0792 - val_loss: 0.0139 - val_mae: 0.1042\n",
      "Epoch 49/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0116 - mae: 0.0804 - val_loss: 0.0111 - val_mae: 0.0763\n",
      "Epoch 50/50\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0096 - mae: 0.0720 - val_loss: 0.0119 - val_mae: 0.0900\n",
      "\u001b[1m36/36\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "âœ… Done with capsicum_Kalburgi_daily.csv | MAE=609.56, RMSE=777.5, R2=0.59, MAPE=49.4%, Accuracy=50.6%\n",
      "\n",
      "ğŸš€ Processing: capsicum_Kolar_daily.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\2714984772.py:101: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Average Price'].fillna(df['Average Price'].mean(), inplace=True)\n",
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\2714984772.py:106: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['MA_7'].fillna(df['MA_7'].mean(), inplace=True)\n",
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\2714984772.py:107: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['MA_30'].fillna(df['MA_30'].mean(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 17ms/step - loss: 0.1497 - mae: 0.1803 - val_loss: 0.0018 - val_mae: 0.0319\n",
      "Epoch 2/50\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 17ms/step - loss: 0.0090 - mae: 0.0698 - val_loss: 0.0074 - val_mae: 0.0773\n",
      "Epoch 3/50\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 16ms/step - loss: 0.0085 - mae: 0.0655 - val_loss: 0.0024 - val_mae: 0.0395\n",
      "Epoch 4/50\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 14ms/step - loss: 0.0077 - mae: 0.0631 - val_loss: 0.0018 - val_mae: 0.0324\n",
      "Epoch 5/50\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 13ms/step - loss: 0.0077 - mae: 0.0631 - val_loss: 0.0019 - val_mae: 0.0334\n",
      "Epoch 6/50\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 16ms/step - loss: 0.0077 - mae: 0.0628 - val_loss: 0.0024 - val_mae: 0.0390\n",
      "Epoch 7/50\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 17ms/step - loss: 0.0078 - mae: 0.0631 - val_loss: 0.0055 - val_mae: 0.0648\n",
      "Epoch 8/50\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 17ms/step - loss: 0.0080 - mae: 0.0639 - val_loss: 0.0066 - val_mae: 0.0711\n",
      "Epoch 9/50\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 16ms/step - loss: 0.0074 - mae: 0.0614 - val_loss: 0.0019 - val_mae: 0.0319\n",
      "Epoch 10/50\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 15ms/step - loss: 0.0074 - mae: 0.0606 - val_loss: 0.0052 - val_mae: 0.0629\n",
      "Epoch 11/50\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 15ms/step - loss: 0.0075 - mae: 0.0618 - val_loss: 0.0039 - val_mae: 0.0520\n",
      "Epoch 12/50\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - loss: 0.0073 - mae: 0.0603 - val_loss: 0.0028 - val_mae: 0.0433\n",
      "Epoch 13/50\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 15ms/step - loss: 0.0074 - mae: 0.0609 - val_loss: 0.0024 - val_mae: 0.0393\n",
      "Epoch 14/50\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 17ms/step - loss: 0.0074 - mae: 0.0606 - val_loss: 0.0027 - val_mae: 0.0417\n",
      "Epoch 15/50\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 17ms/step - loss: 0.0075 - mae: 0.0607 - val_loss: 0.0018 - val_mae: 0.0322\n",
      "Epoch 16/50\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - loss: 0.0073 - mae: 0.0603 - val_loss: 0.0022 - val_mae: 0.0369\n",
      "Epoch 17/50\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 17ms/step - loss: 0.0073 - mae: 0.0600 - val_loss: 0.0017 - val_mae: 0.0312\n",
      "Epoch 18/50\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - loss: 0.0072 - mae: 0.0599 - val_loss: 0.0018 - val_mae: 0.0321\n",
      "Epoch 19/50\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 14ms/step - loss: 0.0073 - mae: 0.0601 - val_loss: 0.0017 - val_mae: 0.0309\n",
      "Epoch 20/50\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 14ms/step - loss: 0.0072 - mae: 0.0594 - val_loss: 0.0020 - val_mae: 0.0354\n",
      "Epoch 21/50\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 14ms/step - loss: 0.0073 - mae: 0.0600 - val_loss: 0.0022 - val_mae: 0.0367\n",
      "Epoch 22/50\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 16ms/step - loss: 0.0074 - mae: 0.0607 - val_loss: 0.0018 - val_mae: 0.0323\n",
      "Epoch 23/50\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 17ms/step - loss: 0.0073 - mae: 0.0607 - val_loss: 0.0018 - val_mae: 0.0326\n",
      "Epoch 24/50\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 16ms/step - loss: 0.0073 - mae: 0.0598 - val_loss: 0.0017 - val_mae: 0.0315\n",
      "Epoch 25/50\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 15ms/step - loss: 0.0071 - mae: 0.0588 - val_loss: 0.0018 - val_mae: 0.0319\n",
      "Epoch 26/50\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 14ms/step - loss: 0.0072 - mae: 0.0590 - val_loss: 0.0017 - val_mae: 0.0314\n",
      "Epoch 27/50\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 15ms/step - loss: 0.0071 - mae: 0.0590 - val_loss: 0.0027 - val_mae: 0.0410\n",
      "Epoch 28/50\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - loss: 0.0074 - mae: 0.0606 - val_loss: 0.0018 - val_mae: 0.0327\n",
      "Epoch 29/50\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 16ms/step - loss: 0.0073 - mae: 0.0599 - val_loss: 0.0018 - val_mae: 0.0320\n",
      "Epoch 30/50\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 17ms/step - loss: 0.0072 - mae: 0.0594 - val_loss: 0.0023 - val_mae: 0.0371\n",
      "Epoch 31/50\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 16ms/step - loss: 0.0073 - mae: 0.0597 - val_loss: 0.0021 - val_mae: 0.0352\n",
      "Epoch 32/50\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 16ms/step - loss: 0.0074 - mae: 0.0606 - val_loss: 0.0019 - val_mae: 0.0339\n",
      "Epoch 33/50\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 16ms/step - loss: 0.0073 - mae: 0.0600 - val_loss: 0.0017 - val_mae: 0.0314\n",
      "Epoch 34/50\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 17ms/step - loss: 0.0073 - mae: 0.0598 - val_loss: 0.0017 - val_mae: 0.0308\n",
      "Epoch 35/50\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 17ms/step - loss: 0.0073 - mae: 0.0596 - val_loss: 0.0017 - val_mae: 0.0313\n",
      "Epoch 36/50\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - loss: 0.0074 - mae: 0.0599 - val_loss: 0.0018 - val_mae: 0.0321\n",
      "Epoch 37/50\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 16ms/step - loss: 0.0073 - mae: 0.0599 - val_loss: 0.0018 - val_mae: 0.0317\n",
      "Epoch 38/50\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 17ms/step - loss: 0.0072 - mae: 0.0596 - val_loss: 0.0017 - val_mae: 0.0309\n",
      "Epoch 39/50\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 15ms/step - loss: 0.0072 - mae: 0.0594 - val_loss: 0.0017 - val_mae: 0.0311\n",
      "Epoch 40/50\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 16ms/step - loss: 0.0072 - mae: 0.0595 - val_loss: 0.0018 - val_mae: 0.0324\n",
      "Epoch 41/50\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 15ms/step - loss: 0.0071 - mae: 0.0587 - val_loss: 0.0019 - val_mae: 0.0330\n",
      "Epoch 42/50\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 14ms/step - loss: 0.0075 - mae: 0.0607 - val_loss: 0.0017 - val_mae: 0.0308\n",
      "Epoch 43/50\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 16ms/step - loss: 0.0073 - mae: 0.0593 - val_loss: 0.0019 - val_mae: 0.0330\n",
      "Epoch 44/50\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 16ms/step - loss: 0.0074 - mae: 0.0601 - val_loss: 0.0018 - val_mae: 0.0316\n",
      "Epoch 45/50\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 14ms/step - loss: 0.0073 - mae: 0.0597 - val_loss: 0.0017 - val_mae: 0.0309\n",
      "Epoch 46/50\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 16ms/step - loss: 0.0070 - mae: 0.0583 - val_loss: 0.0018 - val_mae: 0.0323\n",
      "Epoch 47/50\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 14ms/step - loss: 0.0071 - mae: 0.0588 - val_loss: 0.0018 - val_mae: 0.0320\n",
      "Epoch 48/50\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 14ms/step - loss: 0.0071 - mae: 0.0589 - val_loss: 0.0017 - val_mae: 0.0311\n",
      "Epoch 49/50\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 16ms/step - loss: 0.0073 - mae: 0.0602 - val_loss: 0.0017 - val_mae: 0.0315\n",
      "Epoch 50/50\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 15ms/step - loss: 0.0072 - mae: 0.0599 - val_loss: 0.0019 - val_mae: 0.0335\n",
      "\u001b[1m672/672\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step\n",
      "âœ… Done with capsicum_Kolar_daily.csv | MAE=1012.45, RMSE=1424.7, R2=0.23, MAPE=51.69%, Accuracy=48.31%\n",
      "\n",
      "ğŸš€ Processing: capsicum_Mandya_daily.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\2714984772.py:101: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Average Price'].fillna(df['Average Price'].mean(), inplace=True)\n",
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\2714984772.py:106: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['MA_7'].fillna(df['MA_7'].mean(), inplace=True)\n",
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\2714984772.py:107: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['MA_30'].fillna(df['MA_30'].mean(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 19ms/step - loss: 0.2353 - mae: 0.2787 - val_loss: 0.0250 - val_mae: 0.1362\n",
      "Epoch 2/50\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - loss: 0.0158 - mae: 0.0927 - val_loss: 0.0433 - val_mae: 0.1874\n",
      "Epoch 3/50\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - loss: 0.0127 - mae: 0.0813 - val_loss: 0.0125 - val_mae: 0.0924\n",
      "Epoch 4/50\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - loss: 0.0072 - mae: 0.0582 - val_loss: 0.0167 - val_mae: 0.1110\n",
      "Epoch 5/50\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - loss: 0.0058 - mae: 0.0499 - val_loss: 0.0115 - val_mae: 0.0884\n",
      "Epoch 6/50\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - loss: 0.0061 - mae: 0.0501 - val_loss: 0.0095 - val_mae: 0.0751\n",
      "Epoch 7/50\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - loss: 0.0061 - mae: 0.0501 - val_loss: 0.0095 - val_mae: 0.0787\n",
      "Epoch 8/50\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - loss: 0.0058 - mae: 0.0498 - val_loss: 0.0125 - val_mae: 0.0942\n",
      "Epoch 9/50\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - loss: 0.0050 - mae: 0.0448 - val_loss: 0.0146 - val_mae: 0.1035\n",
      "Epoch 10/50\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - loss: 0.0052 - mae: 0.0445 - val_loss: 0.0101 - val_mae: 0.0825\n",
      "Epoch 11/50\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - loss: 0.0057 - mae: 0.0489 - val_loss: 0.0136 - val_mae: 0.0993\n",
      "Epoch 12/50\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.0049 - mae: 0.0419 - val_loss: 0.0097 - val_mae: 0.0805\n",
      "Epoch 13/50\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - loss: 0.0054 - mae: 0.0448 - val_loss: 0.0090 - val_mae: 0.0743\n",
      "Epoch 14/50\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 0.0048 - mae: 0.0420 - val_loss: 0.0088 - val_mae: 0.0742\n",
      "Epoch 15/50\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - loss: 0.0051 - mae: 0.0426 - val_loss: 0.0092 - val_mae: 0.0731\n",
      "Epoch 16/50\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - loss: 0.0053 - mae: 0.0438 - val_loss: 0.0124 - val_mae: 0.0934\n",
      "Epoch 17/50\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - loss: 0.0047 - mae: 0.0410 - val_loss: 0.0112 - val_mae: 0.0742\n",
      "Epoch 18/50\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - loss: 0.0050 - mae: 0.0418 - val_loss: 0.0094 - val_mae: 0.0783\n",
      "Epoch 19/50\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.0052 - mae: 0.0430 - val_loss: 0.0092 - val_mae: 0.0781\n",
      "Epoch 20/50\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.0052 - mae: 0.0434 - val_loss: 0.0088 - val_mae: 0.0747\n",
      "Epoch 21/50\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - loss: 0.0050 - mae: 0.0452 - val_loss: 0.0120 - val_mae: 0.0922\n",
      "Epoch 22/50\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - loss: 0.0048 - mae: 0.0421 - val_loss: 0.0101 - val_mae: 0.0834\n",
      "Epoch 23/50\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - loss: 0.0052 - mae: 0.0442 - val_loss: 0.0100 - val_mae: 0.0820\n",
      "Epoch 24/50\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - loss: 0.0042 - mae: 0.0368 - val_loss: 0.0117 - val_mae: 0.0765\n",
      "Epoch 25/50\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - loss: 0.0054 - mae: 0.0483 - val_loss: 0.0091 - val_mae: 0.0764\n",
      "Epoch 26/50\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - loss: 0.0053 - mae: 0.0438 - val_loss: 0.0108 - val_mae: 0.0856\n",
      "Epoch 27/50\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - loss: 0.0047 - mae: 0.0404 - val_loss: 0.0090 - val_mae: 0.0760\n",
      "Epoch 28/50\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - loss: 0.0047 - mae: 0.0394 - val_loss: 0.0167 - val_mae: 0.1122\n",
      "Epoch 29/50\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - loss: 0.0056 - mae: 0.0467 - val_loss: 0.0087 - val_mae: 0.0739\n",
      "Epoch 30/50\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - loss: 0.0047 - mae: 0.0379 - val_loss: 0.0086 - val_mae: 0.0736\n",
      "Epoch 31/50\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - loss: 0.0048 - mae: 0.0403 - val_loss: 0.0095 - val_mae: 0.0794\n",
      "Epoch 32/50\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - loss: 0.0052 - mae: 0.0435 - val_loss: 0.0089 - val_mae: 0.0734\n",
      "Epoch 33/50\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - loss: 0.0046 - mae: 0.0403 - val_loss: 0.0085 - val_mae: 0.0722\n",
      "Epoch 34/50\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - loss: 0.0045 - mae: 0.0392 - val_loss: 0.0090 - val_mae: 0.0768\n",
      "Epoch 35/50\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - loss: 0.0048 - mae: 0.0394 - val_loss: 0.0124 - val_mae: 0.0943\n",
      "Epoch 36/50\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - loss: 0.0051 - mae: 0.0408 - val_loss: 0.0171 - val_mae: 0.1001\n",
      "Epoch 37/50\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - loss: 0.0048 - mae: 0.0394 - val_loss: 0.0089 - val_mae: 0.0718\n",
      "Epoch 38/50\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - loss: 0.0043 - mae: 0.0364 - val_loss: 0.0089 - val_mae: 0.0717\n",
      "Epoch 39/50\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - loss: 0.0047 - mae: 0.0385 - val_loss: 0.0087 - val_mae: 0.0702\n",
      "Epoch 40/50\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - loss: 0.0049 - mae: 0.0397 - val_loss: 0.0101 - val_mae: 0.0719\n",
      "Epoch 41/50\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - loss: 0.0046 - mae: 0.0393 - val_loss: 0.0087 - val_mae: 0.0746\n",
      "Epoch 42/50\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - loss: 0.0050 - mae: 0.0396 - val_loss: 0.0087 - val_mae: 0.0732\n",
      "Epoch 43/50\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - loss: 0.0053 - mae: 0.0415 - val_loss: 0.0090 - val_mae: 0.0729\n",
      "Epoch 44/50\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - loss: 0.0047 - mae: 0.0383 - val_loss: 0.0093 - val_mae: 0.0790\n",
      "Epoch 45/50\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - loss: 0.0043 - mae: 0.0374 - val_loss: 0.0086 - val_mae: 0.0722\n",
      "Epoch 46/50\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - loss: 0.0047 - mae: 0.0374 - val_loss: 0.0089 - val_mae: 0.0706\n",
      "Epoch 47/50\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - loss: 0.0049 - mae: 0.0400 - val_loss: 0.0092 - val_mae: 0.0783\n",
      "Epoch 48/50\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - loss: 0.0052 - mae: 0.0418 - val_loss: 0.0088 - val_mae: 0.0724\n",
      "Epoch 49/50\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - loss: 0.0049 - mae: 0.0382 - val_loss: 0.0090 - val_mae: 0.0710\n",
      "Epoch 50/50\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - loss: 0.0049 - mae: 0.0377 - val_loss: 0.0089 - val_mae: 0.0724\n",
      "\u001b[1m178/178\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step\n",
      "âœ… Done with capsicum_Mandya_daily.csv | MAE=319.69, RMSE=469.11, R2=0.83, MAPE=18.19%, Accuracy=81.81%\n",
      "\n",
      "ğŸš€ Processing: capsicum_Mysore_daily.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\2714984772.py:101: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Average Price'].fillna(df['Average Price'].mean(), inplace=True)\n",
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\2714984772.py:106: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['MA_7'].fillna(df['MA_7'].mean(), inplace=True)\n",
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\2714984772.py:107: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['MA_30'].fillna(df['MA_30'].mean(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 17ms/step - loss: 0.1369 - mae: 0.1710 - val_loss: 0.0024 - val_mae: 0.0372\n",
      "Epoch 2/50\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 17ms/step - loss: 0.0025 - mae: 0.0392 - val_loss: 0.0021 - val_mae: 0.0324\n",
      "Epoch 3/50\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 18ms/step - loss: 0.0020 - mae: 0.0341 - val_loss: 0.0022 - val_mae: 0.0324\n",
      "Epoch 4/50\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 18ms/step - loss: 0.0020 - mae: 0.0337 - val_loss: 0.0021 - val_mae: 0.0324\n",
      "Epoch 5/50\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 17ms/step - loss: 0.0016 - mae: 0.0292 - val_loss: 0.0024 - val_mae: 0.0336\n",
      "Epoch 6/50\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 17ms/step - loss: 0.0016 - mae: 0.0295 - val_loss: 0.0067 - val_mae: 0.0734\n",
      "Epoch 7/50\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 18ms/step - loss: 0.0015 - mae: 0.0294 - val_loss: 0.0058 - val_mae: 0.0675\n",
      "Epoch 8/50\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - loss: 0.0017 - mae: 0.0305 - val_loss: 0.0023 - val_mae: 0.0360\n",
      "Epoch 9/50\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 17ms/step - loss: 0.0014 - mae: 0.0274 - val_loss: 0.0020 - val_mae: 0.0326\n",
      "Epoch 10/50\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - loss: 0.0013 - mae: 0.0260 - val_loss: 0.0029 - val_mae: 0.0424\n",
      "Epoch 11/50\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 15ms/step - loss: 0.0013 - mae: 0.0258 - val_loss: 0.0023 - val_mae: 0.0330\n",
      "Epoch 12/50\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - loss: 0.0013 - mae: 0.0260 - val_loss: 0.0020 - val_mae: 0.0319\n",
      "Epoch 13/50\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - loss: 0.0012 - mae: 0.0250 - val_loss: 0.0021 - val_mae: 0.0336\n",
      "Epoch 14/50\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 15ms/step - loss: 0.0012 - mae: 0.0246 - val_loss: 0.0024 - val_mae: 0.0338\n",
      "Epoch 15/50\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 17ms/step - loss: 0.0012 - mae: 0.0257 - val_loss: 0.0020 - val_mae: 0.0313\n",
      "Epoch 16/50\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 18ms/step - loss: 0.0012 - mae: 0.0251 - val_loss: 0.0026 - val_mae: 0.0355\n",
      "Epoch 17/50\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 17ms/step - loss: 0.0014 - mae: 0.0279 - val_loss: 0.0020 - val_mae: 0.0314\n",
      "Epoch 18/50\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 18ms/step - loss: 0.0011 - mae: 0.0239 - val_loss: 0.0025 - val_mae: 0.0378\n",
      "Epoch 19/50\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 17ms/step - loss: 0.0012 - mae: 0.0252 - val_loss: 0.0020 - val_mae: 0.0322\n",
      "Epoch 20/50\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 18ms/step - loss: 0.0012 - mae: 0.0257 - val_loss: 0.0020 - val_mae: 0.0313\n",
      "Epoch 21/50\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 18ms/step - loss: 0.0012 - mae: 0.0251 - val_loss: 0.0021 - val_mae: 0.0337\n",
      "Epoch 22/50\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 17ms/step - loss: 0.0012 - mae: 0.0247 - val_loss: 0.0020 - val_mae: 0.0324\n",
      "Epoch 23/50\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - loss: 0.0011 - mae: 0.0242 - val_loss: 0.0020 - val_mae: 0.0313\n",
      "Epoch 24/50\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - loss: 0.0013 - mae: 0.0258 - val_loss: 0.0020 - val_mae: 0.0325\n",
      "Epoch 25/50\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - loss: 0.0011 - mae: 0.0241 - val_loss: 0.0020 - val_mae: 0.0313\n",
      "Epoch 26/50\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 18ms/step - loss: 0.0013 - mae: 0.0253 - val_loss: 0.0021 - val_mae: 0.0318\n",
      "Epoch 27/50\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 18ms/step - loss: 0.0012 - mae: 0.0249 - val_loss: 0.0020 - val_mae: 0.0320\n",
      "Epoch 28/50\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 17ms/step - loss: 0.0011 - mae: 0.0240 - val_loss: 0.0022 - val_mae: 0.0341\n",
      "Epoch 29/50\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 18ms/step - loss: 0.0011 - mae: 0.0242 - val_loss: 0.0024 - val_mae: 0.0337\n",
      "Epoch 30/50\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 15ms/step - loss: 0.0011 - mae: 0.0239 - val_loss: 0.0021 - val_mae: 0.0333\n",
      "Epoch 31/50\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - loss: 0.0011 - mae: 0.0238 - val_loss: 0.0020 - val_mae: 0.0316\n",
      "Epoch 32/50\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - loss: 0.0012 - mae: 0.0246 - val_loss: 0.0020 - val_mae: 0.0323\n",
      "Epoch 33/50\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - loss: 0.0012 - mae: 0.0243 - val_loss: 0.0020 - val_mae: 0.0314\n",
      "Epoch 34/50\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 15ms/step - loss: 0.0012 - mae: 0.0244 - val_loss: 0.0020 - val_mae: 0.0318\n",
      "Epoch 35/50\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 12ms/step - loss: 0.0011 - mae: 0.0240 - val_loss: 0.0021 - val_mae: 0.0332\n",
      "Epoch 36/50\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 13ms/step - loss: 0.0011 - mae: 0.0236 - val_loss: 0.0021 - val_mae: 0.0334\n",
      "Epoch 37/50\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - loss: 0.0012 - mae: 0.0244 - val_loss: 0.0020 - val_mae: 0.0314\n",
      "Epoch 38/50\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 17ms/step - loss: 0.0011 - mae: 0.0243 - val_loss: 0.0020 - val_mae: 0.0323\n",
      "Epoch 39/50\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - loss: 0.0011 - mae: 0.0236 - val_loss: 0.0020 - val_mae: 0.0319\n",
      "Epoch 40/50\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 17ms/step - loss: 0.0011 - mae: 0.0239 - val_loss: 0.0020 - val_mae: 0.0319\n",
      "Epoch 41/50\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 17ms/step - loss: 0.0011 - mae: 0.0236 - val_loss: 0.0025 - val_mae: 0.0375\n",
      "Epoch 42/50\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 17ms/step - loss: 0.0011 - mae: 0.0235 - val_loss: 0.0021 - val_mae: 0.0326\n",
      "Epoch 43/50\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 17ms/step - loss: 0.0011 - mae: 0.0238 - val_loss: 0.0024 - val_mae: 0.0358\n",
      "Epoch 44/50\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 17ms/step - loss: 0.0011 - mae: 0.0242 - val_loss: 0.0020 - val_mae: 0.0317\n",
      "Epoch 45/50\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - loss: 0.0011 - mae: 0.0235 - val_loss: 0.0023 - val_mae: 0.0357\n",
      "Epoch 46/50\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - loss: 0.0012 - mae: 0.0242 - val_loss: 0.0027 - val_mae: 0.0366\n",
      "Epoch 47/50\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 17ms/step - loss: 0.0011 - mae: 0.0241 - val_loss: 0.0020 - val_mae: 0.0322\n",
      "Epoch 48/50\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - loss: 0.0011 - mae: 0.0235 - val_loss: 0.0021 - val_mae: 0.0330\n",
      "Epoch 49/50\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - loss: 0.0011 - mae: 0.0240 - val_loss: 0.0023 - val_mae: 0.0351\n",
      "Epoch 50/50\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 17ms/step - loss: 0.0011 - mae: 0.0236 - val_loss: 0.0020 - val_mae: 0.0316\n",
      "\u001b[1m319/319\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step\n",
      "âœ… Done with capsicum_Mysore_daily.csv | MAE=582.75, RMSE=833.66, R2=0.56, MAPE=22.91%, Accuracy=77.09%\n",
      "\n",
      "ğŸš€ Processing: capsicum_Shimoga_daily.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\2714984772.py:101: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Average Price'].fillna(df['Average Price'].mean(), inplace=True)\n",
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\2714984772.py:106: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['MA_7'].fillna(df['MA_7'].mean(), inplace=True)\n",
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\2714984772.py:107: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['MA_30'].fillna(df['MA_30'].mean(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 183ms/step - loss: 0.3414 - mae: 0.5094 - val_loss: 0.6273 - val_mae: 0.7909\n",
      "Epoch 2/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.2234 - mae: 0.4327 - val_loss: 0.0076 - val_mae: 0.0757\n",
      "Epoch 3/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.1024 - mae: 0.2693 - val_loss: 0.0334 - val_mae: 0.1780\n",
      "Epoch 4/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.0226 - mae: 0.1162 - val_loss: 0.2174 - val_mae: 0.4646\n",
      "Epoch 5/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.0595 - mae: 0.2050 - val_loss: 0.0063 - val_mae: 0.0689\n",
      "Epoch 6/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0319 - mae: 0.1546 - val_loss: 0.0149 - val_mae: 0.1160\n",
      "Epoch 7/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.0208 - mae: 0.1188 - val_loss: 0.1122 - val_mae: 0.3329\n",
      "Epoch 8/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.0333 - mae: 0.1538 - val_loss: 0.0079 - val_mae: 0.0804\n",
      "Epoch 9/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.0234 - mae: 0.1276 - val_loss: 0.0040 - val_mae: 0.0523\n",
      "Epoch 10/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0172 - mae: 0.1103 - val_loss: 0.0461 - val_mae: 0.2116\n",
      "Epoch 11/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.0163 - mae: 0.1020 - val_loss: 0.0128 - val_mae: 0.1069\n",
      "Epoch 12/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.0109 - mae: 0.0809 - val_loss: 0.0156 - val_mae: 0.1195\n",
      "Epoch 13/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.0115 - mae: 0.0872 - val_loss: 0.0237 - val_mae: 0.1497\n",
      "Epoch 14/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0123 - mae: 0.0911 - val_loss: 0.0138 - val_mae: 0.1118\n",
      "Epoch 15/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0108 - mae: 0.0842 - val_loss: 0.0204 - val_mae: 0.1380\n",
      "Epoch 16/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0102 - mae: 0.0849 - val_loss: 0.0205 - val_mae: 0.1383\n",
      "Epoch 17/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.0118 - mae: 0.0940 - val_loss: 0.0231 - val_mae: 0.1477\n",
      "Epoch 18/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.0083 - mae: 0.0745 - val_loss: 0.0318 - val_mae: 0.1745\n",
      "Epoch 19/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0123 - mae: 0.0905 - val_loss: 0.0136 - val_mae: 0.1105\n",
      "Epoch 20/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.0117 - mae: 0.0867 - val_loss: 0.0215 - val_mae: 0.1419\n",
      "Epoch 21/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0091 - mae: 0.0793 - val_loss: 0.0302 - val_mae: 0.1698\n",
      "Epoch 22/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0065 - mae: 0.0649 - val_loss: 0.0242 - val_mae: 0.1514\n",
      "Epoch 23/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.0093 - mae: 0.0760 - val_loss: 0.0219 - val_mae: 0.1435\n",
      "Epoch 24/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.0082 - mae: 0.0739 - val_loss: 0.0107 - val_mae: 0.0969\n",
      "Epoch 25/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0125 - mae: 0.0887 - val_loss: 0.0446 - val_mae: 0.2082\n",
      "Epoch 26/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.0107 - mae: 0.0860 - val_loss: 0.0137 - val_mae: 0.1114\n",
      "Epoch 27/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.0103 - mae: 0.0819 - val_loss: 0.0221 - val_mae: 0.1441\n",
      "Epoch 28/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0081 - mae: 0.0713 - val_loss: 0.0237 - val_mae: 0.1497\n",
      "Epoch 29/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.0113 - mae: 0.0851 - val_loss: 0.0131 - val_mae: 0.1085\n",
      "Epoch 30/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0111 - mae: 0.0856 - val_loss: 0.0465 - val_mae: 0.2125\n",
      "Epoch 31/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0108 - mae: 0.0810 - val_loss: 0.0148 - val_mae: 0.1159\n",
      "Epoch 32/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.0098 - mae: 0.0840 - val_loss: 0.0217 - val_mae: 0.1426\n",
      "Epoch 33/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.0077 - mae: 0.0679 - val_loss: 0.0276 - val_mae: 0.1622\n",
      "Epoch 34/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0083 - mae: 0.0730 - val_loss: 0.0178 - val_mae: 0.1283\n",
      "Epoch 35/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.0100 - mae: 0.0813 - val_loss: 0.0349 - val_mae: 0.1831\n",
      "Epoch 36/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0090 - mae: 0.0763 - val_loss: 0.0173 - val_mae: 0.1266\n",
      "Epoch 37/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0074 - mae: 0.0758 - val_loss: 0.0275 - val_mae: 0.1619\n",
      "Epoch 38/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0083 - mae: 0.0721 - val_loss: 0.0217 - val_mae: 0.1429\n",
      "Epoch 39/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0073 - mae: 0.0726 - val_loss: 0.0237 - val_mae: 0.1497\n",
      "Epoch 40/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.0072 - mae: 0.0722 - val_loss: 0.0218 - val_mae: 0.1432\n",
      "Epoch 41/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0079 - mae: 0.0730 - val_loss: 0.0201 - val_mae: 0.1371\n",
      "Epoch 42/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 0.0051 - mae: 0.0569 - val_loss: 0.0416 - val_mae: 0.2008\n",
      "Epoch 43/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.0082 - mae: 0.0729 - val_loss: 0.0157 - val_mae: 0.1200\n",
      "Epoch 44/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.0086 - mae: 0.0780 - val_loss: 0.0318 - val_mae: 0.1744\n",
      "Epoch 45/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0076 - mae: 0.0727 - val_loss: 0.0192 - val_mae: 0.1338\n",
      "Epoch 46/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0100 - mae: 0.0832 - val_loss: 0.0290 - val_mae: 0.1664\n",
      "Epoch 47/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0071 - mae: 0.0692 - val_loss: 0.0172 - val_mae: 0.1262\n",
      "Epoch 48/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0059 - mae: 0.0657 - val_loss: 0.0314 - val_mae: 0.1735\n",
      "Epoch 49/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0079 - mae: 0.0720 - val_loss: 0.0242 - val_mae: 0.1515\n",
      "Epoch 50/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.0062 - mae: 0.0663 - val_loss: 0.0198 - val_mae: 0.1359\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 146ms/step\n",
      "âœ… Done with capsicum_Shimoga_daily.csv | MAE=54.96, RMSE=62.64, R2=0.79, MAPE=3.29%, Accuracy=96.71%\n",
      "\n",
      "ğŸš€ Processing: capsicum_Udupi_daily.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\2714984772.py:101: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Average Price'].fillna(df['Average Price'].mean(), inplace=True)\n",
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\2714984772.py:106: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['MA_7'].fillna(df['MA_7'].mean(), inplace=True)\n",
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\2714984772.py:107: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['MA_30'].fillna(df['MA_30'].mean(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 15ms/step - loss: 0.1110 - mae: 0.2106 - val_loss: 0.0226 - val_mae: 0.1099\n",
      "Epoch 2/50\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - loss: 0.0090 - mae: 0.0739 - val_loss: 0.0152 - val_mae: 0.0934\n",
      "Epoch 3/50\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - loss: 0.0043 - mae: 0.0474 - val_loss: 0.0137 - val_mae: 0.0851\n",
      "Epoch 4/50\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 0.0036 - mae: 0.0436 - val_loss: 0.0175 - val_mae: 0.1042\n",
      "Epoch 5/50\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - loss: 0.0034 - mae: 0.0404 - val_loss: 0.0196 - val_mae: 0.1121\n",
      "Epoch 6/50\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - loss: 0.0025 - mae: 0.0354 - val_loss: 0.0190 - val_mae: 0.1094\n",
      "Epoch 7/50\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - loss: 0.0027 - mae: 0.0366 - val_loss: 0.0151 - val_mae: 0.0903\n",
      "Epoch 8/50\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - loss: 0.0023 - mae: 0.0312 - val_loss: 0.0171 - val_mae: 0.0992\n",
      "Epoch 9/50\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 0.0023 - mae: 0.0313 - val_loss: 0.0248 - val_mae: 0.1294\n",
      "Epoch 10/50\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - loss: 0.0023 - mae: 0.0324 - val_loss: 0.0187 - val_mae: 0.1051\n",
      "Epoch 11/50\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - loss: 0.0019 - mae: 0.0296 - val_loss: 0.0159 - val_mae: 0.0924\n",
      "Epoch 12/50\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - loss: 0.0021 - mae: 0.0313 - val_loss: 0.0232 - val_mae: 0.1210\n",
      "Epoch 13/50\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - loss: 0.0021 - mae: 0.0322 - val_loss: 0.0197 - val_mae: 0.1080\n",
      "Epoch 14/50\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 0.0019 - mae: 0.0288 - val_loss: 0.0155 - val_mae: 0.0887\n",
      "Epoch 15/50\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - loss: 0.0022 - mae: 0.0318 - val_loss: 0.0167 - val_mae: 0.0936\n",
      "Epoch 16/50\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0019 - mae: 0.0292 - val_loss: 0.0168 - val_mae: 0.0938\n",
      "Epoch 17/50\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - loss: 0.0020 - mae: 0.0289 - val_loss: 0.0237 - val_mae: 0.1171\n",
      "Epoch 18/50\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - loss: 0.0024 - mae: 0.0332 - val_loss: 0.0193 - val_mae: 0.1005\n",
      "Epoch 19/50\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.0021 - mae: 0.0306 - val_loss: 0.0152 - val_mae: 0.0881\n",
      "Epoch 20/50\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - loss: 0.0021 - mae: 0.0312 - val_loss: 0.0175 - val_mae: 0.0947\n",
      "Epoch 21/50\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - loss: 0.0018 - mae: 0.0281 - val_loss: 0.0178 - val_mae: 0.0956\n",
      "Epoch 22/50\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - loss: 0.0025 - mae: 0.0323 - val_loss: 0.0242 - val_mae: 0.1164\n",
      "Epoch 23/50\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - loss: 0.0019 - mae: 0.0300 - val_loss: 0.0145 - val_mae: 0.0861\n",
      "Epoch 24/50\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 0.0022 - mae: 0.0312 - val_loss: 0.0199 - val_mae: 0.1015\n",
      "Epoch 25/50\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 0.0020 - mae: 0.0292 - val_loss: 0.0208 - val_mae: 0.1063\n",
      "Epoch 26/50\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - loss: 0.0018 - mae: 0.0277 - val_loss: 0.0244 - val_mae: 0.1149\n",
      "Epoch 27/50\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - loss: 0.0019 - mae: 0.0277 - val_loss: 0.0243 - val_mae: 0.1167\n",
      "Epoch 28/50\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - loss: 0.0019 - mae: 0.0278 - val_loss: 0.0298 - val_mae: 0.1302\n",
      "Epoch 29/50\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - loss: 0.0021 - mae: 0.0301 - val_loss: 0.0152 - val_mae: 0.0878\n",
      "Epoch 30/50\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 0.0020 - mae: 0.0285 - val_loss: 0.0238 - val_mae: 0.1122\n",
      "Epoch 31/50\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - loss: 0.0019 - mae: 0.0281 - val_loss: 0.0162 - val_mae: 0.0901\n",
      "Epoch 32/50\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - loss: 0.0022 - mae: 0.0301 - val_loss: 0.0212 - val_mae: 0.1035\n",
      "Epoch 33/50\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - loss: 0.0021 - mae: 0.0298 - val_loss: 0.0228 - val_mae: 0.1117\n",
      "Epoch 34/50\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - loss: 0.0020 - mae: 0.0287 - val_loss: 0.0286 - val_mae: 0.1252\n",
      "Epoch 35/50\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - loss: 0.0017 - mae: 0.0279 - val_loss: 0.0218 - val_mae: 0.1057\n",
      "Epoch 36/50\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - loss: 0.0017 - mae: 0.0259 - val_loss: 0.0228 - val_mae: 0.1079\n",
      "Epoch 37/50\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - loss: 0.0018 - mae: 0.0278 - val_loss: 0.0232 - val_mae: 0.1083\n",
      "Epoch 38/50\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - loss: 0.0015 - mae: 0.0252 - val_loss: 0.0287 - val_mae: 0.1284\n",
      "Epoch 39/50\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - loss: 0.0019 - mae: 0.0274 - val_loss: 0.0214 - val_mae: 0.1058\n",
      "Epoch 40/50\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - loss: 0.0020 - mae: 0.0290 - val_loss: 0.0203 - val_mae: 0.1034\n",
      "Epoch 41/50\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - loss: 0.0019 - mae: 0.0267 - val_loss: 0.0230 - val_mae: 0.1096\n",
      "Epoch 42/50\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - loss: 0.0018 - mae: 0.0272 - val_loss: 0.0208 - val_mae: 0.1021\n",
      "Epoch 43/50\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - loss: 0.0018 - mae: 0.0267 - val_loss: 0.0287 - val_mae: 0.1283\n",
      "Epoch 44/50\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - loss: 0.0018 - mae: 0.0268 - val_loss: 0.0186 - val_mae: 0.0968\n",
      "Epoch 45/50\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - loss: 0.0019 - mae: 0.0281 - val_loss: 0.0181 - val_mae: 0.0943\n",
      "Epoch 46/50\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - loss: 0.0019 - mae: 0.0274 - val_loss: 0.0208 - val_mae: 0.1009\n",
      "Epoch 47/50\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - loss: 0.0017 - mae: 0.0259 - val_loss: 0.0172 - val_mae: 0.0931\n",
      "Epoch 48/50\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - loss: 0.0017 - mae: 0.0265 - val_loss: 0.0244 - val_mae: 0.1161\n",
      "Epoch 49/50\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - loss: 0.0018 - mae: 0.0264 - val_loss: 0.0228 - val_mae: 0.1069\n",
      "Epoch 50/50\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - loss: 0.0019 - mae: 0.0269 - val_loss: 0.0157 - val_mae: 0.0887\n",
      "\u001b[1m155/155\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n",
      "âœ… Done with capsicum_Udupi_daily.csv | MAE=345.72, RMSE=602.99, R2=0.84, MAPE=9.64%, Accuracy=90.36%\n",
      "\n",
      "ğŸ“Š Metrics saved to tat_ha_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "\n",
    "# -----------------------------\n",
    "# Output directories\n",
    "# -----------------------------\n",
    "input_folder = \"dataset\"\n",
    "output_models = \"tat_ha_output_models\"\n",
    "output_csv = \"tat_ha_output_csv\"\n",
    "output_graphs = \"tat_ha_output_graphs\"\n",
    "output_logs = \"tat_ha_output_logs\"\n",
    "metrics_file = \"tat_ha_metrics.csv\"\n",
    "\n",
    "os.makedirs(output_models, exist_ok=True)\n",
    "os.makedirs(output_csv, exist_ok=True)\n",
    "os.makedirs(output_graphs, exist_ok=True)\n",
    "os.makedirs(output_logs, exist_ok=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Function to create dataset\n",
    "# -----------------------------\n",
    "def create_dataset(data, look_back=30):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - look_back):\n",
    "        X.append(data[i:i+look_back, 0])\n",
    "        y.append(data[i+look_back, 0])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# -----------------------------\n",
    "# Local Attention Layer\n",
    "# -----------------------------\n",
    "class LocalAttention(layers.Layer):\n",
    "    def __init__(self, key_dim=64, dropout_rate=0.1, **kwargs):\n",
    "        super(LocalAttention, self).__init__(**kwargs)\n",
    "        self.key_dim = key_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.q_dense = layers.Dense(key_dim)\n",
    "        self.k_dense = layers.Dense(key_dim)\n",
    "        self.v_dense = layers.Dense(key_dim)\n",
    "        self.dropout = layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x):\n",
    "        Q = self.q_dense(x)\n",
    "        K = self.k_dense(x)\n",
    "        V = self.v_dense(x)\n",
    "        scores = tf.matmul(Q, K, transpose_b=True) / tf.math.sqrt(tf.cast(tf.shape(K)[-1], tf.float32))\n",
    "        weights = tf.nn.softmax(scores, axis=-1)\n",
    "        output = tf.matmul(weights, V)\n",
    "        output = self.dropout(output)\n",
    "        return output\n",
    "\n",
    "# -----------------------------\n",
    "# Hierarchical Attention Model\n",
    "# -----------------------------\n",
    "def build_tat_ha_model(input_shape, d_model=64, ff_dim=128, dropout_rate=0.2):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    local_attn = LocalAttention(key_dim=d_model, dropout_rate=dropout_rate)(inputs)\n",
    "    local_attn = layers.LayerNormalization(epsilon=1e-6)(local_attn + inputs)\n",
    "    global_attn = layers.MultiHeadAttention(num_heads=4, key_dim=d_model)(local_attn, local_attn)\n",
    "    global_attn = layers.Dropout(dropout_rate)(global_attn)\n",
    "    global_attn = layers.LayerNormalization(epsilon=1e-6)(global_attn + local_attn)\n",
    "    ff = layers.Dense(ff_dim, activation='relu')(global_attn)\n",
    "    ff = layers.Dense(input_shape[1])(ff)\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(ff + global_attn)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    outputs = layers.Dense(1)(x)\n",
    "    model = models.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "                  loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# -----------------------------\n",
    "# Metrics storage\n",
    "# -----------------------------\n",
    "metrics_list = []\n",
    "\n",
    "# -----------------------------\n",
    "# Process each CSV file\n",
    "# -----------------------------\n",
    "look_back = 30\n",
    "for file in os.listdir(input_folder):\n",
    "    if file.endswith(\".csv\"):\n",
    "        file_path = os.path.join(input_folder, file)\n",
    "        print(f\"ğŸš€ Processing: {file}\")\n",
    "\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Date parsing\n",
    "        df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "        df = df.dropna(subset=['Date'])\n",
    "        df = df.sort_values('Date')\n",
    "\n",
    "        # Handle missing Average Price\n",
    "        df['Average Price'].fillna(df['Average Price'].mean(), inplace=True)\n",
    "\n",
    "        # Moving averages\n",
    "        df['MA_7'] = df['Average Price'].rolling(window=7).mean()\n",
    "        df['MA_30'] = df['Average Price'].rolling(window=30).mean()\n",
    "        df['MA_7'].fillna(df['MA_7'].mean(), inplace=True)\n",
    "        df['MA_30'].fillna(df['MA_30'].mean(), inplace=True)\n",
    "\n",
    "        # Prepare data\n",
    "        values = df[['Average Price']].values.astype('float32')\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        scaled_values = scaler.fit_transform(values)\n",
    "        X, y = create_dataset(scaled_values, look_back)\n",
    "\n",
    "        # -------------------------------------\n",
    "        # â— FIX 1 â€” Prevent IndexError\n",
    "        # -------------------------------------\n",
    "        if X.size == 0:\n",
    "            print(f\"âŒ Skipped {file}: Not enough rows to form a sequence (need > {look_back})\")\n",
    "            continue\n",
    "\n",
    "        X = np.reshape(X, (X.shape[0], X.shape[1], 1))\n",
    "\n",
    "        # Build model\n",
    "        model = build_tat_ha_model(input_shape=(look_back,1))\n",
    "\n",
    "        # Train\n",
    "        history = model.fit(X, y, epochs=50, batch_size=16, validation_split=0.2, verbose=1)\n",
    "\n",
    "        # Save logs\n",
    "        log_file = os.path.join(output_logs, file.replace(\".csv\", \"_tat_ha_training.txt\"))\n",
    "        with open(log_file, \"w\") as f:\n",
    "            f.write(\"Training Loss per Epoch:\\n\")\n",
    "            for i, loss in enumerate(history.history['loss']):\n",
    "                f.write(f\"Epoch {i+1}: Loss={loss}, Val_Loss={history.history['val_loss'][i]}\\n\")\n",
    "\n",
    "        # Predictions\n",
    "        predictions = model.predict(X)\n",
    "        predictions_rescaled = scaler.inverse_transform(predictions)\n",
    "        df['Predicted'] = [np.nan]*look_back + list(predictions_rescaled.flatten())\n",
    "\n",
    "        # Round values\n",
    "        df['Average Price'] = df['Average Price'].round(2)\n",
    "        df['Predicted'] = df['Predicted'].round(2)\n",
    "\n",
    "        # Metrics\n",
    "        y_true = df['Average Price'].values[look_back:]\n",
    "        y_pred = predictions_rescaled.flatten()\n",
    "        mae = round(mean_absolute_error(y_true, y_pred), 2)\n",
    "        rmse = round(np.sqrt(mean_squared_error(y_true, y_pred)), 2)\n",
    "        r2 = round(r2_score(y_true, y_pred), 2)\n",
    "        mape = round(np.mean(np.abs((y_true - y_pred)/y_true))*100, 2)\n",
    "        accuracy = round(100 - mape, 2)\n",
    "        metrics_list.append([file.replace(\".csv\",\"\"), mae, rmse, r2, mape, accuracy])\n",
    "\n",
    "        # Save model\n",
    "        model_file = os.path.join(output_models, file.replace(\".csv\", \"_tat_ha_model.pkl\"))\n",
    "        joblib.dump(model, model_file)\n",
    "\n",
    "        # Save updated CSV\n",
    "        save_df = df[['Date', 'Average Price', 'Predicted']].rename(columns={'Average Price': 'Actual'})\n",
    "        updated_csv_path = os.path.join(output_csv, file.replace(\".csv\", \"_tat_ha_updated.csv\"))\n",
    "        save_df.to_csv(updated_csv_path, index=False)\n",
    "\n",
    "        # -------------------------------------\n",
    "        # â— FIX 2 â€” Prevent KeyError\n",
    "        # -------------------------------------\n",
    "        plt.figure(figsize=(12,6))\n",
    "        plt.plot(df['Date'], df['Average Price'], label='Actual', color='blue')\n",
    "        plt.plot(df['Date'], df['MA_7'], label='MA 7', color='orange')\n",
    "        plt.plot(df['Date'], df['MA_30'], label='MA 30', color='green')\n",
    "        plt.plot(df['Date'], df['Predicted'], label='Predicted (TAT-HA)', color='red', linestyle='dashed')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Price')\n",
    "        plt.title(f'Price Prediction (TAT-HA) - {file}')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        graph_file = os.path.join(output_graphs, file.replace(\".csv\", \"_tat_ha_graph.png\"))\n",
    "        plt.savefig(graph_file)\n",
    "        plt.close()\n",
    "\n",
    "        print(f\"âœ… Done with {file} | MAE={mae}, RMSE={rmse}, R2={r2}, MAPE={mape}%, Accuracy={accuracy}%\\n\")\n",
    "\n",
    "# Save metrics\n",
    "metrics_df = pd.DataFrame(metrics_list, columns=['District','MAE','RMSE','R2','MAPE(%)','Accuracy(%)'])\n",
    "metrics_df.to_csv(metrics_file, index=False)\n",
    "print(f\"ğŸ“Š Metrics saved to {metrics_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32199c7f-1ed8-4ea9-a327-2d7168dd616a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67c47c20-3406-4de0-9f51-e12c60352731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Processing: capsicum_Bangalore_daily.csv ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\3711824180.py:52: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df['Date'] = pd.to_datetime(df['Date'], infer_datetime_format=True, errors='coerce')\n",
      "C:\\Users\\ravik\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m233/233\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 0.0283 - mae: 0.1187 - val_loss: 0.0118 - val_mae: 0.0820\n",
      "Epoch 2/20\n",
      "\u001b[1m233/233\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0054 - mae: 0.0464 - val_loss: 0.0117 - val_mae: 0.0831\n",
      "Epoch 3/20\n",
      "\u001b[1m233/233\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0054 - mae: 0.0453 - val_loss: 0.0122 - val_mae: 0.0840\n",
      "Epoch 4/20\n",
      "\u001b[1m233/233\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0059 - mae: 0.0474 - val_loss: 0.0117 - val_mae: 0.0818\n",
      "Epoch 5/20\n",
      "\u001b[1m233/233\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0055 - mae: 0.0467 - val_loss: 0.0118 - val_mae: 0.0813\n",
      "Epoch 6/20\n",
      "\u001b[1m233/233\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0050 - mae: 0.0443 - val_loss: 0.0116 - val_mae: 0.0829\n",
      "Epoch 7/20\n",
      "\u001b[1m233/233\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0053 - mae: 0.0455 - val_loss: 0.0115 - val_mae: 0.0825\n",
      "Epoch 8/20\n",
      "\u001b[1m233/233\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0051 - mae: 0.0448 - val_loss: 0.0116 - val_mae: 0.0831\n",
      "Epoch 9/20\n",
      "\u001b[1m233/233\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0050 - mae: 0.0443 - val_loss: 0.0123 - val_mae: 0.0880\n",
      "Epoch 10/20\n",
      "\u001b[1m233/233\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0053 - mae: 0.0462 - val_loss: 0.0112 - val_mae: 0.0801\n",
      "Epoch 11/20\n",
      "\u001b[1m233/233\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0051 - mae: 0.0448 - val_loss: 0.0112 - val_mae: 0.0797\n",
      "Epoch 12/20\n",
      "\u001b[1m233/233\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0050 - mae: 0.0442 - val_loss: 0.0111 - val_mae: 0.0795\n",
      "Epoch 13/20\n",
      "\u001b[1m233/233\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0050 - mae: 0.0440 - val_loss: 0.0109 - val_mae: 0.0804\n",
      "Epoch 14/20\n",
      "\u001b[1m233/233\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0050 - mae: 0.0444 - val_loss: 0.0108 - val_mae: 0.0790\n",
      "Epoch 15/20\n",
      "\u001b[1m233/233\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0050 - mae: 0.0436 - val_loss: 0.0109 - val_mae: 0.0783\n",
      "Epoch 16/20\n",
      "\u001b[1m233/233\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0051 - mae: 0.0456 - val_loss: 0.0109 - val_mae: 0.0786\n",
      "Epoch 17/20\n",
      "\u001b[1m233/233\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0050 - mae: 0.0444 - val_loss: 0.0113 - val_mae: 0.0787\n",
      "Epoch 18/20\n",
      "\u001b[1m233/233\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0049 - mae: 0.0440 - val_loss: 0.0109 - val_mae: 0.0774\n",
      "Epoch 19/20\n",
      "\u001b[1m233/233\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0050 - mae: 0.0445 - val_loss: 0.0106 - val_mae: 0.0775\n",
      "Epoch 20/20\n",
      "\u001b[1m233/233\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0045 - mae: 0.0429 - val_loss: 0.0106 - val_mae: 0.0776\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
      "âœ… Done with capsicum_Bangalore_daily.csv | MAE=749.21, RMSE=991.47, R2=0.31, MAPE=20.24%, Accuracy=79.76%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Processing: capsicum_Belgaum_daily.csv ==========\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\3711824180.py:52: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df['Date'] = pd.to_datetime(df['Date'], infer_datetime_format=True, errors='coerce')\n",
      "C:\\Users\\ravik\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m31/31\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - loss: 0.1183 - mae: 0.3177 - val_loss: 0.0523 - val_mae: 0.2230\n",
      "Epoch 2/20\n",
      "\u001b[1m31/31\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0199 - mae: 0.1110 - val_loss: 0.0011 - val_mae: 0.0252\n",
      "Epoch 3/20\n",
      "\u001b[1m31/31\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0061 - mae: 0.0538 - val_loss: 0.0016 - val_mae: 0.0276\n",
      "Epoch 4/20\n",
      "\u001b[1m31/31\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0043 - mae: 0.0480 - val_loss: 0.0013 - val_mae: 0.0261\n",
      "Epoch 5/20\n",
      "\u001b[1m31/31\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0034 - mae: 0.0413 - val_loss: 8.2127e-04 - val_mae: 0.0190\n",
      "Epoch 6/20\n",
      "\u001b[1m31/31\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0028 - mae: 0.0376 - val_loss: 5.4809e-04 - val_mae: 0.0149\n",
      "Epoch 7/20\n",
      "\u001b[1m31/31\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0037 - mae: 0.0384 - val_loss: 5.1170e-04 - val_mae: 0.0155\n",
      "Epoch 8/20\n",
      "\u001b[1m31/31\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0040 - mae: 0.0366 - val_loss: 6.5592e-04 - val_mae: 0.0167\n",
      "Epoch 9/20\n",
      "\u001b[1m31/31\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0033 - mae: 0.0341 - val_loss: 8.5343e-04 - val_mae: 0.0219\n",
      "Epoch 10/20\n",
      "\u001b[1m31/31\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0036 - mae: 0.0353 - val_loss: 5.9311e-04 - val_mae: 0.0152\n",
      "Epoch 11/20\n",
      "\u001b[1m31/31\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0026 - mae: 0.0311 - val_loss: 4.8621e-04 - val_mae: 0.0159\n",
      "Epoch 12/20\n",
      "\u001b[1m31/31\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0031 - mae: 0.0314 - val_loss: 9.3198e-04 - val_mae: 0.0241\n",
      "Epoch 13/20\n",
      "\u001b[1m31/31\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0029 - mae: 0.0331 - val_loss: 0.0013 - val_mae: 0.0304\n",
      "Epoch 14/20\n",
      "\u001b[1m31/31\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0029 - mae: 0.0322 - val_loss: 6.1232e-04 - val_mae: 0.0159\n",
      "Epoch 15/20\n",
      "\u001b[1m31/31\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0029 - mae: 0.0290 - val_loss: 0.0011 - val_mae: 0.0282\n",
      "Epoch 16/20\n",
      "\u001b[1m31/31\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0035 - mae: 0.0324 - val_loss: 8.1368e-04 - val_mae: 0.0217\n",
      "Epoch 17/20\n",
      "\u001b[1m31/31\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0025 - mae: 0.0282 - val_loss: 4.5843e-04 - val_mae: 0.0131\n",
      "Epoch 18/20\n",
      "\u001b[1m31/31\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0027 - mae: 0.0287 - val_loss: 5.3165e-04 - val_mae: 0.0141\n",
      "Epoch 19/20\n",
      "\u001b[1m31/31\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0025 - mae: 0.0270 - val_loss: 4.2758e-04 - val_mae: 0.0149\n",
      "Epoch 20/20\n",
      "\u001b[1m31/31\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0026 - mae: 0.0274 - val_loss: 6.7970e-04 - val_mae: 0.0190\n",
      "\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "âœ… Done with capsicum_Belgaum_daily.csv | MAE=126.09, RMSE=173.37, R2=0.85, MAPE=3.47%, Accuracy=96.53%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Processing: capsicum_Bellary_daily.csv ==========\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\3711824180.py:52: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df['Date'] = pd.to_datetime(df['Date'], infer_datetime_format=True, errors='coerce')\n",
      "C:\\Users\\ravik\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 248ms/step - loss: 0.1206 - mae: 0.2875 - val_loss: 0.3273 - val_mae: 0.5123\n",
      "Epoch 2/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.1015 - mae: 0.2596 - val_loss: 0.3043 - val_mae: 0.4907\n",
      "Epoch 3/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.0860 - mae: 0.2380 - val_loss: 0.2814 - val_mae: 0.4680\n",
      "Epoch 4/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.0816 - mae: 0.2290 - val_loss: 0.2587 - val_mae: 0.4442\n",
      "Epoch 5/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.0667 - mae: 0.1993 - val_loss: 0.2360 - val_mae: 0.4210\n",
      "Epoch 6/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.0612 - mae: 0.1953 - val_loss: 0.2131 - val_mae: 0.3963\n",
      "Epoch 7/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.0543 - mae: 0.1758 - val_loss: 0.1899 - val_mae: 0.3721\n",
      "Epoch 8/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.0424 - mae: 0.1529 - val_loss: 0.1665 - val_mae: 0.3481\n",
      "Epoch 9/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.0411 - mae: 0.1443 - val_loss: 0.1438 - val_mae: 0.3253\n",
      "Epoch 10/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 0.0315 - mae: 0.1309 - val_loss: 0.1226 - val_mae: 0.3014\n",
      "Epoch 11/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.0291 - mae: 0.1163 - val_loss: 0.1039 - val_mae: 0.2769\n",
      "Epoch 12/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0201 - mae: 0.1052 - val_loss: 0.0893 - val_mae: 0.2564\n",
      "Epoch 13/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.0302 - mae: 0.1213 - val_loss: 0.0787 - val_mae: 0.2417\n",
      "Epoch 14/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0244 - mae: 0.1189 - val_loss: 0.0741 - val_mae: 0.2345\n",
      "Epoch 15/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.0305 - mae: 0.1280 - val_loss: 0.0727 - val_mae: 0.2322\n",
      "Epoch 16/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.0236 - mae: 0.1157 - val_loss: 0.0748 - val_mae: 0.2359\n",
      "Epoch 17/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.0239 - mae: 0.1143 - val_loss: 0.0777 - val_mae: 0.2407\n",
      "Epoch 18/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.0245 - mae: 0.1024 - val_loss: 0.0806 - val_mae: 0.2460\n",
      "Epoch 19/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.0220 - mae: 0.1078 - val_loss: 0.0834 - val_mae: 0.2507\n",
      "Epoch 20/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.0258 - mae: 0.1063 - val_loss: 0.0848 - val_mae: 0.2533\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 257ms/step\n",
      "âœ… Done with capsicum_Bellary_daily.csv | MAE=531.85, RMSE=611.61, R2=-0.19, MAPE=18.02%, Accuracy=81.98%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Processing: capsicum_Chikmagalur_daily.csv ==========\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\3711824180.py:52: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df['Date'] = pd.to_datetime(df['Date'], infer_datetime_format=True, errors='coerce')\n",
      "C:\\Users\\ravik\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m77/77\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.0947 - mae: 0.2676 - val_loss: 0.0237 - val_mae: 0.1359\n",
      "Epoch 2/20\n",
      "\u001b[1m77/77\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0065 - mae: 0.0645 - val_loss: 0.0126 - val_mae: 0.0800\n",
      "Epoch 3/20\n",
      "\u001b[1m77/77\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0043 - mae: 0.0478 - val_loss: 0.0121 - val_mae: 0.0713\n",
      "Epoch 4/20\n",
      "\u001b[1m77/77\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0041 - mae: 0.0453 - val_loss: 0.0121 - val_mae: 0.0735\n",
      "Epoch 5/20\n",
      "\u001b[1m77/77\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0042 - mae: 0.0470 - val_loss: 0.0123 - val_mae: 0.0763\n",
      "Epoch 6/20\n",
      "\u001b[1m77/77\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0042 - mae: 0.0469 - val_loss: 0.0120 - val_mae: 0.0704\n",
      "Epoch 7/20\n",
      "\u001b[1m77/77\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0043 - mae: 0.0459 - val_loss: 0.0121 - val_mae: 0.0734\n",
      "Epoch 8/20\n",
      "\u001b[1m77/77\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0043 - mae: 0.0477 - val_loss: 0.0120 - val_mae: 0.0707\n",
      "Epoch 9/20\n",
      "\u001b[1m77/77\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0042 - mae: 0.0471 - val_loss: 0.0120 - val_mae: 0.0703\n",
      "Epoch 10/20\n",
      "\u001b[1m77/77\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0041 - mae: 0.0465 - val_loss: 0.0120 - val_mae: 0.0709\n",
      "Epoch 11/20\n",
      "\u001b[1m77/77\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0041 - mae: 0.0468 - val_loss: 0.0120 - val_mae: 0.0705\n",
      "Epoch 12/20\n",
      "\u001b[1m77/77\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0043 - mae: 0.0470 - val_loss: 0.0120 - val_mae: 0.0714\n",
      "Epoch 13/20\n",
      "\u001b[1m77/77\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0042 - mae: 0.0477 - val_loss: 0.0123 - val_mae: 0.0757\n",
      "Epoch 14/20\n",
      "\u001b[1m77/77\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0043 - mae: 0.0463 - val_loss: 0.0123 - val_mae: 0.0769\n",
      "Epoch 15/20\n",
      "\u001b[1m77/77\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0043 - mae: 0.0480 - val_loss: 0.0121 - val_mae: 0.0732\n",
      "Epoch 16/20\n",
      "\u001b[1m77/77\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0039 - mae: 0.0444 - val_loss: 0.0120 - val_mae: 0.0720\n",
      "Epoch 17/20\n",
      "\u001b[1m77/77\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0042 - mae: 0.0467 - val_loss: 0.0125 - val_mae: 0.0779\n",
      "Epoch 18/20\n",
      "\u001b[1m77/77\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0043 - mae: 0.0472 - val_loss: 0.0120 - val_mae: 0.0716\n",
      "Epoch 19/20\n",
      "\u001b[1m77/77\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0039 - mae: 0.0448 - val_loss: 0.0119 - val_mae: 0.0708\n",
      "Epoch 20/20\n",
      "\u001b[1m77/77\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0041 - mae: 0.0467 - val_loss: 0.0121 - val_mae: 0.0727\n",
      "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "âœ… Done with capsicum_Chikmagalur_daily.csv | MAE=254.51, RMSE=384.63, R2=0.77, MAPE=13.56%, Accuracy=86.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Processing: capsicum_Davangere_daily.csv ==========\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\3711824180.py:52: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df['Date'] = pd.to_datetime(df['Date'], infer_datetime_format=True, errors='coerce')\n",
      "C:\\Users\\ravik\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m113/113\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0090 - mae: 0.0698 - val_loss: 0.0027 - val_mae: 0.0348\n",
      "Epoch 2/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 4.1921e-04 - mae: 0.0126 - val_loss: 0.0018 - val_mae: 0.0247\n",
      "Epoch 3/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 3.8883e-04 - mae: 0.0111 - val_loss: 0.0018 - val_mae: 0.0236\n",
      "Epoch 4/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 3.3515e-04 - mae: 0.0104 - val_loss: 0.0017 - val_mae: 0.0231\n",
      "Epoch 5/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 3.4037e-04 - mae: 0.0108 - val_loss: 0.0017 - val_mae: 0.0242\n",
      "Epoch 6/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 3.1318e-04 - mae: 0.0105 - val_loss: 0.0016 - val_mae: 0.0229\n",
      "Epoch 7/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 3.5118e-04 - mae: 0.0104 - val_loss: 0.0016 - val_mae: 0.0227\n",
      "Epoch 8/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 3.0014e-04 - mae: 0.0099 - val_loss: 0.0015 - val_mae: 0.0228\n",
      "Epoch 9/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2.7771e-04 - mae: 0.0099 - val_loss: 0.0014 - val_mae: 0.0231\n",
      "Epoch 10/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 2.3039e-04 - mae: 0.0089 - val_loss: 0.0013 - val_mae: 0.0204\n",
      "Epoch 11/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 2.7210e-04 - mae: 0.0097 - val_loss: 0.0013 - val_mae: 0.0214\n",
      "Epoch 12/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 2.0275e-04 - mae: 0.0083 - val_loss: 0.0012 - val_mae: 0.0189\n",
      "Epoch 13/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.9924e-04 - mae: 0.0082 - val_loss: 0.0012 - val_mae: 0.0213\n",
      "Epoch 14/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.9792e-04 - mae: 0.0077 - val_loss: 0.0011 - val_mae: 0.0181\n",
      "Epoch 15/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.9064e-04 - mae: 0.0076 - val_loss: 0.0011 - val_mae: 0.0183\n",
      "Epoch 16/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.8446e-04 - mae: 0.0072 - val_loss: 9.7425e-04 - val_mae: 0.0165\n",
      "Epoch 17/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.8385e-04 - mae: 0.0075 - val_loss: 0.0011 - val_mae: 0.0193\n",
      "Epoch 18/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.6505e-04 - mae: 0.0074 - val_loss: 0.0011 - val_mae: 0.0189\n",
      "Epoch 19/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.5672e-04 - mae: 0.0068 - val_loss: 9.0411e-04 - val_mae: 0.0164\n",
      "Epoch 20/20\n",
      "\u001b[1m113/113\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.4475e-04 - mae: 0.0069 - val_loss: 9.1788e-04 - val_mae: 0.0171\n",
      "\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "âœ… Done with capsicum_Davangere_daily.csv | MAE=270.82, RMSE=478.69, R2=0.92, MAPE=4.94%, Accuracy=95.06%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Processing: capsicum_Dharwad_daily.csv ==========\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\3711824180.py:52: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df['Date'] = pd.to_datetime(df['Date'], infer_datetime_format=True, errors='coerce')\n",
      "C:\\Users\\ravik\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 99ms/step - loss: 0.1429 - mae: 0.2952 - val_loss: 0.6926 - val_mae: 0.8223\n",
      "Epoch 2/20\n",
      "\u001b[1m6/6\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.1417 - mae: 0.2852 - val_loss: 0.6189 - val_mae: 0.7764\n",
      "Epoch 3/20\n",
      "\u001b[1m6/6\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0956 - mae: 0.2290 - val_loss: 0.5416 - val_mae: 0.7251\n",
      "Epoch 4/20\n",
      "\u001b[1m6/6\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.1145 - mae: 0.2530 - val_loss: 0.4554 - val_mae: 0.6633\n",
      "Epoch 5/20\n",
      "\u001b[1m6/6\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0902 - mae: 0.2281 - val_loss: 0.3620 - val_mae: 0.5888\n",
      "Epoch 6/20\n",
      "\u001b[1m6/6\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0734 - mae: 0.2076 - val_loss: 0.2633 - val_mae: 0.4981\n",
      "Epoch 7/20\n",
      "\u001b[1m6/6\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0692 - mae: 0.2080 - val_loss: 0.1708 - val_mae: 0.3988\n",
      "Epoch 8/20\n",
      "\u001b[1m6/6\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0622 - mae: 0.2039 - val_loss: 0.1210 - val_mae: 0.3345\n",
      "Epoch 9/20\n",
      "\u001b[1m6/6\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0606 - mae: 0.2048 - val_loss: 0.1164 - val_mae: 0.3279\n",
      "Epoch 10/20\n",
      "\u001b[1m6/6\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0655 - mae: 0.2100 - val_loss: 0.1368 - val_mae: 0.3563\n",
      "Epoch 11/20\n",
      "\u001b[1m6/6\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0714 - mae: 0.2223 - val_loss: 0.1544 - val_mae: 0.3791\n",
      "Epoch 12/20\n",
      "\u001b[1m6/6\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0696 - mae: 0.2139 - val_loss: 0.1666 - val_mae: 0.3939\n",
      "Epoch 13/20\n",
      "\u001b[1m6/6\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0678 - mae: 0.2094 - val_loss: 0.1700 - val_mae: 0.3979\n",
      "Epoch 14/20\n",
      "\u001b[1m6/6\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0629 - mae: 0.2027 - val_loss: 0.1661 - val_mae: 0.3932\n",
      "Epoch 15/20\n",
      "\u001b[1m6/6\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0616 - mae: 0.1983 - val_loss: 0.1588 - val_mae: 0.3844\n",
      "Epoch 16/20\n",
      "\u001b[1m6/6\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0636 - mae: 0.2057 - val_loss: 0.1586 - val_mae: 0.3842\n",
      "Epoch 17/20\n",
      "\u001b[1m6/6\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0656 - mae: 0.2127 - val_loss: 0.1538 - val_mae: 0.3783\n",
      "Epoch 18/20\n",
      "\u001b[1m6/6\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0615 - mae: 0.2048 - val_loss: 0.1545 - val_mae: 0.3791\n",
      "Epoch 19/20\n",
      "\u001b[1m6/6\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0612 - mae: 0.1992 - val_loss: 0.1558 - val_mae: 0.3807\n",
      "Epoch 20/20\n",
      "\u001b[1m6/6\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0597 - mae: 0.2007 - val_loss: 0.1590 - val_mae: 0.3846\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step\n",
      "âœ… Done with capsicum_Dharwad_daily.csv | MAE=1442.34, RMSE=1495.2, R2=-8.32, MAPE=34.99%, Accuracy=65.01%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Processing: capsicum_Hassan_daily.csv ==========\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\3711824180.py:52: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df['Date'] = pd.to_datetime(df['Date'], infer_datetime_format=True, errors='coerce')\n",
      "C:\\Users\\ravik\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m174/174\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0274 - mae: 0.1124 - val_loss: 0.0233 - val_mae: 0.1213\n",
      "Epoch 2/20\n",
      "\u001b[1m174/174\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 3.5590e-04 - mae: 0.0103 - val_loss: 0.0235 - val_mae: 0.1215\n",
      "Epoch 3/20\n",
      "\u001b[1m174/174\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 3.2265e-04 - mae: 0.0101 - val_loss: 0.0233 - val_mae: 0.1206\n",
      "Epoch 4/20\n",
      "\u001b[1m174/174\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 3.3201e-04 - mae: 0.0103 - val_loss: 0.0232 - val_mae: 0.1210\n",
      "Epoch 5/20\n",
      "\u001b[1m174/174\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 3.1566e-04 - mae: 0.0101 - val_loss: 0.0234 - val_mae: 0.1217\n",
      "Epoch 6/20\n",
      "\u001b[1m174/174\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 3.3501e-04 - mae: 0.0104 - val_loss: 0.0233 - val_mae: 0.1209\n",
      "Epoch 7/20\n",
      "\u001b[1m174/174\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 3.5164e-04 - mae: 0.0106 - val_loss: 0.0233 - val_mae: 0.1208\n",
      "Epoch 8/20\n",
      "\u001b[1m174/174\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 3.6342e-04 - mae: 0.0106 - val_loss: 0.0234 - val_mae: 0.1214\n",
      "Epoch 9/20\n",
      "\u001b[1m174/174\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 3.3423e-04 - mae: 0.0104 - val_loss: 0.0233 - val_mae: 0.1208\n",
      "Epoch 10/20\n",
      "\u001b[1m174/174\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 3.4908e-04 - mae: 0.0108 - val_loss: 0.0234 - val_mae: 0.1212\n",
      "Epoch 11/20\n",
      "\u001b[1m174/174\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 3.3071e-04 - mae: 0.0102 - val_loss: 0.0233 - val_mae: 0.1214\n",
      "Epoch 12/20\n",
      "\u001b[1m174/174\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 3.0212e-04 - mae: 0.0101 - val_loss: 0.0232 - val_mae: 0.1214\n",
      "Epoch 13/20\n",
      "\u001b[1m174/174\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 3.2729e-04 - mae: 0.0106 - val_loss: 0.0234 - val_mae: 0.1216\n",
      "Epoch 14/20\n",
      "\u001b[1m174/174\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 3.1625e-04 - mae: 0.0101 - val_loss: 0.0235 - val_mae: 0.1212\n",
      "Epoch 15/20\n",
      "\u001b[1m174/174\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 3.3809e-04 - mae: 0.0107 - val_loss: 0.0232 - val_mae: 0.1211\n",
      "Epoch 16/20\n",
      "\u001b[1m174/174\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 3.1928e-04 - mae: 0.0101 - val_loss: 0.0231 - val_mae: 0.1202\n",
      "Epoch 17/20\n",
      "\u001b[1m174/174\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 3.6322e-04 - mae: 0.0105 - val_loss: 0.0232 - val_mae: 0.1214\n",
      "Epoch 18/20\n",
      "\u001b[1m174/174\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 3.6091e-04 - mae: 0.0110 - val_loss: 0.0231 - val_mae: 0.1207\n",
      "Epoch 19/20\n",
      "\u001b[1m174/174\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 3.2063e-04 - mae: 0.0100 - val_loss: 0.0233 - val_mae: 0.1219\n",
      "Epoch 20/20\n",
      "\u001b[1m174/174\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 3.5023e-04 - mae: 0.0105 - val_loss: 0.0229 - val_mae: 0.1198\n",
      "\u001b[1m44/44\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
      "âœ… Done with capsicum_Hassan_daily.csv | MAE=738.6, RMSE=933.39, R2=0.26, MAPE=22.42%, Accuracy=77.58%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Processing: capsicum_Kalburgi_daily.csv ==========\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\3711824180.py:52: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df['Date'] = pd.to_datetime(df['Date'], infer_datetime_format=True, errors='coerce')\n",
      "C:\\Users\\ravik\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m30/30\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0909 - mae: 0.2549 - val_loss: 0.0395 - val_mae: 0.1835\n",
      "Epoch 2/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0100 - mae: 0.0754 - val_loss: 0.0038 - val_mae: 0.0474\n",
      "Epoch 3/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0050 - mae: 0.0469 - val_loss: 0.0020 - val_mae: 0.0318\n",
      "Epoch 4/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0041 - mae: 0.0402 - val_loss: 0.0024 - val_mae: 0.0354\n",
      "Epoch 5/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0040 - mae: 0.0408 - val_loss: 0.0022 - val_mae: 0.0324\n",
      "Epoch 6/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0041 - mae: 0.0398 - val_loss: 0.0019 - val_mae: 0.0293\n",
      "Epoch 7/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0042 - mae: 0.0396 - val_loss: 0.0018 - val_mae: 0.0268\n",
      "Epoch 8/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0039 - mae: 0.0376 - val_loss: 0.0017 - val_mae: 0.0290\n",
      "Epoch 9/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0037 - mae: 0.0392 - val_loss: 0.0017 - val_mae: 0.0258\n",
      "Epoch 10/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0032 - mae: 0.0339 - val_loss: 0.0017 - val_mae: 0.0262\n",
      "Epoch 11/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0038 - mae: 0.0368 - val_loss: 0.0016 - val_mae: 0.0255\n",
      "Epoch 12/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0033 - mae: 0.0359 - val_loss: 0.0017 - val_mae: 0.0277\n",
      "Epoch 13/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0038 - mae: 0.0374 - val_loss: 0.0015 - val_mae: 0.0237\n",
      "Epoch 14/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0033 - mae: 0.0362 - val_loss: 0.0016 - val_mae: 0.0265\n",
      "Epoch 15/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0037 - mae: 0.0373 - val_loss: 0.0014 - val_mae: 0.0226\n",
      "Epoch 16/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0039 - mae: 0.0364 - val_loss: 0.0014 - val_mae: 0.0229\n",
      "Epoch 17/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0035 - mae: 0.0360 - val_loss: 0.0020 - val_mae: 0.0335\n",
      "Epoch 18/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0036 - mae: 0.0374 - val_loss: 0.0013 - val_mae: 0.0224\n",
      "Epoch 19/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0039 - mae: 0.0406 - val_loss: 0.0016 - val_mae: 0.0274\n",
      "Epoch 20/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0034 - mae: 0.0355 - val_loss: 0.0012 - val_mae: 0.0198\n",
      "\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step\n",
      "âœ… Done with capsicum_Kalburgi_daily.csv | MAE=136.85, RMSE=239.47, R2=0.91, MAPE=3.46%, Accuracy=96.54%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Processing: capsicum_Kolar_daily.csv ==========\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\3711824180.py:52: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df['Date'] = pd.to_datetime(df['Date'], infer_datetime_format=True, errors='coerce')\n",
      "C:\\Users\\ravik\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m539/539\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 0.0104 - mae: 0.0720 - val_loss: 0.0019 - val_mae: 0.0317\n",
      "Epoch 2/20\n",
      "\u001b[1m539/539\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0076 - mae: 0.0610 - val_loss: 0.0019 - val_mae: 0.0323\n",
      "Epoch 3/20\n",
      "\u001b[1m539/539\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0075 - mae: 0.0600 - val_loss: 0.0019 - val_mae: 0.0324\n",
      "Epoch 4/20\n",
      "\u001b[1m539/539\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0073 - mae: 0.0588 - val_loss: 0.0019 - val_mae: 0.0319\n",
      "Epoch 5/20\n",
      "\u001b[1m539/539\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0069 - mae: 0.0574 - val_loss: 0.0019 - val_mae: 0.0320\n",
      "Epoch 6/20\n",
      "\u001b[1m539/539\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0071 - mae: 0.0583 - val_loss: 0.0021 - val_mae: 0.0342\n",
      "Epoch 7/20\n",
      "\u001b[1m539/539\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0068 - mae: 0.0574 - val_loss: 0.0019 - val_mae: 0.0316\n",
      "Epoch 8/20\n",
      "\u001b[1m539/539\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 0.0069 - mae: 0.0572 - val_loss: 0.0019 - val_mae: 0.0316\n",
      "Epoch 9/20\n",
      "\u001b[1m539/539\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0070 - mae: 0.0578 - val_loss: 0.0019 - val_mae: 0.0318\n",
      "Epoch 10/20\n",
      "\u001b[1m539/539\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0067 - mae: 0.0568 - val_loss: 0.0019 - val_mae: 0.0317\n",
      "Epoch 11/20\n",
      "\u001b[1m539/539\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0066 - mae: 0.0565 - val_loss: 0.0019 - val_mae: 0.0322\n",
      "Epoch 12/20\n",
      "\u001b[1m539/539\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0066 - mae: 0.0566 - val_loss: 0.0019 - val_mae: 0.0316\n",
      "Epoch 13/20\n",
      "\u001b[1m539/539\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0068 - mae: 0.0571 - val_loss: 0.0020 - val_mae: 0.0323\n",
      "Epoch 14/20\n",
      "\u001b[1m539/539\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0066 - mae: 0.0565 - val_loss: 0.0019 - val_mae: 0.0318\n",
      "Epoch 15/20\n",
      "\u001b[1m539/539\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0066 - mae: 0.0563 - val_loss: 0.0019 - val_mae: 0.0318\n",
      "Epoch 16/20\n",
      "\u001b[1m539/539\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0066 - mae: 0.0563 - val_loss: 0.0020 - val_mae: 0.0332\n",
      "Epoch 17/20\n",
      "\u001b[1m539/539\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0066 - mae: 0.0563 - val_loss: 0.0021 - val_mae: 0.0337\n",
      "Epoch 18/20\n",
      "\u001b[1m539/539\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0064 - mae: 0.0550 - val_loss: 0.0020 - val_mae: 0.0331\n",
      "Epoch 19/20\n",
      "\u001b[1m539/539\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0063 - mae: 0.0554 - val_loss: 0.0019 - val_mae: 0.0321\n",
      "Epoch 20/20\n",
      "\u001b[1m539/539\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0065 - mae: 0.0559 - val_loss: 0.0019 - val_mae: 0.0322\n",
      "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "âœ… Done with capsicum_Kolar_daily.csv | MAE=579.29, RMSE=784.69, R2=0.26, MAPE=25.4%, Accuracy=74.6%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Processing: capsicum_Mandya_daily.csv ==========\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\3711824180.py:52: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df['Date'] = pd.to_datetime(df['Date'], infer_datetime_format=True, errors='coerce')\n",
      "C:\\Users\\ravik\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0265 - mae: 0.1042 - val_loss: 0.0078 - val_mae: 0.0697\n",
      "Epoch 2/20\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0043 - mae: 0.0298 - val_loss: 0.0072 - val_mae: 0.0671\n",
      "Epoch 3/20\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0040 - mae: 0.0260 - val_loss: 0.0068 - val_mae: 0.0630\n",
      "Epoch 4/20\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0034 - mae: 0.0254 - val_loss: 0.0068 - val_mae: 0.0593\n",
      "Epoch 5/20\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0037 - mae: 0.0284 - val_loss: 0.0071 - val_mae: 0.0667\n",
      "Epoch 6/20\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0038 - mae: 0.0278 - val_loss: 0.0077 - val_mae: 0.0709\n",
      "Epoch 7/20\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0035 - mae: 0.0254 - val_loss: 0.0064 - val_mae: 0.0596\n",
      "Epoch 8/20\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0034 - mae: 0.0286 - val_loss: 0.0064 - val_mae: 0.0624\n",
      "Epoch 9/20\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0029 - mae: 0.0243 - val_loss: 0.0063 - val_mae: 0.0623\n",
      "Epoch 10/20\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0035 - mae: 0.0284 - val_loss: 0.0065 - val_mae: 0.0629\n",
      "Epoch 11/20\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0030 - mae: 0.0250 - val_loss: 0.0069 - val_mae: 0.0651\n",
      "Epoch 12/20\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0032 - mae: 0.0247 - val_loss: 0.0061 - val_mae: 0.0560\n",
      "Epoch 13/20\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0029 - mae: 0.0255 - val_loss: 0.0057 - val_mae: 0.0545\n",
      "Epoch 14/20\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0027 - mae: 0.0265 - val_loss: 0.0060 - val_mae: 0.0532\n",
      "Epoch 15/20\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0026 - mae: 0.0237 - val_loss: 0.0056 - val_mae: 0.0512\n",
      "Epoch 16/20\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0028 - mae: 0.0240 - val_loss: 0.0086 - val_mae: 0.0754\n",
      "Epoch 17/20\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0031 - mae: 0.0244 - val_loss: 0.0058 - val_mae: 0.0509\n",
      "Epoch 18/20\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0027 - mae: 0.0220 - val_loss: 0.0058 - val_mae: 0.0509\n",
      "Epoch 19/20\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0029 - mae: 0.0239 - val_loss: 0.0056 - val_mae: 0.0507\n",
      "Epoch 20/20\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0026 - mae: 0.0218 - val_loss: 0.0059 - val_mae: 0.0519\n",
      "\u001b[1m36/36\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "âœ… Done with capsicum_Mandya_daily.csv | MAE=293.29, RMSE=434.57, R2=0.53, MAPE=10.39%, Accuracy=89.61%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Processing: capsicum_Mysore_daily.csv ==========\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\3711824180.py:52: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df['Date'] = pd.to_datetime(df['Date'], infer_datetime_format=True, errors='coerce')\n",
      "C:\\Users\\ravik\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m256/256\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 0.0030 - mae: 0.0394 - val_loss: 0.0019 - val_mae: 0.0297\n",
      "Epoch 2/20\n",
      "\u001b[1m256/256\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0011 - mae: 0.0227 - val_loss: 0.0019 - val_mae: 0.0296\n",
      "Epoch 3/20\n",
      "\u001b[1m256/256\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0011 - mae: 0.0224 - val_loss: 0.0019 - val_mae: 0.0297\n",
      "Epoch 4/20\n",
      "\u001b[1m256/256\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0010 - mae: 0.0218 - val_loss: 0.0018 - val_mae: 0.0292\n",
      "Epoch 5/20\n",
      "\u001b[1m256/256\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0010 - mae: 0.0216 - val_loss: 0.0018 - val_mae: 0.0291\n",
      "Epoch 6/20\n",
      "\u001b[1m256/256\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 9.3968e-04 - mae: 0.0207 - val_loss: 0.0018 - val_mae: 0.0287\n",
      "Epoch 7/20\n",
      "\u001b[1m256/256\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 8.8455e-04 - mae: 0.0198 - val_loss: 0.0020 - val_mae: 0.0303\n",
      "Epoch 8/20\n",
      "\u001b[1m256/256\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 9.1347e-04 - mae: 0.0203 - val_loss: 0.0019 - val_mae: 0.0291\n",
      "Epoch 9/20\n",
      "\u001b[1m256/256\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 8.9118e-04 - mae: 0.0198 - val_loss: 0.0019 - val_mae: 0.0294\n",
      "Epoch 10/20\n",
      "\u001b[1m256/256\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 9.4818e-04 - mae: 0.0205 - val_loss: 0.0019 - val_mae: 0.0288\n",
      "Epoch 11/20\n",
      "\u001b[1m256/256\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 9.0856e-04 - mae: 0.0198 - val_loss: 0.0019 - val_mae: 0.0289\n",
      "Epoch 12/20\n",
      "\u001b[1m256/256\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 8.7848e-04 - mae: 0.0195 - val_loss: 0.0019 - val_mae: 0.0288\n",
      "Epoch 13/20\n",
      "\u001b[1m256/256\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 8.7597e-04 - mae: 0.0194 - val_loss: 0.0021 - val_mae: 0.0313\n",
      "Epoch 14/20\n",
      "\u001b[1m256/256\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 8.5460e-04 - mae: 0.0192 - val_loss: 0.0019 - val_mae: 0.0288\n",
      "Epoch 15/20\n",
      "\u001b[1m256/256\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 8.6513e-04 - mae: 0.0194 - val_loss: 0.0019 - val_mae: 0.0287\n",
      "Epoch 16/20\n",
      "\u001b[1m256/256\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 9.3236e-04 - mae: 0.0196 - val_loss: 0.0019 - val_mae: 0.0287\n",
      "Epoch 17/20\n",
      "\u001b[1m256/256\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 8.4471e-04 - mae: 0.0191 - val_loss: 0.0019 - val_mae: 0.0291\n",
      "Epoch 18/20\n",
      "\u001b[1m256/256\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 9.2611e-04 - mae: 0.0197 - val_loss: 0.0019 - val_mae: 0.0288\n",
      "Epoch 19/20\n",
      "\u001b[1m256/256\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 8.4767e-04 - mae: 0.0191 - val_loss: 0.0019 - val_mae: 0.0290\n",
      "Epoch 20/20\n",
      "\u001b[1m256/256\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 8.9810e-04 - mae: 0.0199 - val_loss: 0.0019 - val_mae: 0.0291\n",
      "\u001b[1m64/64\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "âœ… Done with capsicum_Mysore_daily.csv | MAE=693.23, RMSE=1044.46, R2=0.26, MAPE=20.75%, Accuracy=79.25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Processing: capsicum_Shimoga_daily.csv ==========\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\3711824180.py:52: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df['Date'] = pd.to_datetime(df['Date'], infer_datetime_format=True, errors='coerce')\n",
      "C:\\Users\\ravik\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 200ms/step - loss: 0.2406 - mae: 0.4305 - val_loss: 0.9030 - val_mae: 0.9482\n",
      "Epoch 2/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.2328 - mae: 0.4178 - val_loss: 0.8596 - val_mae: 0.9251\n",
      "Epoch 3/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.2262 - mae: 0.4125 - val_loss: 0.8209 - val_mae: 0.9040\n",
      "Epoch 4/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.2081 - mae: 0.3942 - val_loss: 0.7864 - val_mae: 0.8848\n",
      "Epoch 5/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.1905 - mae: 0.3736 - val_loss: 0.7524 - val_mae: 0.8654\n",
      "Epoch 6/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.2074 - mae: 0.3938 - val_loss: 0.7181 - val_mae: 0.8454\n",
      "Epoch 7/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.1828 - mae: 0.3663 - val_loss: 0.6843 - val_mae: 0.8253\n",
      "Epoch 8/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1616 - mae: 0.3356 - val_loss: 0.6501 - val_mae: 0.8044\n",
      "Epoch 9/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.1559 - mae: 0.3318 - val_loss: 0.6154 - val_mae: 0.7825\n",
      "Epoch 10/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.1372 - mae: 0.3090 - val_loss: 0.5787 - val_mae: 0.7588\n",
      "Epoch 11/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.1358 - mae: 0.3108 - val_loss: 0.5388 - val_mae: 0.7321\n",
      "Epoch 12/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.1100 - mae: 0.2715 - val_loss: 0.4936 - val_mae: 0.7007\n",
      "Epoch 13/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.1181 - mae: 0.2833 - val_loss: 0.4424 - val_mae: 0.6632\n",
      "Epoch 14/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.0901 - mae: 0.2481 - val_loss: 0.3868 - val_mae: 0.6200\n",
      "Epoch 15/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.0689 - mae: 0.2136 - val_loss: 0.3252 - val_mae: 0.5684\n",
      "Epoch 16/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.0588 - mae: 0.1978 - val_loss: 0.2594 - val_mae: 0.5074\n",
      "Epoch 17/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.0423 - mae: 0.1677 - val_loss: 0.1922 - val_mae: 0.4365\n",
      "Epoch 18/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.0327 - mae: 0.1522 - val_loss: 0.1290 - val_mae: 0.3574\n",
      "Epoch 19/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.0213 - mae: 0.1253 - val_loss: 0.0761 - val_mae: 0.2741\n",
      "Epoch 20/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.0171 - mae: 0.1124 - val_loss: 0.0385 - val_mae: 0.1946\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 292ms/step\n",
      "âœ… Done with capsicum_Shimoga_daily.csv | MAE=126.46, RMSE=127.52, R2=-10.12, MAPE=6.87%, Accuracy=93.13%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Processing: capsicum_Udupi_daily.csv ==========\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\3711824180.py:52: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df['Date'] = pd.to_datetime(df['Date'], infer_datetime_format=True, errors='coerce')\n",
      "C:\\Users\\ravik\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0157 - mae: 0.0806 - val_loss: 0.0040 - val_mae: 0.0344\n",
      "Epoch 2/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 5.7202e-04 - mae: 0.0090 - val_loss: 0.0038 - val_mae: 0.0354\n",
      "Epoch 3/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 3.9146e-04 - mae: 0.0085 - val_loss: 0.0035 - val_mae: 0.0320\n",
      "Epoch 4/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 4.0982e-04 - mae: 0.0080 - val_loss: 0.0034 - val_mae: 0.0315\n",
      "Epoch 5/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2.9001e-04 - mae: 0.0078 - val_loss: 0.0036 - val_mae: 0.0370\n",
      "Epoch 6/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 4.0213e-04 - mae: 0.0109 - val_loss: 0.0030 - val_mae: 0.0287\n",
      "Epoch 7/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 3.3739e-04 - mae: 0.0076 - val_loss: 0.0029 - val_mae: 0.0268\n",
      "Epoch 8/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 3.7715e-04 - mae: 0.0089 - val_loss: 0.0027 - val_mae: 0.0271\n",
      "Epoch 9/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 3.4613e-04 - mae: 0.0079 - val_loss: 0.0025 - val_mae: 0.0260\n",
      "Epoch 10/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 3.7084e-04 - mae: 0.0086 - val_loss: 0.0024 - val_mae: 0.0257\n",
      "Epoch 11/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 2.6213e-04 - mae: 0.0069 - val_loss: 0.0022 - val_mae: 0.0239\n",
      "Epoch 12/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 2.4175e-04 - mae: 0.0072 - val_loss: 0.0021 - val_mae: 0.0216\n",
      "Epoch 13/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 2.3345e-04 - mae: 0.0079 - val_loss: 0.0021 - val_mae: 0.0229\n",
      "Epoch 14/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 2.3566e-04 - mae: 0.0071 - val_loss: 0.0023 - val_mae: 0.0273\n",
      "Epoch 15/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.9021e-04 - mae: 0.0068 - val_loss: 0.0020 - val_mae: 0.0213\n",
      "Epoch 16/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.6595e-04 - mae: 0.0063 - val_loss: 0.0021 - val_mae: 0.0259\n",
      "Epoch 17/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 2.3312e-04 - mae: 0.0090 - val_loss: 0.0020 - val_mae: 0.0228\n",
      "Epoch 18/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.9394e-04 - mae: 0.0066 - val_loss: 0.0019 - val_mae: 0.0199\n",
      "Epoch 19/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.8696e-04 - mae: 0.0055 - val_loss: 0.0019 - val_mae: 0.0195\n",
      "Epoch 20/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.9091e-04 - mae: 0.0066 - val_loss: 0.0019 - val_mae: 0.0207\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "âœ… Done with capsicum_Udupi_daily.csv | MAE=182.37, RMSE=384.65, R2=0.91, MAPE=3.67%, Accuracy=96.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Metrics saved to output_lstm_csv\\lstm_metrics.csv\n",
      "\n",
      "All districts processed successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# -----------------------------\n",
    "# Paths\n",
    "# -----------------------------\n",
    "input_folder = \"dataset\"\n",
    "output_models = \"output_lstm_models\"\n",
    "output_csv = \"output_lstm_csv\"\n",
    "output_graphs = \"output_lstm_graphs\"\n",
    "output_logs = \"output_lstm_logs\"\n",
    "metrics_file = os.path.join(output_csv, \"lstm_metrics.csv\")\n",
    "\n",
    "os.makedirs(output_models, exist_ok=True)\n",
    "os.makedirs(output_csv, exist_ok=True)\n",
    "os.makedirs(output_graphs, exist_ok=True)\n",
    "os.makedirs(output_logs, exist_ok=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Function to create sequences\n",
    "# -----------------------------\n",
    "def create_sequences(data, seq_length=5):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X.append(data[i:i+seq_length])\n",
    "        y.append(data[i+seq_length])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# -----------------------------\n",
    "# Metrics storage\n",
    "# -----------------------------\n",
    "metrics_list = []\n",
    "\n",
    "# -----------------------------\n",
    "# Process each CSV\n",
    "# -----------------------------\n",
    "seq_length = 5\n",
    "for file in os.listdir(input_folder):\n",
    "    if file.endswith(\".csv\"):\n",
    "        district_name = file.split(\".\")[0]\n",
    "        print(f\"\\n========== Processing: {file} ==========\")\n",
    "\n",
    "        # Load CSV with multiple date formats\n",
    "        df = pd.read_csv(os.path.join(input_folder, file))\n",
    "        df['Date'] = pd.to_datetime(df['Date'], infer_datetime_format=True, errors='coerce')\n",
    "        df = df.dropna(subset=['Date'])\n",
    "        df = df.sort_values('Date')\n",
    "\n",
    "        # Fill missing Average Price\n",
    "        df['Average Price'] = df['Average Price'].fillna(df['Average Price'].mean())\n",
    "\n",
    "        # Moving averages (for plotting only)\n",
    "        df['MA_7'] = df['Average Price'].rolling(window=7).mean().fillna(df['Average Price'].mean())\n",
    "        df['MA_30'] = df['Average Price'].rolling(window=30).mean().fillna(df['Average Price'].mean())\n",
    "\n",
    "        # Scaling\n",
    "        scaler = MinMaxScaler()\n",
    "        prices_scaled = scaler.fit_transform(df['Average Price'].values.reshape(-1, 1))\n",
    "\n",
    "        # Prepare sequences\n",
    "        X, y = create_sequences(prices_scaled, seq_length)\n",
    "\n",
    "        # Train-test split (80%-20%)\n",
    "        split = int(0.8 * len(X))\n",
    "        X_train, X_test = X[:split], X[split:]\n",
    "        y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "        # Build LSTM model\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(50, activation='relu', input_shape=(seq_length, 1)))\n",
    "        model.add(Dense(1))\n",
    "        model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=20,\n",
    "            batch_size=32,\n",
    "            validation_data=(X_test, y_test),\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # Predictions\n",
    "        y_pred_scaled = model.predict(X_test)\n",
    "        y_pred = scaler.inverse_transform(y_pred_scaled)\n",
    "        y_true = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "        # Round Actual & Predicted\n",
    "        y_true_round = np.round(y_true, 2)\n",
    "        y_pred_round = np.round(y_pred, 2)\n",
    "\n",
    "        # Save predictions in CSV (only Date, Actual, Predicted)\n",
    "        df_pred = df.iloc[seq_length + split:].copy()\n",
    "        df_pred = df_pred[['Date']].copy()\n",
    "        df_pred['Actual'] = y_true_round.flatten()\n",
    "        df_pred['Predicted'] = y_pred_round.flatten()\n",
    "        df_pred.to_csv(os.path.join(output_csv, f\"{district_name}_lstm_updated.csv\"), index=False)\n",
    "\n",
    "        # Metrics calculation\n",
    "        mae_val = round(mean_absolute_error(y_true, y_pred), 2)\n",
    "        rmse_val = round(mean_squared_error(y_true, y_pred, squared=False), 2)\n",
    "        r2_val = round(r2_score(y_true, y_pred), 2)\n",
    "        mape_val = round(np.mean(np.abs((y_true - y_pred) / y_true)) * 100, 2)\n",
    "        accuracy_val = round(100 - mape_val, 2)\n",
    "        metrics_list.append([district_name, mae_val, rmse_val, r2_val, mape_val, accuracy_val])\n",
    "\n",
    "        print(f\"âœ… Done with {file} | MAE={mae_val}, RMSE={rmse_val}, R2={r2_val}, \"\n",
    "              f\"MAPE={mape_val}%, Accuracy={accuracy_val}%\")\n",
    "\n",
    "        # Save model as .pkl\n",
    "        model_file = os.path.join(output_models, f\"{district_name}_lstm_model.pkl\")\n",
    "        joblib.dump(model, model_file)\n",
    "\n",
    "        # Save prediction graph (with MA7 & MA30 only for graphing)\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(df['Date'], df['Average Price'], label='Actual', color='blue')\n",
    "        plt.plot(df['Date'], df['MA_7'], label='MA 7', color='orange')\n",
    "        plt.plot(df['Date'], df['MA_30'], label='MA 30', color='green')\n",
    "        plt.plot(df_pred['Date'], df_pred['Predicted'], label='Predicted', color='red', linestyle='dashed')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Average Price')\n",
    "        plt.title(f\"LSTM Predictions for {district_name}\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig(os.path.join(output_graphs, f\"{district_name}_lstm_graph.png\"))\n",
    "        plt.close()\n",
    "\n",
    "        # Save training loss graph\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        plt.plot(history.history['loss'], label='Train Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "        plt.title(f\"Training Loss for {district_name}\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "        plt.savefig(os.path.join(output_graphs, f\"{district_name}_lstm_training_loss.png\"))\n",
    "        plt.close()\n",
    "\n",
    "# Save metrics CSV\n",
    "metrics_df = pd.DataFrame(metrics_list, columns=['District', 'MAE', 'RMSE', 'R2', 'MAPE(%)', 'Accuracy(%)'])\n",
    "metrics_df.to_csv(metrics_file, index=False)\n",
    "print(f\"\\nğŸ“Š Metrics saved to {metrics_file}\")\n",
    "print(\"\\nAll districts processed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36651f8d-7c6c-4f8b-8e3d-bc1de5f8397e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "692646be-5b8b-400a-ad87-7a8c36d59735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ Processing: capsicum_Bangalore_daily.csv\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\1911045302.py:54: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df['Date'] = pd.to_datetime(df['Date'], errors='coerce', infer_datetime_format=True)\n",
      "C:\\Users\\ravik\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 24ms/step - loss: 0.0093 - mae: 0.0650 - val_loss: 0.0124 - val_mae: 0.0812\n",
      "Epoch 2/20\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 30ms/step - loss: 0.0060 - mae: 0.0519 - val_loss: 0.0106 - val_mae: 0.0800\n",
      "Epoch 3/20\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 28ms/step - loss: 0.0058 - mae: 0.0506 - val_loss: 0.0100 - val_mae: 0.0757\n",
      "Epoch 4/20\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 38ms/step - loss: 0.0050 - mae: 0.0469 - val_loss: 0.0105 - val_mae: 0.0758\n",
      "Epoch 5/20\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 31ms/step - loss: 0.0055 - mae: 0.0475 - val_loss: 0.0100 - val_mae: 0.0758\n",
      "Epoch 6/20\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 35ms/step - loss: 0.0051 - mae: 0.0470 - val_loss: 0.0101 - val_mae: 0.0751\n",
      "Epoch 7/20\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 26ms/step - loss: 0.0051 - mae: 0.0472 - val_loss: 0.0107 - val_mae: 0.0763\n",
      "Epoch 8/20\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 37ms/step - loss: 0.0052 - mae: 0.0463 - val_loss: 0.0103 - val_mae: 0.0754\n",
      "Epoch 9/20\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 34ms/step - loss: 0.0048 - mae: 0.0451 - val_loss: 0.0105 - val_mae: 0.0755\n",
      "Epoch 10/20\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 35ms/step - loss: 0.0047 - mae: 0.0458 - val_loss: 0.0103 - val_mae: 0.0745\n",
      "Epoch 11/20\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 30ms/step - loss: 0.0048 - mae: 0.0446 - val_loss: 0.0104 - val_mae: 0.0761\n",
      "Epoch 12/20\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 32ms/step - loss: 0.0051 - mae: 0.0462 - val_loss: 0.0104 - val_mae: 0.0784\n",
      "Epoch 13/20\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 35ms/step - loss: 0.0046 - mae: 0.0446 - val_loss: 0.0101 - val_mae: 0.0771\n",
      "Epoch 14/20\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 27ms/step - loss: 0.0050 - mae: 0.0454 - val_loss: 0.0100 - val_mae: 0.0757\n",
      "Epoch 15/20\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 29ms/step - loss: 0.0044 - mae: 0.0437 - val_loss: 0.0105 - val_mae: 0.0764\n",
      "Epoch 16/20\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 29ms/step - loss: 0.0046 - mae: 0.0448 - val_loss: 0.0101 - val_mae: 0.0771\n",
      "Epoch 17/20\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 30ms/step - loss: 0.0044 - mae: 0.0437 - val_loss: 0.0101 - val_mae: 0.0748\n",
      "Epoch 18/20\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 28ms/step - loss: 0.0042 - mae: 0.0431 - val_loss: 0.0099 - val_mae: 0.0757\n",
      "Epoch 19/20\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 28ms/step - loss: 0.0047 - mae: 0.0449 - val_loss: 0.0099 - val_mae: 0.0758\n",
      "Epoch 20/20\n",
      "\u001b[1m463/463\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 28ms/step - loss: 0.0045 - mae: 0.0444 - val_loss: 0.0099 - val_mae: 0.0759\n",
      "\u001b[1m58/58\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "âœ… Done with capsicum_Bangalore_daily.csv | MAE=731.95, RMSE=958.64, R2=0.35, MAPE=20.27%, Accuracy=79.73%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ Processing: capsicum_Belgaum_daily.csv\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\1911045302.py:54: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df['Date'] = pd.to_datetime(df['Date'], errors='coerce', infer_datetime_format=True)\n",
      "C:\\Users\\ravik\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - loss: 0.0264 - mae: 0.1230 - val_loss: 6.2409e-04 - val_mae: 0.0154\n",
      "Epoch 2/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0046 - mae: 0.0495 - val_loss: 4.3884e-04 - val_mae: 0.0148\n",
      "Epoch 3/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0052 - mae: 0.0499 - val_loss: 4.5446e-04 - val_mae: 0.0125\n",
      "Epoch 4/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0053 - mae: 0.0464 - val_loss: 9.5836e-04 - val_mae: 0.0257\n",
      "Epoch 5/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0036 - mae: 0.0411 - val_loss: 4.4142e-04 - val_mae: 0.0166\n",
      "Epoch 6/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0033 - mae: 0.0414 - val_loss: 7.0919e-04 - val_mae: 0.0211\n",
      "Epoch 7/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0030 - mae: 0.0392 - val_loss: 7.9345e-04 - val_mae: 0.0231\n",
      "Epoch 8/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 0.0029 - mae: 0.0365 - val_loss: 4.1795e-04 - val_mae: 0.0135\n",
      "Epoch 9/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0038 - mae: 0.0389 - val_loss: 9.1138e-04 - val_mae: 0.0261\n",
      "Epoch 10/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0026 - mae: 0.0360 - val_loss: 3.3978e-04 - val_mae: 0.0128\n",
      "Epoch 11/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0035 - mae: 0.0409 - val_loss: 0.0018 - val_mae: 0.0389\n",
      "Epoch 12/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0028 - mae: 0.0343 - val_loss: 4.1866e-04 - val_mae: 0.0145\n",
      "Epoch 13/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0024 - mae: 0.0331 - val_loss: 9.2661e-04 - val_mae: 0.0263\n",
      "Epoch 14/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0029 - mae: 0.0347 - val_loss: 3.0551e-04 - val_mae: 0.0103\n",
      "Epoch 15/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0028 - mae: 0.0348 - val_loss: 3.3667e-04 - val_mae: 0.0117\n",
      "Epoch 16/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - loss: 0.0027 - mae: 0.0330 - val_loss: 2.8440e-04 - val_mae: 0.0090\n",
      "Epoch 17/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 0.0031 - mae: 0.0333 - val_loss: 0.0020 - val_mae: 0.0411\n",
      "Epoch 18/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0024 - mae: 0.0316 - val_loss: 8.2047e-04 - val_mae: 0.0248\n",
      "Epoch 19/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0025 - mae: 0.0311 - val_loss: 2.3737e-04 - val_mae: 0.0074\n",
      "Epoch 20/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0025 - mae: 0.0303 - val_loss: 2.9583e-04 - val_mae: 0.0104\n",
      "\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step\n",
      "âœ… Done with capsicum_Belgaum_daily.csv | MAE=69.18, RMSE=114.38, R2=0.93, MAPE=1.89%, Accuracy=98.11%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ Processing: capsicum_Bellary_daily.csv\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\1911045302.py:54: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df['Date'] = pd.to_datetime(df['Date'], errors='coerce', infer_datetime_format=True)\n",
      "C:\\Users\\ravik\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 200ms/step - loss: 0.1022 - mae: 0.2580 - val_loss: 0.2230 - val_mae: 0.4211\n",
      "Epoch 2/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.0272 - mae: 0.1280 - val_loss: 0.0819 - val_mae: 0.2605\n",
      "Epoch 3/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.0277 - mae: 0.1373 - val_loss: 0.0757 - val_mae: 0.2461\n",
      "Epoch 4/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.0201 - mae: 0.1238 - val_loss: 0.0983 - val_mae: 0.2886\n",
      "Epoch 5/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.0180 - mae: 0.1033 - val_loss: 0.1102 - val_mae: 0.3039\n",
      "Epoch 6/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.0159 - mae: 0.0982 - val_loss: 0.1009 - val_mae: 0.2918\n",
      "Epoch 7/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.0158 - mae: 0.0895 - val_loss: 0.0857 - val_mae: 0.2679\n",
      "Epoch 8/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0169 - mae: 0.0976 - val_loss: 0.0769 - val_mae: 0.2493\n",
      "Epoch 9/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.0151 - mae: 0.0960 - val_loss: 0.0755 - val_mae: 0.2451\n",
      "Epoch 10/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.0132 - mae: 0.0887 - val_loss: 0.0762 - val_mae: 0.2464\n",
      "Epoch 11/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.0128 - mae: 0.0852 - val_loss: 0.0814 - val_mae: 0.2575\n",
      "Epoch 12/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.0133 - mae: 0.0835 - val_loss: 0.0782 - val_mae: 0.2502\n",
      "Epoch 13/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.0121 - mae: 0.0763 - val_loss: 0.0737 - val_mae: 0.2397\n",
      "Epoch 14/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.0119 - mae: 0.0796 - val_loss: 0.0695 - val_mae: 0.2300\n",
      "Epoch 15/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.0094 - mae: 0.0727 - val_loss: 0.0699 - val_mae: 0.2310\n",
      "Epoch 16/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.0150 - mae: 0.0936 - val_loss: 0.0706 - val_mae: 0.2321\n",
      "Epoch 17/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.0138 - mae: 0.0845 - val_loss: 0.0708 - val_mae: 0.2321\n",
      "Epoch 18/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.0122 - mae: 0.0746 - val_loss: 0.0682 - val_mae: 0.2262\n",
      "Epoch 19/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0093 - mae: 0.0696 - val_loss: 0.0659 - val_mae: 0.2207\n",
      "Epoch 20/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0092 - mae: 0.0707 - val_loss: 0.0629 - val_mae: 0.2128\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 405ms/step\n",
      "âœ… Done with capsicum_Bellary_daily.csv | MAE=446.81, RMSE=526.7, R2=-0.03, MAPE=15.45%, Accuracy=84.55%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ Processing: capsicum_Chikmagalur_daily.csv\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\1911045302.py:54: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df['Date'] = pd.to_datetime(df['Date'], errors='coerce', infer_datetime_format=True)\n",
      "C:\\Users\\ravik\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 23ms/step - loss: 0.0225 - mae: 0.1082 - val_loss: 0.0118 - val_mae: 0.0738\n",
      "Epoch 2/20\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 0.0062 - mae: 0.0623 - val_loss: 0.0129 - val_mae: 0.0862\n",
      "Epoch 3/20\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0057 - mae: 0.0586 - val_loss: 0.0125 - val_mae: 0.0766\n",
      "Epoch 4/20\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0052 - mae: 0.0564 - val_loss: 0.0131 - val_mae: 0.0798\n",
      "Epoch 5/20\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 0.0050 - mae: 0.0563 - val_loss: 0.0138 - val_mae: 0.0857\n",
      "Epoch 6/20\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0046 - mae: 0.0530 - val_loss: 0.0138 - val_mae: 0.0785\n",
      "Epoch 7/20\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - loss: 0.0042 - mae: 0.0489 - val_loss: 0.0146 - val_mae: 0.0801\n",
      "Epoch 8/20\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - loss: 0.0042 - mae: 0.0490 - val_loss: 0.0152 - val_mae: 0.0790\n",
      "Epoch 9/20\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 0.0041 - mae: 0.0475 - val_loss: 0.0159 - val_mae: 0.0882\n",
      "Epoch 10/20\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.0037 - mae: 0.0447 - val_loss: 0.0181 - val_mae: 0.1001\n",
      "Epoch 11/20\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 0.0036 - mae: 0.0442 - val_loss: 0.0172 - val_mae: 0.0852\n",
      "Epoch 12/20\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 0.0037 - mae: 0.0445 - val_loss: 0.0177 - val_mae: 0.0895\n",
      "Epoch 13/20\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - loss: 0.0033 - mae: 0.0412 - val_loss: 0.0188 - val_mae: 0.0935\n",
      "Epoch 14/20\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - loss: 0.0040 - mae: 0.0446 - val_loss: 0.0186 - val_mae: 0.0893\n",
      "Epoch 15/20\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - loss: 0.0032 - mae: 0.0408 - val_loss: 0.0189 - val_mae: 0.0904\n",
      "Epoch 16/20\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 24ms/step - loss: 0.0035 - mae: 0.0424 - val_loss: 0.0189 - val_mae: 0.0843\n",
      "Epoch 17/20\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - loss: 0.0034 - mae: 0.0421 - val_loss: 0.0194 - val_mae: 0.0893\n",
      "Epoch 18/20\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - loss: 0.0037 - mae: 0.0444 - val_loss: 0.0198 - val_mae: 0.0987\n",
      "Epoch 19/20\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 27ms/step - loss: 0.0030 - mae: 0.0388 - val_loss: 0.0197 - val_mae: 0.0889\n",
      "Epoch 20/20\n",
      "\u001b[1m152/152\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 25ms/step - loss: 0.0030 - mae: 0.0393 - val_loss: 0.0203 - val_mae: 0.0898\n",
      "\u001b[1m19/19\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step\n",
      "âœ… Done with capsicum_Chikmagalur_daily.csv | MAE=314.29, RMSE=498.59, R2=0.61, MAPE=17.4%, Accuracy=82.6%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ Processing: capsicum_Davangere_daily.csv\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\1911045302.py:54: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df['Date'] = pd.to_datetime(df['Date'], errors='coerce', infer_datetime_format=True)\n",
      "C:\\Users\\ravik\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 23ms/step - loss: 0.0044 - mae: 0.0405 - val_loss: 0.0017 - val_mae: 0.0239\n",
      "Epoch 2/20\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 6.4806e-04 - mae: 0.0177 - val_loss: 0.0018 - val_mae: 0.0327\n",
      "Epoch 3/20\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 5.1426e-04 - mae: 0.0156 - val_loss: 0.0010 - val_mae: 0.0190\n",
      "Epoch 4/20\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - loss: 3.6846e-04 - mae: 0.0128 - val_loss: 9.5409e-04 - val_mae: 0.0205\n",
      "Epoch 5/20\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - loss: 3.6230e-04 - mae: 0.0127 - val_loss: 9.1608e-04 - val_mae: 0.0201\n",
      "Epoch 6/20\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 3.3897e-04 - mae: 0.0118 - val_loss: 7.6991e-04 - val_mae: 0.0165\n",
      "Epoch 7/20\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 20ms/step - loss: 3.3983e-04 - mae: 0.0114 - val_loss: 7.2280e-04 - val_mae: 0.0145\n",
      "Epoch 8/20\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - loss: 3.8562e-04 - mae: 0.0115 - val_loss: 0.0013 - val_mae: 0.0262\n",
      "Epoch 9/20\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - loss: 3.0900e-04 - mae: 0.0108 - val_loss: 9.2440e-04 - val_mae: 0.0214\n",
      "Epoch 10/20\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - loss: 3.0208e-04 - mae: 0.0108 - val_loss: 7.6557e-04 - val_mae: 0.0166\n",
      "Epoch 11/20\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 27ms/step - loss: 3.0965e-04 - mae: 0.0113 - val_loss: 7.0265e-04 - val_mae: 0.0142\n",
      "Epoch 12/20\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - loss: 2.5597e-04 - mae: 0.0104 - val_loss: 8.0983e-04 - val_mae: 0.0170\n",
      "Epoch 13/20\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - loss: 2.4342e-04 - mae: 0.0104 - val_loss: 8.3257e-04 - val_mae: 0.0183\n",
      "Epoch 14/20\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 2.5398e-04 - mae: 0.0103 - val_loss: 7.7458e-04 - val_mae: 0.0164\n",
      "Epoch 15/20\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 26ms/step - loss: 2.8310e-04 - mae: 0.0103 - val_loss: 7.6684e-04 - val_mae: 0.0162\n",
      "Epoch 16/20\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 2.6510e-04 - mae: 0.0105 - val_loss: 8.2553e-04 - val_mae: 0.0187\n",
      "Epoch 17/20\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - loss: 3.0727e-04 - mae: 0.0106 - val_loss: 8.9781e-04 - val_mae: 0.0196\n",
      "Epoch 18/20\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - loss: 2.5060e-04 - mae: 0.0098 - val_loss: 6.9915e-04 - val_mae: 0.0132\n",
      "Epoch 19/20\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 26ms/step - loss: 2.4632e-04 - mae: 0.0104 - val_loss: 7.0961e-04 - val_mae: 0.0133\n",
      "Epoch 20/20\n",
      "\u001b[1m224/224\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 32ms/step - loss: 2.5441e-04 - mae: 0.0100 - val_loss: 8.8822e-04 - val_mae: 0.0190\n",
      "\u001b[1m28/28\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
      "âœ… Done with capsicum_Davangere_daily.csv | MAE=300.1, RMSE=470.89, R2=0.93, MAPE=5.53%, Accuracy=94.47%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ Processing: capsicum_Dharwad_daily.csv\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\1911045302.py:54: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df['Date'] = pd.to_datetime(df['Date'], errors='coerce', infer_datetime_format=True)\n",
      "C:\\Users\\ravik\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 85ms/step - loss: 0.1317 - mae: 0.2859 - val_loss: 0.0132 - val_mae: 0.0915\n",
      "Epoch 2/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0717 - mae: 0.2221 - val_loss: 0.0511 - val_mae: 0.2126\n",
      "Epoch 3/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0742 - mae: 0.2266 - val_loss: 0.1194 - val_mae: 0.3373\n",
      "Epoch 4/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0720 - mae: 0.2246 - val_loss: 0.0719 - val_mae: 0.2563\n",
      "Epoch 5/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0677 - mae: 0.2230 - val_loss: 0.0310 - val_mae: 0.1555\n",
      "Epoch 6/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0682 - mae: 0.2216 - val_loss: 0.0785 - val_mae: 0.2681\n",
      "Epoch 7/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0767 - mae: 0.2308 - val_loss: 0.0682 - val_mae: 0.2474\n",
      "Epoch 8/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0665 - mae: 0.2181 - val_loss: 0.0500 - val_mae: 0.2058\n",
      "Epoch 9/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.0747 - mae: 0.2331 - val_loss: 0.0551 - val_mae: 0.2176\n",
      "Epoch 10/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0777 - mae: 0.2383 - val_loss: 0.0408 - val_mae: 0.1803\n",
      "Epoch 11/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0703 - mae: 0.2227 - val_loss: 0.0621 - val_mae: 0.2321\n",
      "Epoch 12/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0750 - mae: 0.2242 - val_loss: 0.0490 - val_mae: 0.2005\n",
      "Epoch 13/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0666 - mae: 0.2166 - val_loss: 0.0680 - val_mae: 0.2427\n",
      "Epoch 14/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0675 - mae: 0.2136 - val_loss: 0.0327 - val_mae: 0.1501\n",
      "Epoch 15/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0677 - mae: 0.2184 - val_loss: 0.0646 - val_mae: 0.2337\n",
      "Epoch 16/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.0746 - mae: 0.2261 - val_loss: 0.0331 - val_mae: 0.1479\n",
      "Epoch 17/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0597 - mae: 0.2086 - val_loss: 0.0351 - val_mae: 0.1526\n",
      "Epoch 18/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0796 - mae: 0.2322 - val_loss: 0.0333 - val_mae: 0.1455\n",
      "Epoch 19/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0570 - mae: 0.2029 - val_loss: 0.0246 - val_mae: 0.1176\n",
      "Epoch 20/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0752 - mae: 0.2317 - val_loss: 0.0430 - val_mae: 0.1731\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 445ms/step\n",
      "âœ… Done with capsicum_Dharwad_daily.csv | MAE=649.04, RMSE=777.48, R2=-8.2, MAPE=15.29%, Accuracy=84.71%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ Processing: capsicum_Hassan_daily.csv\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\1911045302.py:54: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df['Date'] = pd.to_datetime(df['Date'], errors='coerce', infer_datetime_format=True)\n",
      "C:\\Users\\ravik\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 20ms/step - loss: 0.0077 - mae: 0.0537 - val_loss: 0.0223 - val_mae: 0.1207\n",
      "Epoch 2/20\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 19ms/step - loss: 0.0011 - mae: 0.0237 - val_loss: 0.0224 - val_mae: 0.1204\n",
      "Epoch 3/20\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - loss: 8.9775e-04 - mae: 0.0205 - val_loss: 0.0223 - val_mae: 0.1189\n",
      "Epoch 4/20\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - loss: 8.1397e-04 - mae: 0.0189 - val_loss: 0.0224 - val_mae: 0.1192\n",
      "Epoch 5/20\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 20ms/step - loss: 6.5297e-04 - mae: 0.0171 - val_loss: 0.0225 - val_mae: 0.1215\n",
      "Epoch 6/20\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - loss: 6.3766e-04 - mae: 0.0169 - val_loss: 0.0220 - val_mae: 0.1181\n",
      "Epoch 7/20\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - loss: 5.7534e-04 - mae: 0.0163 - val_loss: 0.0220 - val_mae: 0.1195\n",
      "Epoch 8/20\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 19ms/step - loss: 5.4858e-04 - mae: 0.0156 - val_loss: 0.0214 - val_mae: 0.1163\n",
      "Epoch 9/20\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 20ms/step - loss: 6.0849e-04 - mae: 0.0171 - val_loss: 0.0213 - val_mae: 0.1167\n",
      "Epoch 10/20\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 20ms/step - loss: 5.1893e-04 - mae: 0.0147 - val_loss: 0.0212 - val_mae: 0.1171\n",
      "Epoch 11/20\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - loss: 5.5013e-04 - mae: 0.0161 - val_loss: 0.0205 - val_mae: 0.1159\n",
      "Epoch 12/20\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 20ms/step - loss: 5.1670e-04 - mae: 0.0153 - val_loss: 0.0202 - val_mae: 0.1132\n",
      "Epoch 13/20\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 20ms/step - loss: 4.6824e-04 - mae: 0.0142 - val_loss: 0.0203 - val_mae: 0.1152\n",
      "Epoch 14/20\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 23ms/step - loss: 5.2251e-04 - mae: 0.0156 - val_loss: 0.0200 - val_mae: 0.1123\n",
      "Epoch 15/20\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 27ms/step - loss: 4.6226e-04 - mae: 0.0144 - val_loss: 0.0193 - val_mae: 0.1106\n",
      "Epoch 16/20\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - loss: 4.8833e-04 - mae: 0.0149 - val_loss: 0.0192 - val_mae: 0.1102\n",
      "Epoch 17/20\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - loss: 4.5945e-04 - mae: 0.0145 - val_loss: 0.0188 - val_mae: 0.1090\n",
      "Epoch 18/20\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 23ms/step - loss: 4.4558e-04 - mae: 0.0140 - val_loss: 0.0185 - val_mae: 0.1090\n",
      "Epoch 19/20\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 23ms/step - loss: 4.0850e-04 - mae: 0.0132 - val_loss: 0.0182 - val_mae: 0.1071\n",
      "Epoch 20/20\n",
      "\u001b[1m347/347\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 24ms/step - loss: 4.4081e-04 - mae: 0.0143 - val_loss: 0.0181 - val_mae: 0.1066\n",
      "\u001b[1m44/44\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "âœ… Done with capsicum_Hassan_daily.csv | MAE=657.47, RMSE=828.6, R2=0.42, MAPE=19.96%, Accuracy=80.04%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ Processing: capsicum_Kalburgi_daily.csv\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\1911045302.py:54: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df['Date'] = pd.to_datetime(df['Date'], errors='coerce', infer_datetime_format=True)\n",
      "C:\\Users\\ravik\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 27ms/step - loss: 0.0460 - mae: 0.1625 - val_loss: 0.0044 - val_mae: 0.0496\n",
      "Epoch 2/20\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0058 - mae: 0.0546 - val_loss: 0.0030 - val_mae: 0.0395\n",
      "Epoch 3/20\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0060 - mae: 0.0546 - val_loss: 0.0020 - val_mae: 0.0289\n",
      "Epoch 4/20\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0050 - mae: 0.0515 - val_loss: 0.0025 - val_mae: 0.0371\n",
      "Epoch 5/20\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0054 - mae: 0.0520 - val_loss: 0.0025 - val_mae: 0.0356\n",
      "Epoch 6/20\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0056 - mae: 0.0537 - val_loss: 0.0015 - val_mae: 0.0235\n",
      "Epoch 7/20\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0053 - mae: 0.0493 - val_loss: 0.0015 - val_mae: 0.0246\n",
      "Epoch 8/20\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0047 - mae: 0.0483 - val_loss: 0.0016 - val_mae: 0.0263\n",
      "Epoch 9/20\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0042 - mae: 0.0458 - val_loss: 0.0025 - val_mae: 0.0395\n",
      "Epoch 10/20\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0043 - mae: 0.0439 - val_loss: 0.0014 - val_mae: 0.0293\n",
      "Epoch 11/20\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0047 - mae: 0.0471 - val_loss: 0.0011 - val_mae: 0.0198\n",
      "Epoch 12/20\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0045 - mae: 0.0466 - val_loss: 0.0011 - val_mae: 0.0221\n",
      "Epoch 13/20\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0045 - mae: 0.0464 - val_loss: 0.0032 - val_mae: 0.0479\n",
      "Epoch 14/20\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0042 - mae: 0.0439 - val_loss: 0.0023 - val_mae: 0.0381\n",
      "Epoch 15/20\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0042 - mae: 0.0438 - val_loss: 0.0016 - val_mae: 0.0307\n",
      "Epoch 16/20\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0047 - mae: 0.0477 - val_loss: 0.0020 - val_mae: 0.0346\n",
      "Epoch 17/20\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0041 - mae: 0.0434 - val_loss: 0.0015 - val_mae: 0.0280\n",
      "Epoch 18/20\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0051 - mae: 0.0492 - val_loss: 0.0014 - val_mae: 0.0267\n",
      "Epoch 19/20\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0046 - mae: 0.0447 - val_loss: 0.0016 - val_mae: 0.0314\n",
      "Epoch 20/20\n",
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0037 - mae: 0.0411 - val_loss: 0.0020 - val_mae: 0.0349\n",
      "\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step\n",
      "âœ… Done with capsicum_Kalburgi_daily.csv | MAE=241.18, RMSE=310.08, R2=0.85, MAPE=6.27%, Accuracy=93.73%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ Processing: capsicum_Kolar_daily.csv\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\1911045302.py:54: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df['Date'] = pd.to_datetime(df['Date'], errors='coerce', infer_datetime_format=True)\n",
      "C:\\Users\\ravik\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 22ms/step - loss: 0.0081 - mae: 0.0637 - val_loss: 0.0018 - val_mae: 0.0319\n",
      "Epoch 2/20\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 23ms/step - loss: 0.0065 - mae: 0.0562 - val_loss: 0.0017 - val_mae: 0.0309\n",
      "Epoch 3/20\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 22ms/step - loss: 0.0062 - mae: 0.0548 - val_loss: 0.0021 - val_mae: 0.0346\n",
      "Epoch 4/20\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 24ms/step - loss: 0.0062 - mae: 0.0548 - val_loss: 0.0020 - val_mae: 0.0343\n",
      "Epoch 5/20\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 21ms/step - loss: 0.0061 - mae: 0.0547 - val_loss: 0.0017 - val_mae: 0.0309\n",
      "Epoch 6/20\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 21ms/step - loss: 0.0063 - mae: 0.0547 - val_loss: 0.0018 - val_mae: 0.0317\n",
      "Epoch 7/20\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 23ms/step - loss: 0.0061 - mae: 0.0537 - val_loss: 0.0019 - val_mae: 0.0327\n",
      "Epoch 8/20\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 23ms/step - loss: 0.0058 - mae: 0.0525 - val_loss: 0.0018 - val_mae: 0.0319\n",
      "Epoch 9/20\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 22ms/step - loss: 0.0061 - mae: 0.0544 - val_loss: 0.0018 - val_mae: 0.0320\n",
      "Epoch 10/20\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 22ms/step - loss: 0.0059 - mae: 0.0532 - val_loss: 0.0018 - val_mae: 0.0312\n",
      "Epoch 11/20\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 23ms/step - loss: 0.0059 - mae: 0.0534 - val_loss: 0.0019 - val_mae: 0.0323\n",
      "Epoch 12/20\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 25ms/step - loss: 0.0060 - mae: 0.0537 - val_loss: 0.0020 - val_mae: 0.0333\n",
      "Epoch 13/20\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 27ms/step - loss: 0.0059 - mae: 0.0531 - val_loss: 0.0018 - val_mae: 0.0317\n",
      "Epoch 14/20\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 24ms/step - loss: 0.0060 - mae: 0.0531 - val_loss: 0.0018 - val_mae: 0.0318\n",
      "Epoch 15/20\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 24ms/step - loss: 0.0059 - mae: 0.0531 - val_loss: 0.0019 - val_mae: 0.0325\n",
      "Epoch 16/20\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 25ms/step - loss: 0.0059 - mae: 0.0528 - val_loss: 0.0018 - val_mae: 0.0312\n",
      "Epoch 17/20\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 25ms/step - loss: 0.0057 - mae: 0.0518 - val_loss: 0.0018 - val_mae: 0.0322\n",
      "Epoch 18/20\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 25ms/step - loss: 0.0056 - mae: 0.0518 - val_loss: 0.0017 - val_mae: 0.0311\n",
      "Epoch 19/20\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 24ms/step - loss: 0.0058 - mae: 0.0524 - val_loss: 0.0017 - val_mae: 0.0306\n",
      "Epoch 20/20\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 24ms/step - loss: 0.0057 - mae: 0.0518 - val_loss: 0.0018 - val_mae: 0.0314\n",
      "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Done with capsicum_Kolar_daily.csv | MAE=563.92, RMSE=753.79, R2=0.31, MAPE=26.24%, Accuracy=73.76%\n",
      "\n",
      "ğŸš€ Processing: capsicum_Mandya_daily.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\1911045302.py:54: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df['Date'] = pd.to_datetime(df['Date'], errors='coerce', infer_datetime_format=True)\n",
      "C:\\Users\\ravik\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 28ms/step - loss: 0.0070 - mae: 0.0470 - val_loss: 0.0062 - val_mae: 0.0578\n",
      "Epoch 2/20\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - loss: 0.0046 - mae: 0.0368 - val_loss: 0.0079 - val_mae: 0.0719\n",
      "Epoch 3/20\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 25ms/step - loss: 0.0041 - mae: 0.0358 - val_loss: 0.0057 - val_mae: 0.0585\n",
      "Epoch 4/20\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - loss: 0.0040 - mae: 0.0328 - val_loss: 0.0055 - val_mae: 0.0550\n",
      "Epoch 5/20\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - loss: 0.0034 - mae: 0.0309 - val_loss: 0.0053 - val_mae: 0.0518\n",
      "Epoch 6/20\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - loss: 0.0037 - mae: 0.0311 - val_loss: 0.0053 - val_mae: 0.0535\n",
      "Epoch 7/20\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 19ms/step - loss: 0.0029 - mae: 0.0274 - val_loss: 0.0050 - val_mae: 0.0467\n",
      "Epoch 8/20\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 25ms/step - loss: 0.0032 - mae: 0.0283 - val_loss: 0.0052 - val_mae: 0.0531\n",
      "Epoch 9/20\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - loss: 0.0028 - mae: 0.0271 - val_loss: 0.0056 - val_mae: 0.0572\n",
      "Epoch 10/20\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - loss: 0.0032 - mae: 0.0286 - val_loss: 0.0057 - val_mae: 0.0557\n",
      "Epoch 11/20\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - loss: 0.0031 - mae: 0.0271 - val_loss: 0.0055 - val_mae: 0.0540\n",
      "Epoch 12/20\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - loss: 0.0027 - mae: 0.0261 - val_loss: 0.0072 - val_mae: 0.0667\n",
      "Epoch 13/20\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - loss: 0.0034 - mae: 0.0300 - val_loss: 0.0076 - val_mae: 0.0706\n",
      "Epoch 14/20\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 20ms/step - loss: 0.0031 - mae: 0.0288 - val_loss: 0.0055 - val_mae: 0.0516\n",
      "Epoch 15/20\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - loss: 0.0033 - mae: 0.0285 - val_loss: 0.0074 - val_mae: 0.0685\n",
      "Epoch 16/20\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - loss: 0.0033 - mae: 0.0288 - val_loss: 0.0051 - val_mae: 0.0449\n",
      "Epoch 17/20\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 20ms/step - loss: 0.0033 - mae: 0.0277 - val_loss: 0.0050 - val_mae: 0.0483\n",
      "Epoch 18/20\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 21ms/step - loss: 0.0034 - mae: 0.0286 - val_loss: 0.0051 - val_mae: 0.0449\n",
      "Epoch 19/20\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - loss: 0.0032 - mae: 0.0272 - val_loss: 0.0057 - val_mae: 0.0546\n",
      "Epoch 20/20\n",
      "\u001b[1m284/284\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - loss: 0.0026 - mae: 0.0254 - val_loss: 0.0052 - val_mae: 0.0479\n",
      "\u001b[1m36/36\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step\n",
      "âœ… Done with capsicum_Mandya_daily.csv | MAE=270.56, RMSE=407.21, R2=0.59, MAPE=9.52%, Accuracy=90.48%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ Processing: capsicum_Mysore_daily.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\1911045302.py:54: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df['Date'] = pd.to_datetime(df['Date'], errors='coerce', infer_datetime_format=True)\n",
      "C:\\Users\\ravik\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - loss: 0.0015 - mae: 0.0276 - val_loss: 0.0018 - val_mae: 0.0293\n",
      "Epoch 2/20\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 18ms/step - loss: 0.0010 - mae: 0.0227 - val_loss: 0.0020 - val_mae: 0.0300\n",
      "Epoch 3/20\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 18ms/step - loss: 0.0010 - mae: 0.0222 - val_loss: 0.0019 - val_mae: 0.0291\n",
      "Epoch 4/20\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 18ms/step - loss: 9.8029e-04 - mae: 0.0216 - val_loss: 0.0021 - val_mae: 0.0317\n",
      "Epoch 5/20\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 21ms/step - loss: 9.4178e-04 - mae: 0.0215 - val_loss: 0.0019 - val_mae: 0.0296\n",
      "Epoch 6/20\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 18ms/step - loss: 9.3967e-04 - mae: 0.0214 - val_loss: 0.0020 - val_mae: 0.0295\n",
      "Epoch 7/20\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - loss: 9.4111e-04 - mae: 0.0211 - val_loss: 0.0023 - val_mae: 0.0321\n",
      "Epoch 8/20\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 18ms/step - loss: 8.8128e-04 - mae: 0.0205 - val_loss: 0.0020 - val_mae: 0.0293\n",
      "Epoch 9/20\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 24ms/step - loss: 8.8749e-04 - mae: 0.0206 - val_loss: 0.0021 - val_mae: 0.0300\n",
      "Epoch 10/20\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 28ms/step - loss: 8.9104e-04 - mae: 0.0204 - val_loss: 0.0019 - val_mae: 0.0293\n",
      "Epoch 11/20\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 25ms/step - loss: 8.7786e-04 - mae: 0.0203 - val_loss: 0.0020 - val_mae: 0.0295\n",
      "Epoch 12/20\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 23ms/step - loss: 8.8531e-04 - mae: 0.0206 - val_loss: 0.0019 - val_mae: 0.0294\n",
      "Epoch 13/20\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 32ms/step - loss: 8.3785e-04 - mae: 0.0202 - val_loss: 0.0021 - val_mae: 0.0297\n",
      "Epoch 14/20\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 22ms/step - loss: 8.2518e-04 - mae: 0.0199 - val_loss: 0.0021 - val_mae: 0.0304\n",
      "Epoch 15/20\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 30ms/step - loss: 8.5811e-04 - mae: 0.0201 - val_loss: 0.0020 - val_mae: 0.0302\n",
      "Epoch 16/20\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 25ms/step - loss: 8.2790e-04 - mae: 0.0200 - val_loss: 0.0020 - val_mae: 0.0293\n",
      "Epoch 17/20\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 26ms/step - loss: 8.9699e-04 - mae: 0.0208 - val_loss: 0.0022 - val_mae: 0.0299\n",
      "Epoch 18/20\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 26ms/step - loss: 8.5098e-04 - mae: 0.0198 - val_loss: 0.0020 - val_mae: 0.0296\n",
      "Epoch 19/20\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 32ms/step - loss: 8.7980e-04 - mae: 0.0201 - val_loss: 0.0020 - val_mae: 0.0294\n",
      "Epoch 20/20\n",
      "\u001b[1m510/510\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 26ms/step - loss: 8.6813e-04 - mae: 0.0198 - val_loss: 0.0022 - val_mae: 0.0302\n",
      "\u001b[1m64/64\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step\n",
      "âœ… Done with capsicum_Mysore_daily.csv | MAE=718.79, RMSE=1104.44, R2=0.17, MAPE=21.83%, Accuracy=78.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ Processing: capsicum_Shimoga_daily.csv\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\1911045302.py:54: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df['Date'] = pd.to_datetime(df['Date'], errors='coerce', infer_datetime_format=True)\n",
      "C:\\Users\\ravik\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 208ms/step - loss: 0.3683 - mae: 0.5778 - val_loss: 0.5260 - val_mae: 0.7243\n",
      "Epoch 2/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 0.1729 - mae: 0.3807 - val_loss: 0.1674 - val_mae: 0.4082\n",
      "Epoch 3/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.0336 - mae: 0.1496 - val_loss: 0.0047 - val_mae: 0.0667\n",
      "Epoch 4/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.0204 - mae: 0.1193 - val_loss: 0.0091 - val_mae: 0.0950\n",
      "Epoch 5/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0357 - mae: 0.1695 - val_loss: 3.9023e-04 - val_mae: 0.0162\n",
      "Epoch 6/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.0122 - mae: 0.0955 - val_loss: 0.0206 - val_mae: 0.1425\n",
      "Epoch 7/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0089 - mae: 0.0780 - val_loss: 0.0439 - val_mae: 0.2087\n",
      "Epoch 8/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0129 - mae: 0.0878 - val_loss: 0.0471 - val_mae: 0.2163\n",
      "Epoch 9/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.0125 - mae: 0.0904 - val_loss: 0.0335 - val_mae: 0.1821\n",
      "Epoch 10/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.0117 - mae: 0.0878 - val_loss: 0.0157 - val_mae: 0.1243\n",
      "Epoch 11/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 0.0093 - mae: 0.0796 - val_loss: 0.0060 - val_mae: 0.0765\n",
      "Epoch 12/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.0077 - mae: 0.0708 - val_loss: 0.0037 - val_mae: 0.0594\n",
      "Epoch 13/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.0073 - mae: 0.0701 - val_loss: 0.0043 - val_mae: 0.0646\n",
      "Epoch 14/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.0088 - mae: 0.0777 - val_loss: 0.0077 - val_mae: 0.0871\n",
      "Epoch 15/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 0.0079 - mae: 0.0687 - val_loss: 0.0112 - val_mae: 0.1051\n",
      "Epoch 16/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.0092 - mae: 0.0759 - val_loss: 0.0116 - val_mae: 0.1071\n",
      "Epoch 17/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.0079 - mae: 0.0668 - val_loss: 0.0099 - val_mae: 0.0989\n",
      "Epoch 18/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 0.0059 - mae: 0.0618 - val_loss: 0.0069 - val_mae: 0.0825\n",
      "Epoch 19/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 0.0059 - mae: 0.0637 - val_loss: 0.0057 - val_mae: 0.0748\n",
      "Epoch 20/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.0047 - mae: 0.0555 - val_loss: 0.0064 - val_mae: 0.0795\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 613ms/step\n",
      "âœ… Done with capsicum_Shimoga_daily.csv | MAE=51.65, RMSE=52.03, R2=-2.3, MAPE=2.78%, Accuracy=97.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ Processing: capsicum_Udupi_daily.csv\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20680\\1911045302.py:54: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df['Date'] = pd.to_datetime(df['Date'], errors='coerce', infer_datetime_format=True)\n",
      "C:\\Users\\ravik\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 15ms/step - loss: 0.0080 - mae: 0.0523 - val_loss: 0.0034 - val_mae: 0.0326\n",
      "Epoch 2/20\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0012 - mae: 0.0220 - val_loss: 0.0029 - val_mae: 0.0289\n",
      "Epoch 3/20\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 9.3509e-04 - mae: 0.0192 - val_loss: 0.0024 - val_mae: 0.0226\n",
      "Epoch 4/20\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 8.4787e-04 - mae: 0.0181 - val_loss: 0.0022 - val_mae: 0.0215\n",
      "Epoch 5/20\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 6.9555e-04 - mae: 0.0166 - val_loss: 0.0022 - val_mae: 0.0288\n",
      "Epoch 6/20\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 6.7212e-04 - mae: 0.0165 - val_loss: 0.0021 - val_mae: 0.0280\n",
      "Epoch 7/20\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - loss: 6.2960e-04 - mae: 0.0154 - val_loss: 0.0016 - val_mae: 0.0178\n",
      "Epoch 8/20\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 5.5326e-04 - mae: 0.0146 - val_loss: 0.0018 - val_mae: 0.0240\n",
      "Epoch 9/20\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 5.2844e-04 - mae: 0.0146 - val_loss: 0.0017 - val_mae: 0.0226\n",
      "Epoch 10/20\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 4.9808e-04 - mae: 0.0144 - val_loss: 0.0015 - val_mae: 0.0166\n",
      "Epoch 11/20\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 5.4254e-04 - mae: 0.0144 - val_loss: 0.0017 - val_mae: 0.0232\n",
      "Epoch 12/20\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - loss: 4.1901e-04 - mae: 0.0133 - val_loss: 0.0015 - val_mae: 0.0149\n",
      "Epoch 13/20\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 4.9532e-04 - mae: 0.0142 - val_loss: 0.0015 - val_mae: 0.0131\n",
      "Epoch 14/20\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 4.7009e-04 - mae: 0.0141 - val_loss: 0.0015 - val_mae: 0.0127\n",
      "Epoch 15/20\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 3.9507e-04 - mae: 0.0129 - val_loss: 0.0014 - val_mae: 0.0138\n",
      "Epoch 16/20\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 4.7271e-04 - mae: 0.0139 - val_loss: 0.0014 - val_mae: 0.0151\n",
      "Epoch 17/20\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 4.6158e-04 - mae: 0.0142 - val_loss: 0.0014 - val_mae: 0.0136\n",
      "Epoch 18/20\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 4.3444e-04 - mae: 0.0137 - val_loss: 0.0016 - val_mae: 0.0195\n",
      "Epoch 19/20\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 4.9271e-04 - mae: 0.0141 - val_loss: 0.0016 - val_mae: 0.0212\n",
      "Epoch 20/20\n",
      "\u001b[1m247/247\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 3.8231e-04 - mae: 0.0138 - val_loss: 0.0014 - val_mae: 0.0155\n",
      "\u001b[1m31/31\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
      "âœ… Done with capsicum_Udupi_daily.csv | MAE=136.2, RMSE=334.67, R2=0.93, MAPE=2.71%, Accuracy=97.29%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Metrics saved to gru_output_csv\\gru_metrics.csv\n",
      "\n",
      "âœ… All districts processed successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import joblib\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout\n",
    "\n",
    "# -----------------------------\n",
    "# Output paths\n",
    "# -----------------------------\n",
    "input_folder = \"dataset\"\n",
    "output_models = \"gru_output_models\"\n",
    "output_csv = \"gru_output_csv\"\n",
    "output_graphs = \"gru_output_graphs\"\n",
    "output_logs = \"gru_output_logs\"\n",
    "metrics_file = os.path.join(output_csv, \"gru_metrics.csv\")\n",
    "\n",
    "os.makedirs(output_models, exist_ok=True)\n",
    "os.makedirs(output_csv, exist_ok=True)\n",
    "os.makedirs(output_graphs, exist_ok=True)\n",
    "os.makedirs(output_logs, exist_ok=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Function to create dataset\n",
    "# -----------------------------\n",
    "def create_dataset(data, look_back=30):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - look_back):\n",
    "        X.append(data[i:i + look_back, 0])\n",
    "        y.append(data[i + look_back, 0])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# -----------------------------\n",
    "# Metrics storage\n",
    "# -----------------------------\n",
    "metrics_list = []\n",
    "\n",
    "# -----------------------------\n",
    "# Process each CSV file\n",
    "# -----------------------------\n",
    "look_back = 30\n",
    "for file in os.listdir(input_folder):\n",
    "    if file.endswith(\".csv\"):\n",
    "        district_name = file.split(\".\")[0]\n",
    "        print(f\"\\nğŸš€ Processing: {file}\")\n",
    "\n",
    "        # Load dataset\n",
    "        df = pd.read_csv(os.path.join(input_folder, file))\n",
    "\n",
    "        # Robust date parsing\n",
    "        df['Date'] = pd.to_datetime(df['Date'], errors='coerce', infer_datetime_format=True)\n",
    "        df = df.dropna(subset=['Date'])\n",
    "        df = df.sort_values('Date')\n",
    "\n",
    "        # Handle missing values\n",
    "        df['Average Price'] = df['Average Price'].fillna(df['Average Price'].mean())\n",
    "\n",
    "        # Add moving averages\n",
    "        df['MA_7'] = df['Average Price'].rolling(window=7).mean().fillna(df['Average Price'].mean())\n",
    "        df['MA_30'] = df['Average Price'].rolling(window=30).mean().fillna(df['Average Price'].mean())\n",
    "\n",
    "        # Scale data\n",
    "        values = df[['Average Price']].values\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        scaled_values = scaler.fit_transform(values)\n",
    "\n",
    "        X, y = create_dataset(scaled_values, look_back)\n",
    "        X = X.reshape((X.shape[0], X.shape[1], 1))\n",
    "\n",
    "        # Train-test split\n",
    "        split = int(len(X) * 0.8)\n",
    "        X_train, X_test = X[:split], X[split:]\n",
    "        y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "        # Build GRU model\n",
    "        model = Sequential([\n",
    "            GRU(64, return_sequences=True, input_shape=(look_back, 1)),\n",
    "            Dropout(0.2),\n",
    "            GRU(32),\n",
    "            Dropout(0.2),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        model.compile(optimizer=\"adam\", loss=\"mse\", metrics=['mae'])\n",
    "\n",
    "        # Train model\n",
    "        history = model.fit(X_train, y_train, epochs=20, batch_size=16,\n",
    "                            validation_data=(X_test, y_test), verbose=1)\n",
    "\n",
    "        # Save training log\n",
    "        log_file = os.path.join(output_logs, f\"{district_name}_gru_training.txt\")\n",
    "        with open(log_file, \"w\") as f:\n",
    "            f.write(\"Epoch\\tTrain_Loss\\tVal_Loss\\n\")\n",
    "            for i, (loss, val_loss) in enumerate(zip(history.history['loss'], history.history['val_loss'])):\n",
    "                f.write(f\"{i+1}\\t{loss:.6f}\\t{val_loss:.6f}\\n\")\n",
    "\n",
    "        # Predictions\n",
    "        y_pred_scaled = model.predict(X_test)\n",
    "        y_pred = scaler.inverse_transform(y_pred_scaled)\n",
    "        y_true = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "        # Round predictions\n",
    "        y_true_round = np.round(y_true, 2)\n",
    "        y_pred_round = np.round(y_pred, 2)\n",
    "\n",
    "        # Save predictions CSV (only Date, Actual, Predicted)\n",
    "        df_pred = df.iloc[-len(y_true):].copy()\n",
    "        df_pred = df_pred[['Date']].reset_index(drop=True)\n",
    "        df_pred['Actual'] = y_true_round.flatten()\n",
    "        df_pred['Predicted'] = y_pred_round.flatten()\n",
    "        df_pred.to_csv(os.path.join(output_csv, f\"{district_name}_gru_updated.csv\"), index=False)\n",
    "\n",
    "        # Metrics\n",
    "        mae_val = round(mean_absolute_error(y_true, y_pred), 2)\n",
    "        rmse_val = round(mean_squared_error(y_true, y_pred, squared=False), 2)\n",
    "        r2_val = round(r2_score(y_true, y_pred), 2)\n",
    "        mape_val = round(np.mean(np.abs((y_true - y_pred) / y_true)) * 100, 2)\n",
    "        accuracy_val = round(100 - mape_val, 2)\n",
    "        metrics_list.append([district_name, mae_val, rmse_val, r2_val, mape_val, accuracy_val])\n",
    "\n",
    "        print(f\"âœ… Done with {file} | MAE={mae_val}, RMSE={rmse_val}, R2={r2_val}, \"\n",
    "              f\"MAPE={mape_val}%, Accuracy={accuracy_val}%\")\n",
    "\n",
    "        # Save model as .pkl\n",
    "        model_file = os.path.join(output_models, f\"{district_name}_gru_model.pkl\")\n",
    "        joblib.dump(model, model_file)\n",
    "\n",
    "        # Save prediction graph (with MA7 & MA30)\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(df['Date'], df['Average Price'], label=\"Actual\", color=\"blue\")\n",
    "        plt.plot(df['Date'], df['MA_7'], label=\"MA_7\", color=\"orange\")\n",
    "        plt.plot(df['Date'], df['MA_30'], label=\"MA_30\", color=\"green\")\n",
    "        plt.plot(df_pred['Date'], df_pred['Predicted'], label=\"Predicted (GRU)\", color=\"red\", linestyle=\"dashed\")\n",
    "        plt.xlabel(\"Date\")\n",
    "        plt.ylabel(\"Average Price\")\n",
    "        plt.title(f\"GRU Predictions - {district_name}\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig(os.path.join(output_graphs, f\"{district_name}_gru_graph.png\"))\n",
    "        plt.close()\n",
    "\n",
    "        # Training loss graph\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        plt.plot(history.history['loss'], label='Train Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "        plt.title(f\"GRU Training Loss - {district_name}\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "        plt.savefig(os.path.join(output_graphs, f\"{district_name}_gru_loss.png\"))\n",
    "        plt.close()\n",
    "\n",
    "# Save metrics CSV\n",
    "metrics_df = pd.DataFrame(metrics_list, columns=['District', 'MAE', 'RMSE', 'R2', 'MAPE(%)', 'Accuracy(%)'])\n",
    "metrics_df.to_csv(metrics_file, index=False)\n",
    "print(\"\\nğŸ“Š Metrics saved to\", metrics_file)\n",
    "print(\"\\nâœ… All districts processed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00fd371-9865-4dcb-9a60-58f8127c4068",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
