{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e4ade2-bf8d-4951-bde2-da47717bf7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0debe30-0cfa-4e14-aa59-4b4e5a640e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: prophet in c:\\users\\ravik\\appdata\\roaming\\python\\python313\\site-packages (1.2.1)\n",
      "Requirement already satisfied: cmdstanpy>=1.0.4 in c:\\users\\ravik\\appdata\\roaming\\python\\python313\\site-packages (from prophet) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.15.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from prophet) (2.1.3)\n",
      "Requirement already satisfied: matplotlib>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from prophet) (3.10.0)\n",
      "Requirement already satisfied: pandas>=1.0.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from prophet) (2.2.3)\n",
      "Requirement already satisfied: holidays<1,>=0.25 in c:\\users\\ravik\\appdata\\roaming\\python\\python313\\site-packages (from prophet) (0.85)\n",
      "Requirement already satisfied: tqdm>=4.36.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from prophet) (4.67.1)\n",
      "Requirement already satisfied: importlib_resources in c:\\users\\ravik\\appdata\\roaming\\python\\python313\\site-packages (from prophet) (6.5.2)\n",
      "Requirement already satisfied: python-dateutil in c:\\programdata\\anaconda3\\lib\\site-packages (from holidays<1,>=0.25->prophet) (2.9.0.post0)\n",
      "Requirement already satisfied: stanio<2.0.0,>=0.4.0 in c:\\users\\ravik\\appdata\\roaming\\python\\python313\\site-packages (from cmdstanpy>=1.0.4->prophet) (0.5.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=2.0.0->prophet) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=2.0.0->prophet) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=2.0.0->prophet) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=2.0.0->prophet) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=2.0.0->prophet) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=2.0.0->prophet) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=2.0.0->prophet) (3.2.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=1.0.4->prophet) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=1.0.4->prophet) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil->holidays<1,>=0.25->prophet) (1.17.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.36.1->prophet) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04b435d3-c33b-47f8-8dd3-a9f0e876745b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ Processing: capsicum_2010_2024_master_weekly.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22:18:54 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:18:55 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… capsicum_2010_2024_master_weekly.csv: MAE=100.03, RMSE=137.56, RÂ²=0.9644, MAPE=3.95%, Accuracy=96.05%\n",
      "\n",
      "ğŸš€ Processing: onion_2010_2024_master_weekly.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22:18:56 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:18:57 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… onion_2010_2024_master_weekly.csv: MAE=41.11, RMSE=68.63, RÂ²=0.9896, MAPE=2.31%, Accuracy=97.69%\n",
      "\n",
      "ğŸš€ Processing: tomato_2010_2024_master_weekly.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22:18:57 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:18:58 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… tomato_2010_2024_master_weekly.csv: MAE=82.5, RMSE=136.61, RÂ²=0.9685, MAPE=6.16%, Accuracy=93.84%\n",
      "\n",
      "ğŸš€ Processing: wheat_2010_2024_master_weekly.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22:18:59 - cmdstanpy - INFO - Chain [1] start processing\n",
      "22:18:59 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… wheat_2010_2024_master_weekly.csv: MAE=13.84, RMSE=22.69, RÂ²=0.998, MAPE=0.62%, Accuracy=99.38%\n",
      "\n",
      "ğŸ“Š All Prophet weekly metrics saved successfully!\n",
      "                               Crop     MAE    RMSE    R2  MAPE(%)  \\\n",
      "0  capsicum_2010_2024_master_weekly  100.03  137.56  0.96     3.95   \n",
      "1     onion_2010_2024_master_weekly   41.11   68.63  0.99     2.31   \n",
      "2    tomato_2010_2024_master_weekly   82.50  136.61  0.97     6.16   \n",
      "3     wheat_2010_2024_master_weekly   13.84   22.69  1.00     0.62   \n",
      "\n",
      "   Accuracy(%)  \n",
      "0        96.05  \n",
      "1        97.69  \n",
      "2        93.84  \n",
      "3        99.38  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from prophet import Prophet\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import warnings\n",
    "import gc\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ======================================================\n",
    "# Paths\n",
    "# ======================================================\n",
    "input_folder = \"dataset\"\n",
    "output_models = \"prophet_output_models_weekly\"\n",
    "output_csv = \"prophet_output_csv_weekly\"\n",
    "output_graphs = \"prophet_output_graphs_weekly\"\n",
    "metrics_file = \"prophet_metrics_weekly.csv\"\n",
    "\n",
    "os.makedirs(output_models, exist_ok=True)\n",
    "os.makedirs(output_csv, exist_ok=True)\n",
    "os.makedirs(output_graphs, exist_ok=True)\n",
    "\n",
    "# ======================================================\n",
    "# Helper Functions\n",
    "# ======================================================\n",
    "def parse_dates_safe(date_series):\n",
    "    try:\n",
    "        return pd.to_datetime(date_series, dayfirst=True)\n",
    "    except:\n",
    "        return pd.to_datetime(date_series, dayfirst=False)\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    mask = y_true != 0\n",
    "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
    "\n",
    "# ======================================================\n",
    "# Process Each Crop File\n",
    "# ======================================================\n",
    "metrics_list = []\n",
    "\n",
    "for file in sorted(os.listdir(input_folder)):\n",
    "    if not file.endswith(\".csv\"):\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nğŸš€ Processing: {file}\")\n",
    "    file_path = os.path.join(input_folder, file)\n",
    "\n",
    "    # -------------------------------------\n",
    "    # Load Data\n",
    "    # -------------------------------------\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['Date'] = parse_dates_safe(df['Date'])\n",
    "    df = df.sort_values('Date')\n",
    "    df = df.groupby('Date', as_index=False).mean(numeric_only=True)\n",
    "\n",
    "    df.set_index('Date', inplace=True)\n",
    "    df = df.asfreq('W')\n",
    "    df['Average Price'] = df['Average Price'].ffill().bfill()\n",
    "\n",
    "    # -------------------------------------\n",
    "    # Feature Engineering (Weekly)\n",
    "    # -------------------------------------\n",
    "    df['Lag_1'] = df['Average Price'].shift(1)\n",
    "    df['Lag_4'] = df['Average Price'].shift(4)\n",
    "    df['MA_4'] = df['Average Price'].rolling(window=4).mean()\n",
    "    df['MA_12'] = df['Average Price'].rolling(window=12).mean()\n",
    "    df = df.bfill()\n",
    "\n",
    "    # Prophet expects ds, y columns\n",
    "    df_prophet = df.reset_index().rename(columns={'Date': 'ds', 'Average Price': 'y'})\n",
    "\n",
    "    # Add custom regressors\n",
    "    df_prophet['Lag_1'] = df_prophet['Lag_1']\n",
    "    df_prophet['Lag_4'] = df_prophet['Lag_4']\n",
    "    df_prophet['MA_4'] = df_prophet['MA_4']\n",
    "    df_prophet['MA_12'] = df_prophet['MA_12']\n",
    "\n",
    "    # -------------------------------------\n",
    "    # Build Prophet Model with Regressors\n",
    "    # -------------------------------------\n",
    "    m = Prophet(\n",
    "        yearly_seasonality=True,\n",
    "        weekly_seasonality=False,\n",
    "        daily_seasonality=False,\n",
    "        seasonality_mode='additive',\n",
    "        changepoint_prior_scale=0.5\n",
    "    )\n",
    "\n",
    "    m.add_regressor('Lag_1')\n",
    "    m.add_regressor('Lag_4')\n",
    "    m.add_regressor('MA_4')\n",
    "    m.add_regressor('MA_12')\n",
    "\n",
    "    # Train model\n",
    "    m.fit(df_prophet)\n",
    "\n",
    "    # -------------------------------------\n",
    "    # Forecasting (Full + Future 12 weeks)\n",
    "    # -------------------------------------\n",
    "    future = m.make_future_dataframe(periods=12, freq='W')\n",
    "    future = future.merge(df_prophet[['ds', 'Lag_1', 'Lag_4', 'MA_4', 'MA_12']], on='ds', how='left')\n",
    "\n",
    "    # Fill missing future regressor values using last known\n",
    "    for col in ['Lag_1', 'Lag_4', 'MA_4', 'MA_12']:\n",
    "        future[col] = future[col].ffill().bfill()\n",
    "\n",
    "    forecast = m.predict(future)\n",
    "\n",
    "    # -------------------------------------\n",
    "    # Evaluation (on known data only)\n",
    "    # -------------------------------------\n",
    "    valid_mask = df_prophet['y'].notna()\n",
    "    y_true = df_prophet.loc[valid_mask, 'y']\n",
    "    y_pred = forecast.loc[:len(df_prophet)-1, 'yhat']\n",
    "\n",
    "    mae = round(mean_absolute_error(y_true, y_pred), 2)\n",
    "    rmse = round(np.sqrt(mean_squared_error(y_true, y_pred)), 2)\n",
    "    r2 = round(r2_score(y_true, y_pred), 4)\n",
    "    mape = round(mean_absolute_percentage_error(y_true, y_pred), 2)\n",
    "    acc = round(100 - mape, 2)\n",
    "\n",
    "    metrics_list.append({\n",
    "        'Crop': file.replace('.csv',''),\n",
    "        'MAE': mae,\n",
    "        'RMSE': rmse,\n",
    "        'R2': r2,\n",
    "        'MAPE(%)': mape,\n",
    "        'Accuracy(%)': acc\n",
    "    })\n",
    "\n",
    "    print(f\"âœ… {file}: MAE={mae}, RMSE={rmse}, RÂ²={r2}, MAPE={mape}%, Accuracy={acc}%\")\n",
    "\n",
    "    # -------------------------------------\n",
    "    # Save Outputs\n",
    "    # -------------------------------------\n",
    "    forecast_merged = forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']]\n",
    "    forecast_merged = forecast_merged.rename(columns={\n",
    "        'ds': 'Date', 'yhat': 'Predicted', 'yhat_lower': 'Lower_Bound', 'yhat_upper': 'Upper_Bound'\n",
    "    })\n",
    "    merged_df = df_prophet.merge(forecast_merged, left_on='ds', right_on='Date', how='left')\n",
    "    merged_df = merged_df[['Date', 'y', 'Predicted', 'Lower_Bound', 'Upper_Bound',\n",
    "                           'Lag_1', 'Lag_4', 'MA_4', 'MA_12']]\n",
    "    merged_df = merged_df.rename(columns={'y': 'Actual'})\n",
    "    merged_df = merged_df.round(2)\n",
    "\n",
    "    # Save updated CSV\n",
    "    merged_df.to_csv(os.path.join(output_csv, file.replace('.csv','_prophet_weekly_updated.csv')), index=False)\n",
    "\n",
    "    # -------------------------------------\n",
    "    # Save Graph\n",
    "    # -------------------------------------\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.plot(merged_df['Date'], merged_df['Actual'], label='Actual', color='blue')\n",
    "    plt.plot(merged_df['Date'], merged_df['Predicted'], label='Predicted', color='red', linestyle='dashed')\n",
    "    plt.fill_between(merged_df['Date'], merged_df['Lower_Bound'], merged_df['Upper_Bound'],\n",
    "                     color='lightgray', alpha=0.3, label='Confidence Interval')\n",
    "    plt.title(f\"Prophet Weekly Forecast - {file}\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Average Price\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_graphs, file.replace('.csv','_prophet_weekly_graph.png')))\n",
    "    plt.close()\n",
    "\n",
    "    # Save model\n",
    "    model_path = os.path.join(output_models, file.replace('.csv','_prophet_model.pkl'))\n",
    "    with open(model_path, 'wb') as f:\n",
    "        import pickle\n",
    "        pickle.dump(m, f)\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "# ======================================================\n",
    "# Save Final Metrics\n",
    "# ======================================================\n",
    "metrics_df = pd.DataFrame(metrics_list).round(2)\n",
    "metrics_df.to_csv(metrics_file, index=False)\n",
    "print(\"\\nğŸ“Š All Prophet weekly metrics saved successfully!\")\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f19962-562f-4656-b647-e7830cc6422b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f818f67-8d57-418e-8801-af5c7360a097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ Processing Crop File: capsicum_2010_2024_master_weekly.csv\n",
      "âœ… capsicum_2010_2024_master_weekly.csv | MAE=104.9, RMSE=156.5, RÂ²=0.9539, MAPE=4.11%, Accuracy=95.89%\n",
      "\n",
      "ğŸš€ Processing Crop File: onion_2010_2024_master_weekly.csv\n",
      "âœ… onion_2010_2024_master_weekly.csv | MAE=44.9, RMSE=94.89, RÂ²=0.9801, MAPE=2.41%, Accuracy=97.59%\n",
      "\n",
      "ğŸš€ Processing Crop File: tomato_2010_2024_master_weekly.csv\n",
      "âœ… tomato_2010_2024_master_weekly.csv | MAE=84.63, RMSE=158.4, RÂ²=0.9577, MAPE=5.88%, Accuracy=94.12%\n",
      "\n",
      "ğŸš€ Processing Crop File: wheat_2010_2024_master_weekly.csv\n",
      "âœ… wheat_2010_2024_master_weekly.csv | MAE=17.71, RMSE=66.47, RÂ²=0.9827, MAPE=0.82%, Accuracy=99.18%\n",
      "\n",
      "ğŸ“Š All ARIMA Weekly metrics saved successfully!\n",
      "                               Crop     MAE    RMSE    R2  MAPE(%)  \\\n",
      "0  capsicum_2010_2024_master_weekly  104.90  156.50  0.95     4.11   \n",
      "1     onion_2010_2024_master_weekly   44.90   94.89  0.98     2.41   \n",
      "2    tomato_2010_2024_master_weekly   84.63  158.40  0.96     5.88   \n",
      "3     wheat_2010_2024_master_weekly   17.71   66.47  0.98     0.82   \n",
      "\n",
      "   Accuracy(%)  \n",
      "0        95.89  \n",
      "1        97.59  \n",
      "2        94.12  \n",
      "3        99.18  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ======================================================\n",
    "# Paths\n",
    "# ======================================================\n",
    "input_folder = \"dataset\"\n",
    "output_models = \"arima_output_models_weekly\"\n",
    "output_csv = \"arima_output_csv_weekly\"\n",
    "output_graphs = \"arima_output_graphs_weekly\"\n",
    "metrics_file = \"arima_metrics_weekly.csv\"\n",
    "\n",
    "os.makedirs(output_models, exist_ok=True)\n",
    "os.makedirs(output_csv, exist_ok=True)\n",
    "os.makedirs(output_graphs, exist_ok=True)\n",
    "\n",
    "# ======================================================\n",
    "# Helper Functions\n",
    "# ======================================================\n",
    "def parse_dates_safe(date_series):\n",
    "    try:\n",
    "        return pd.to_datetime(date_series, dayfirst=True)\n",
    "    except:\n",
    "        return pd.to_datetime(date_series, dayfirst=False)\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    mask = y_true != 0\n",
    "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
    "\n",
    "# ======================================================\n",
    "# Metrics Storage\n",
    "# ======================================================\n",
    "metrics_list = []\n",
    "\n",
    "# ======================================================\n",
    "# Process Each CSV File\n",
    "# ======================================================\n",
    "for file in sorted(os.listdir(input_folder)):\n",
    "    if not file.endswith(\".csv\"):\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nğŸš€ Processing Crop File: {file}\")\n",
    "    file_path = os.path.join(input_folder, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # Parse and prepare\n",
    "    # --------------------------------------------\n",
    "    df['Date'] = parse_dates_safe(df['Date'])\n",
    "    df = df.sort_values('Date')\n",
    "    df = df.groupby('Date', as_index=False).mean(numeric_only=True)\n",
    "    df.set_index('Date', inplace=True)\n",
    "\n",
    "    # Weekly frequency alignment\n",
    "    df = df.asfreq('W')\n",
    "    df['Average Price'] = df['Average Price'].ffill().bfill()\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # Feature Engineering (Weekly)\n",
    "    # --------------------------------------------\n",
    "    df['Lag_1'] = df['Average Price'].shift(1)\n",
    "    df['Lag_4'] = df['Average Price'].shift(4)\n",
    "    df['MA_4'] = df['Average Price'].rolling(window=4).mean()\n",
    "    df['MA_12'] = df['Average Price'].rolling(window=12).mean()\n",
    "    df = df.bfill()\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # Model Training\n",
    "    # --------------------------------------------\n",
    "    y = df['Average Price']\n",
    "\n",
    "    # Simple ARIMA (p,d,q)\n",
    "    order = (1, 1, 1)\n",
    "\n",
    "    try:\n",
    "        model = ARIMA(y, order=order)\n",
    "        model_fit = model.fit()\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ARIMA model failed for {file}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # Predictions\n",
    "    # --------------------------------------------\n",
    "    df['Predicted'] = model_fit.predict(start=0, end=len(df)-1)\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # Metrics\n",
    "    # --------------------------------------------\n",
    "    y_true = df['Average Price'].values\n",
    "    y_pred = df['Predicted'].values\n",
    "\n",
    "    mae = round(mean_absolute_error(y_true, y_pred), 2)\n",
    "    rmse = round(np.sqrt(mean_squared_error(y_true, y_pred)), 2)\n",
    "    r2 = round(r2_score(y_true, y_pred), 4)\n",
    "    mape = round(mean_absolute_percentage_error(y_true, y_pred), 2)\n",
    "    acc = round(100 - mape, 2)\n",
    "\n",
    "    print(f\"âœ… {file} | MAE={mae}, RMSE={rmse}, RÂ²={r2}, MAPE={mape}%, Accuracy={acc}%\")\n",
    "\n",
    "    metrics_list.append({\n",
    "        'Crop': file.replace('.csv',''),\n",
    "        'MAE': mae,\n",
    "        'RMSE': rmse,\n",
    "        'R2': r2,\n",
    "        'MAPE(%)': mape,\n",
    "        'Accuracy(%)': acc\n",
    "    })\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # Save updated CSV\n",
    "    # --------------------------------------------\n",
    "    df = df.round(2)\n",
    "    updated_csv_path = os.path.join(output_csv, file.replace('.csv', '_arima_weekly_updated.csv'))\n",
    "    df.to_csv(updated_csv_path)\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # Save model parameters\n",
    "    # --------------------------------------------\n",
    "    model_file = os.path.join(output_models, file.replace('.csv', '_arima_model.txt'))\n",
    "    with open(model_file, 'w') as f:\n",
    "        f.write(str(model_fit.summary()))\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # Plot Graph\n",
    "    # --------------------------------------------\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.plot(df.index, df['Average Price'], label='Actual', color='blue')\n",
    "    plt.plot(df.index, df['Predicted'], label='Predicted (ARIMA)', color='red', linestyle='dashed')\n",
    "    plt.plot(df.index, df['MA_4'], label='MA 4', color='green', linestyle='--')\n",
    "    plt.plot(df.index, df['MA_12'], label='MA 12', color='orange', linestyle='--')\n",
    "    plt.title(f\"ARIMA Weekly Forecast - {file}\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Average Price\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_graphs, file.replace('.csv', '_arima_weekly_graph.png')))\n",
    "    plt.close()\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "# ======================================================\n",
    "# Save Final Metrics\n",
    "# ======================================================\n",
    "metrics_df = pd.DataFrame(metrics_list).round(2)\n",
    "metrics_df.to_csv(metrics_file, index=False)\n",
    "print(\"\\nğŸ“Š All ARIMA Weekly metrics saved successfully!\")\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdb8ea4-83a5-4faf-8b20-770658e27757",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SARIMAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c94c644d-6bb0-4380-868e-a5a9e7a42a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ Processing Crop File: capsicum_2010_2024_master_weekly.csv\n",
      "âœ… capsicum_2010_2024_master_weekly.csv | MAE=86.28, RMSE=163.5, RÂ²=0.9497, MAPE=3.78%, Accuracy=96.22%\n",
      "\n",
      "ğŸš€ Processing Crop File: onion_2010_2024_master_weekly.csv\n",
      "âœ… onion_2010_2024_master_weekly.csv | MAE=39.97, RMSE=124.44, RÂ²=0.9657, MAPE=2.51%, Accuracy=97.49%\n",
      "\n",
      "ğŸš€ Processing Crop File: tomato_2010_2024_master_weekly.csv\n",
      "âœ… tomato_2010_2024_master_weekly.csv | MAE=69.21, RMSE=120.68, RÂ²=0.9755, MAPE=5.65%, Accuracy=94.35%\n",
      "\n",
      "ğŸš€ Processing Crop File: wheat_2010_2024_master_weekly.csv\n",
      "âœ… wheat_2010_2024_master_weekly.csv | MAE=25.05, RMSE=193.36, RÂ²=0.8533, MAPE=1.41%, Accuracy=98.59%\n",
      "\n",
      "ğŸ“Š All SARIMAX Weekly Metrics Saved Successfully!\n",
      "                               Crop    MAE    RMSE    R2  MAPE(%)  Accuracy(%)\n",
      "0  capsicum_2010_2024_master_weekly  86.28  163.50  0.95     3.78        96.22\n",
      "1     onion_2010_2024_master_weekly  39.97  124.44  0.97     2.51        97.49\n",
      "2    tomato_2010_2024_master_weekly  69.21  120.68  0.98     5.65        94.35\n",
      "3     wheat_2010_2024_master_weekly  25.05  193.36  0.85     1.41        98.59\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import warnings\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ======================================================\n",
    "# Paths\n",
    "# ======================================================\n",
    "input_folder = \"dataset\"\n",
    "output_models = \"sarimax_output_models_weekly\"\n",
    "output_csv = \"sarimax_output_csv_weekly\"\n",
    "output_graphs = \"sarimax_output_graphs_weekly\"\n",
    "metrics_file = \"sarimax_metrics_weekly.csv\"\n",
    "\n",
    "os.makedirs(output_models, exist_ok=True)\n",
    "os.makedirs(output_csv, exist_ok=True)\n",
    "os.makedirs(output_graphs, exist_ok=True)\n",
    "\n",
    "# ======================================================\n",
    "# Helper Functions\n",
    "# ======================================================\n",
    "def parse_dates_safe(date_series):\n",
    "    \"\"\"Safe date parsing supporting both DD-MM-YYYY and MM-DD-YYYY.\"\"\"\n",
    "    try:\n",
    "        return pd.to_datetime(date_series, dayfirst=True)\n",
    "    except:\n",
    "        return pd.to_datetime(date_series, dayfirst=False)\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    \"\"\"Compute MAPE with zero protection.\"\"\"\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    mask = y_true != 0\n",
    "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
    "\n",
    "# ======================================================\n",
    "# Metrics Storage\n",
    "# ======================================================\n",
    "metrics_list = []\n",
    "\n",
    "# ======================================================\n",
    "# Process Each Crop File\n",
    "# ======================================================\n",
    "for file in sorted(os.listdir(input_folder)):\n",
    "    if not file.endswith(\".csv\"):\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nğŸš€ Processing Crop File: {file}\")\n",
    "    file_path = os.path.join(input_folder, file)\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # Load and Clean Data\n",
    "    # --------------------------------------------\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['Date'] = parse_dates_safe(df['Date'])\n",
    "    df = df.sort_values('Date')\n",
    "    df = df.groupby('Date', as_index=False).mean(numeric_only=True)\n",
    "    df.set_index('Date', inplace=True)\n",
    "\n",
    "    # Ensure weekly frequency\n",
    "    df = df.asfreq('W')\n",
    "    df['Average Price'] = df['Average Price'].ffill().bfill()\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # Feature Engineering (Weekly)\n",
    "    # --------------------------------------------\n",
    "    df['Lag_1'] = df['Average Price'].shift(1)\n",
    "    df['Lag_4'] = df['Average Price'].shift(4)\n",
    "    df['MA_4'] = df['Average Price'].rolling(window=4).mean()\n",
    "    df['MA_12'] = df['Average Price'].rolling(window=12).mean()\n",
    "    df = df.bfill().ffill()\n",
    "\n",
    "    # Exogenous features for SARIMAX\n",
    "    exog_features = df[['Lag_1', 'Lag_4', 'MA_4', 'MA_12']]\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # Train SARIMAX Model\n",
    "    # --------------------------------------------\n",
    "    order = (1, 1, 1)           # (p, d, q)\n",
    "    seasonal_order = (1, 1, 1, 52)  # Weekly seasonality (52 weeks per year)\n",
    "\n",
    "    try:\n",
    "        model = SARIMAX(df['Average Price'],\n",
    "                        exog=exog_features,\n",
    "                        order=order,\n",
    "                        seasonal_order=seasonal_order,\n",
    "                        enforce_stationarity=False,\n",
    "                        enforce_invertibility=False)\n",
    "        model_fit = model.fit(disp=False)\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Model failed for {file}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # Predictions\n",
    "    # --------------------------------------------\n",
    "    df['Predicted'] = model_fit.predict(start=0, end=len(df)-1, exog=exog_features)\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # Metrics Calculation\n",
    "    # --------------------------------------------\n",
    "    y_true = df['Average Price']\n",
    "    y_pred = df['Predicted']\n",
    "    mae = round(mean_absolute_error(y_true, y_pred), 2)\n",
    "    rmse = round(np.sqrt(mean_squared_error(y_true, y_pred)), 2)\n",
    "    r2 = round(r2_score(y_true, y_pred), 4)\n",
    "    mape = round(mean_absolute_percentage_error(y_true, y_pred), 2)\n",
    "    acc = round(100 - mape, 2)\n",
    "\n",
    "    metrics_list.append({\n",
    "        'Crop': file.replace('.csv',''),\n",
    "        'MAE': mae,\n",
    "        'RMSE': rmse,\n",
    "        'R2': r2,\n",
    "        'MAPE(%)': mape,\n",
    "        'Accuracy(%)': acc\n",
    "    })\n",
    "\n",
    "    print(f\"âœ… {file} | MAE={mae}, RMSE={rmse}, RÂ²={r2}, MAPE={mape}%, Accuracy={acc}%\")\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # Save Model\n",
    "    # --------------------------------------------\n",
    "    model_file = os.path.join(output_models, file.replace(\".csv\", \"_sarimax_weekly_model.pkl\"))\n",
    "    joblib.dump(model_fit, model_file)\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # Save Updated CSV\n",
    "    # --------------------------------------------\n",
    "    df = df.round(2)\n",
    "    updated_csv_path = os.path.join(output_csv, file.replace(\".csv\", \"_sarimax_weekly_updated.csv\"))\n",
    "    df.to_csv(updated_csv_path)\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # Plot Graph\n",
    "    # --------------------------------------------\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.plot(df.index, df['Average Price'], label='Actual Price', color='blue')\n",
    "    plt.plot(df.index, df['Predicted'], label='Predicted (SARIMAX)', color='red', linestyle='dashed')\n",
    "    plt.plot(df.index, df['MA_4'], label='MA 4', color='green', linestyle='--')\n",
    "    plt.plot(df.index, df['MA_12'], label='MA 12', color='orange', linestyle='--')\n",
    "    plt.title(f\"SARIMAX Weekly Forecast - {file}\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Average Price\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_graphs, file.replace(\".csv\", \"_sarimax_weekly_graph.png\")))\n",
    "    plt.close()\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "# ======================================================\n",
    "# Save Final Metrics\n",
    "# ======================================================\n",
    "metrics_df = pd.DataFrame(metrics_list).round(2)\n",
    "metrics_df.to_csv(metrics_file, index=False)\n",
    "print(\"\\nğŸ“Š All SARIMAX Weekly Metrics Saved Successfully!\")\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f440dbe-0135-40a3-8738-112fcb9e26af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TAT+MHA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6a9c604-17d3-44e9-b51b-314b9fa0ff03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ Processing Crop File: capsicum_2010_2024_master_weekly.csv\n",
      "Epoch 1/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 237ms/step - loss: 0.3285 - mae: 0.3916 - val_loss: 0.0301 - val_mae: 0.1328 - learning_rate: 0.0010\n",
      "Epoch 2/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 66ms/step - loss: 0.0315 - mae: 0.1415 - val_loss: 0.0235 - val_mae: 0.1148 - learning_rate: 0.0010\n",
      "Epoch 3/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - loss: 0.0217 - mae: 0.1181 - val_loss: 0.0203 - val_mae: 0.1066 - learning_rate: 0.0010\n",
      "Epoch 4/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - loss: 0.0205 - mae: 0.1132 - val_loss: 0.0259 - val_mae: 0.1218 - learning_rate: 0.0010\n",
      "Epoch 5/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - loss: 0.0221 - mae: 0.1170 - val_loss: 0.0191 - val_mae: 0.1035 - learning_rate: 0.0010\n",
      "Epoch 6/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - loss: 0.0205 - mae: 0.1146 - val_loss: 0.0230 - val_mae: 0.1149 - learning_rate: 0.0010\n",
      "Epoch 7/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - loss: 0.0184 - mae: 0.1063 - val_loss: 0.0188 - val_mae: 0.1044 - learning_rate: 0.0010\n",
      "Epoch 8/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - loss: 0.0162 - mae: 0.1002 - val_loss: 0.0266 - val_mae: 0.1296 - learning_rate: 0.0010\n",
      "Epoch 9/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - loss: 0.0164 - mae: 0.1027 - val_loss: 0.0135 - val_mae: 0.0857 - learning_rate: 0.0010\n",
      "Epoch 10/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - loss: 0.0110 - mae: 0.0830 - val_loss: 0.0314 - val_mae: 0.1502 - learning_rate: 0.0010\n",
      "Epoch 11/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - loss: 0.0126 - mae: 0.0875 - val_loss: 0.0148 - val_mae: 0.0940 - learning_rate: 0.0010\n",
      "Epoch 12/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - loss: 0.0088 - mae: 0.0711 - val_loss: 0.0092 - val_mae: 0.0717 - learning_rate: 0.0010\n",
      "Epoch 13/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - loss: 0.0085 - mae: 0.0709 - val_loss: 0.0162 - val_mae: 0.1032 - learning_rate: 0.0010\n",
      "Epoch 14/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - loss: 0.0084 - mae: 0.0710 - val_loss: 0.0150 - val_mae: 0.1002 - learning_rate: 0.0010\n",
      "Epoch 15/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - loss: 0.0092 - mae: 0.0725 - val_loss: 0.0097 - val_mae: 0.0778 - learning_rate: 0.0010\n",
      "Epoch 16/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - loss: 0.0070 - mae: 0.0659 - val_loss: 0.0078 - val_mae: 0.0688 - learning_rate: 0.0010\n",
      "Epoch 17/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - loss: 0.0070 - mae: 0.0637 - val_loss: 0.0071 - val_mae: 0.0640 - learning_rate: 0.0010\n",
      "Epoch 18/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - loss: 0.0060 - mae: 0.0581 - val_loss: 0.0088 - val_mae: 0.0731 - learning_rate: 0.0010\n",
      "Epoch 19/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - loss: 0.0073 - mae: 0.0653 - val_loss: 0.0073 - val_mae: 0.0633 - learning_rate: 0.0010\n",
      "Epoch 20/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - loss: 0.0059 - mae: 0.0585 - val_loss: 0.0077 - val_mae: 0.0658 - learning_rate: 0.0010\n",
      "Epoch 21/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 0.0050 - mae: 0.0544 - val_loss: 0.0137 - val_mae: 0.0988 - learning_rate: 0.0010\n",
      "Epoch 22/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - loss: 0.0084 - mae: 0.0714 - val_loss: 0.0069 - val_mae: 0.0625 - learning_rate: 5.0000e-04\n",
      "Epoch 23/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 0.0049 - mae: 0.0535 - val_loss: 0.0072 - val_mae: 0.0647 - learning_rate: 5.0000e-04\n",
      "Epoch 24/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - loss: 0.0047 - mae: 0.0534 - val_loss: 0.0057 - val_mae: 0.0562 - learning_rate: 5.0000e-04\n",
      "Epoch 25/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - loss: 0.0042 - mae: 0.0502 - val_loss: 0.0055 - val_mae: 0.0558 - learning_rate: 5.0000e-04\n",
      "Epoch 26/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - loss: 0.0055 - mae: 0.0558 - val_loss: 0.0077 - val_mae: 0.0685 - learning_rate: 5.0000e-04\n",
      "Epoch 27/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - loss: 0.0050 - mae: 0.0539 - val_loss: 0.0065 - val_mae: 0.0617 - learning_rate: 5.0000e-04\n",
      "Epoch 28/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 0.0076 - mae: 0.0673 - val_loss: 0.0060 - val_mae: 0.0572 - learning_rate: 5.0000e-04\n",
      "Epoch 29/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - loss: 0.0045 - mae: 0.0521 - val_loss: 0.0058 - val_mae: 0.0567 - learning_rate: 5.0000e-04\n",
      "Epoch 30/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - loss: 0.0046 - mae: 0.0518 - val_loss: 0.0062 - val_mae: 0.0578 - learning_rate: 2.5000e-04\n",
      "Epoch 31/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - loss: 0.0047 - mae: 0.0530 - val_loss: 0.0060 - val_mae: 0.0569 - learning_rate: 2.5000e-04\n",
      "Epoch 32/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 0.0048 - mae: 0.0519 - val_loss: 0.0056 - val_mae: 0.0545 - learning_rate: 2.5000e-04\n",
      "Epoch 33/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - loss: 0.0049 - mae: 0.0522 - val_loss: 0.0055 - val_mae: 0.0543 - learning_rate: 2.5000e-04\n",
      "Epoch 34/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - loss: 0.0041 - mae: 0.0489 - val_loss: 0.0054 - val_mae: 0.0540 - learning_rate: 1.2500e-04\n",
      "Epoch 35/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 0.0046 - mae: 0.0531 - val_loss: 0.0056 - val_mae: 0.0548 - learning_rate: 1.2500e-04\n",
      "Epoch 36/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - loss: 0.0049 - mae: 0.0534 - val_loss: 0.0064 - val_mae: 0.0608 - learning_rate: 1.2500e-04\n",
      "Epoch 37/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 0.0044 - mae: 0.0507 - val_loss: 0.0056 - val_mae: 0.0559 - learning_rate: 1.2500e-04\n",
      "Epoch 38/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - loss: 0.0052 - mae: 0.0543 - val_loss: 0.0052 - val_mae: 0.0532 - learning_rate: 1.2500e-04\n",
      "Epoch 39/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - loss: 0.0043 - mae: 0.0507 - val_loss: 0.0053 - val_mae: 0.0534 - learning_rate: 1.2500e-04\n",
      "Epoch 40/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 75ms/step - loss: 0.0047 - mae: 0.0532 - val_loss: 0.0054 - val_mae: 0.0538 - learning_rate: 1.2500e-04\n",
      "Epoch 41/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 0.0048 - mae: 0.0531 - val_loss: 0.0055 - val_mae: 0.0556 - learning_rate: 1.2500e-04\n",
      "Epoch 42/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step - loss: 0.0044 - mae: 0.0508 - val_loss: 0.0052 - val_mae: 0.0533 - learning_rate: 1.2500e-04\n",
      "Epoch 43/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - loss: 0.0042 - mae: 0.0485 - val_loss: 0.0060 - val_mae: 0.0587 - learning_rate: 6.2500e-05\n",
      "Epoch 44/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - loss: 0.0048 - mae: 0.0518 - val_loss: 0.0056 - val_mae: 0.0560 - learning_rate: 6.2500e-05\n",
      "Epoch 45/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - loss: 0.0038 - mae: 0.0470 - val_loss: 0.0054 - val_mae: 0.0547 - learning_rate: 6.2500e-05\n",
      "Epoch 46/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - loss: 0.0039 - mae: 0.0484 - val_loss: 0.0052 - val_mae: 0.0533 - learning_rate: 6.2500e-05\n",
      "Epoch 47/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - loss: 0.0043 - mae: 0.0497 - val_loss: 0.0055 - val_mae: 0.0557 - learning_rate: 3.1250e-05\n",
      "Epoch 48/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - loss: 0.0044 - mae: 0.0506 - val_loss: 0.0054 - val_mae: 0.0550 - learning_rate: 3.1250e-05\n",
      "Epoch 49/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - loss: 0.0041 - mae: 0.0478 - val_loss: 0.0054 - val_mae: 0.0550 - learning_rate: 3.1250e-05\n",
      "Epoch 50/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - loss: 0.0042 - mae: 0.0482 - val_loss: 0.0053 - val_mae: 0.0544 - learning_rate: 3.1250e-05\n",
      "Epoch 51/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - loss: 0.0042 - mae: 0.0488 - val_loss: 0.0052 - val_mae: 0.0539 - learning_rate: 1.5625e-05\n",
      "Epoch 52/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - loss: 0.0039 - mae: 0.0472 - val_loss: 0.0052 - val_mae: 0.0535 - learning_rate: 1.5625e-05\n",
      "Epoch 53/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - loss: 0.0039 - mae: 0.0467 - val_loss: 0.0051 - val_mae: 0.0534 - learning_rate: 1.5625e-05\n",
      "Epoch 54/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - loss: 0.0044 - mae: 0.0499 - val_loss: 0.0051 - val_mae: 0.0534 - learning_rate: 1.5625e-05\n",
      "Epoch 55/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - loss: 0.0042 - mae: 0.0501 - val_loss: 0.0052 - val_mae: 0.0535 - learning_rate: 7.8125e-06\n",
      "Epoch 56/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - loss: 0.0040 - mae: 0.0476 - val_loss: 0.0051 - val_mae: 0.0534 - learning_rate: 7.8125e-06\n",
      "Epoch 57/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - loss: 0.0041 - mae: 0.0486 - val_loss: 0.0052 - val_mae: 0.0538 - learning_rate: 7.8125e-06\n",
      "Epoch 58/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - loss: 0.0040 - mae: 0.0492 - val_loss: 0.0054 - val_mae: 0.0552 - learning_rate: 7.8125e-06\n",
      "Epoch 59/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - loss: 0.0039 - mae: 0.0493 - val_loss: 0.0054 - val_mae: 0.0550 - learning_rate: 3.9063e-06\n",
      "Epoch 60/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 0.0040 - mae: 0.0483 - val_loss: 0.0053 - val_mae: 0.0546 - learning_rate: 3.9063e-06\n",
      "\u001b[1m24/24\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step\n",
      "âœ… capsicum_2010_2024_master_weekly.csv | MAE=140.27, RMSE=192.06, RÂ²=0.9285, MAPE=5.59%, Accuracy=94.41%\n",
      "\n",
      "ğŸš€ Processing Crop File: onion_2010_2024_master_weekly.csv\n",
      "Epoch 1/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 55ms/step - loss: 0.2319 - mae: 0.3317 - val_loss: 0.1764 - val_mae: 0.3829 - learning_rate: 0.0010\n",
      "Epoch 2/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 0.0420 - mae: 0.1581 - val_loss: 0.0896 - val_mae: 0.2466 - learning_rate: 0.0010\n",
      "Epoch 3/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 0.0270 - mae: 0.1257 - val_loss: 0.0561 - val_mae: 0.1757 - learning_rate: 0.0010\n",
      "Epoch 4/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 0.0211 - mae: 0.1099 - val_loss: 0.0703 - val_mae: 0.2078 - learning_rate: 0.0010\n",
      "Epoch 5/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.0230 - mae: 0.1120 - val_loss: 0.0354 - val_mae: 0.1414 - learning_rate: 0.0010\n",
      "Epoch 6/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 0.0181 - mae: 0.0987 - val_loss: 0.0401 - val_mae: 0.1485 - learning_rate: 0.0010\n",
      "Epoch 7/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 0.0158 - mae: 0.0910 - val_loss: 0.0180 - val_mae: 0.1111 - learning_rate: 0.0010\n",
      "Epoch 8/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 0.0113 - mae: 0.0738 - val_loss: 0.0119 - val_mae: 0.0859 - learning_rate: 0.0010\n",
      "Epoch 9/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 0.0079 - mae: 0.0619 - val_loss: 0.0062 - val_mae: 0.0606 - learning_rate: 0.0010\n",
      "Epoch 10/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 0.0074 - mae: 0.0619 - val_loss: 0.0055 - val_mae: 0.0576 - learning_rate: 0.0010\n",
      "Epoch 11/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 0.0076 - mae: 0.0685 - val_loss: 0.0038 - val_mae: 0.0474 - learning_rate: 0.0010\n",
      "Epoch 12/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - loss: 0.0065 - mae: 0.0604 - val_loss: 0.0035 - val_mae: 0.0475 - learning_rate: 0.0010\n",
      "Epoch 13/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 0.0062 - mae: 0.0610 - val_loss: 0.0029 - val_mae: 0.0397 - learning_rate: 0.0010\n",
      "Epoch 14/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 0.0050 - mae: 0.0515 - val_loss: 0.0028 - val_mae: 0.0396 - learning_rate: 0.0010\n",
      "Epoch 15/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 0.0047 - mae: 0.0503 - val_loss: 0.0040 - val_mae: 0.0513 - learning_rate: 0.0010\n",
      "Epoch 16/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 0.0039 - mae: 0.0475 - val_loss: 0.0038 - val_mae: 0.0479 - learning_rate: 0.0010\n",
      "Epoch 17/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 0.0047 - mae: 0.0499 - val_loss: 0.0023 - val_mae: 0.0365 - learning_rate: 0.0010\n",
      "Epoch 18/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 0.0049 - mae: 0.0521 - val_loss: 0.0095 - val_mae: 0.0860 - learning_rate: 0.0010\n",
      "Epoch 19/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 0.0044 - mae: 0.0511 - val_loss: 0.0024 - val_mae: 0.0409 - learning_rate: 0.0010\n",
      "Epoch 20/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.0042 - mae: 0.0474 - val_loss: 0.0046 - val_mae: 0.0544 - learning_rate: 0.0010\n",
      "Epoch 21/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 0.0033 - mae: 0.0433 - val_loss: 0.0023 - val_mae: 0.0382 - learning_rate: 0.0010\n",
      "Epoch 22/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 0.0040 - mae: 0.0430 - val_loss: 0.0025 - val_mae: 0.0415 - learning_rate: 5.0000e-04\n",
      "Epoch 23/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 0.0039 - mae: 0.0448 - val_loss: 0.0025 - val_mae: 0.0409 - learning_rate: 5.0000e-04\n",
      "Epoch 24/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 0.0039 - mae: 0.0434 - val_loss: 0.0021 - val_mae: 0.0362 - learning_rate: 5.0000e-04\n",
      "Epoch 25/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 0.0032 - mae: 0.0412 - val_loss: 0.0020 - val_mae: 0.0349 - learning_rate: 5.0000e-04\n",
      "Epoch 26/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 0.0036 - mae: 0.0430 - val_loss: 0.0024 - val_mae: 0.0401 - learning_rate: 5.0000e-04\n",
      "Epoch 27/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 0.0033 - mae: 0.0413 - val_loss: 0.0019 - val_mae: 0.0357 - learning_rate: 5.0000e-04\n",
      "Epoch 28/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 0.0034 - mae: 0.0428 - val_loss: 0.0019 - val_mae: 0.0352 - learning_rate: 5.0000e-04\n",
      "Epoch 29/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.0032 - mae: 0.0410 - val_loss: 0.0019 - val_mae: 0.0344 - learning_rate: 5.0000e-04\n",
      "Epoch 30/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - loss: 0.0030 - mae: 0.0396 - val_loss: 0.0023 - val_mae: 0.0362 - learning_rate: 5.0000e-04\n",
      "Epoch 31/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - loss: 0.0030 - mae: 0.0384 - val_loss: 0.0019 - val_mae: 0.0341 - learning_rate: 5.0000e-04\n",
      "Epoch 32/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 0.0024 - mae: 0.0362 - val_loss: 0.0020 - val_mae: 0.0344 - learning_rate: 2.5000e-04\n",
      "Epoch 33/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 0.0028 - mae: 0.0382 - val_loss: 0.0018 - val_mae: 0.0341 - learning_rate: 2.5000e-04\n",
      "Epoch 34/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 0.0025 - mae: 0.0354 - val_loss: 0.0021 - val_mae: 0.0349 - learning_rate: 2.5000e-04\n",
      "Epoch 35/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 0.0027 - mae: 0.0363 - val_loss: 0.0018 - val_mae: 0.0336 - learning_rate: 2.5000e-04\n",
      "Epoch 36/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 0.0029 - mae: 0.0383 - val_loss: 0.0018 - val_mae: 0.0344 - learning_rate: 2.5000e-04\n",
      "Epoch 37/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - loss: 0.0028 - mae: 0.0379 - val_loss: 0.0019 - val_mae: 0.0341 - learning_rate: 2.5000e-04\n",
      "Epoch 38/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - loss: 0.0034 - mae: 0.0409 - val_loss: 0.0022 - val_mae: 0.0392 - learning_rate: 2.5000e-04\n",
      "Epoch 39/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - loss: 0.0028 - mae: 0.0375 - val_loss: 0.0017 - val_mae: 0.0328 - learning_rate: 2.5000e-04\n",
      "Epoch 40/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - loss: 0.0027 - mae: 0.0358 - val_loss: 0.0017 - val_mae: 0.0328 - learning_rate: 1.2500e-04\n",
      "Epoch 41/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - loss: 0.0027 - mae: 0.0367 - val_loss: 0.0017 - val_mae: 0.0328 - learning_rate: 1.2500e-04\n",
      "Epoch 42/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - loss: 0.0029 - mae: 0.0382 - val_loss: 0.0017 - val_mae: 0.0326 - learning_rate: 1.2500e-04\n",
      "Epoch 43/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - loss: 0.0026 - mae: 0.0362 - val_loss: 0.0017 - val_mae: 0.0325 - learning_rate: 1.2500e-04\n",
      "Epoch 44/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 0.0027 - mae: 0.0362 - val_loss: 0.0019 - val_mae: 0.0343 - learning_rate: 1.2500e-04\n",
      "Epoch 45/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - loss: 0.0033 - mae: 0.0385 - val_loss: 0.0018 - val_mae: 0.0335 - learning_rate: 1.2500e-04\n",
      "Epoch 46/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - loss: 0.0026 - mae: 0.0368 - val_loss: 0.0017 - val_mae: 0.0330 - learning_rate: 6.2500e-05\n",
      "Epoch 47/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 0.0027 - mae: 0.0363 - val_loss: 0.0017 - val_mae: 0.0324 - learning_rate: 6.2500e-05\n",
      "Epoch 48/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 0.0027 - mae: 0.0365 - val_loss: 0.0017 - val_mae: 0.0326 - learning_rate: 6.2500e-05\n",
      "Epoch 49/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 0.0030 - mae: 0.0366 - val_loss: 0.0016 - val_mae: 0.0320 - learning_rate: 6.2500e-05\n",
      "Epoch 50/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - loss: 0.0027 - mae: 0.0363 - val_loss: 0.0017 - val_mae: 0.0323 - learning_rate: 3.1250e-05\n",
      "Epoch 51/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - loss: 0.0024 - mae: 0.0343 - val_loss: 0.0016 - val_mae: 0.0320 - learning_rate: 3.1250e-05\n",
      "Epoch 52/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - loss: 0.0023 - mae: 0.0350 - val_loss: 0.0016 - val_mae: 0.0320 - learning_rate: 3.1250e-05\n",
      "Epoch 53/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - loss: 0.0028 - mae: 0.0367 - val_loss: 0.0016 - val_mae: 0.0320 - learning_rate: 3.1250e-05\n",
      "Epoch 54/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - loss: 0.0025 - mae: 0.0360 - val_loss: 0.0017 - val_mae: 0.0322 - learning_rate: 1.5625e-05\n",
      "Epoch 55/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - loss: 0.0027 - mae: 0.0353 - val_loss: 0.0017 - val_mae: 0.0324 - learning_rate: 1.5625e-05\n",
      "Epoch 56/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 0.0027 - mae: 0.0357 - val_loss: 0.0017 - val_mae: 0.0324 - learning_rate: 1.5625e-05\n",
      "Epoch 57/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - loss: 0.0022 - mae: 0.0348 - val_loss: 0.0016 - val_mae: 0.0319 - learning_rate: 1.5625e-05\n",
      "Epoch 58/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - loss: 0.0030 - mae: 0.0374 - val_loss: 0.0017 - val_mae: 0.0322 - learning_rate: 7.8125e-06\n",
      "Epoch 59/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 47ms/step - loss: 0.0025 - mae: 0.0354 - val_loss: 0.0017 - val_mae: 0.0323 - learning_rate: 7.8125e-06\n",
      "Epoch 60/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 0.0030 - mae: 0.0363 - val_loss: 0.0017 - val_mae: 0.0323 - learning_rate: 7.8125e-06\n",
      "\u001b[1m24/24\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step  \n",
      "âœ… onion_2010_2024_master_weekly.csv | MAE=71.63, RMSE=102.99, RÂ²=0.976, MAPE=4.29%, Accuracy=95.71%\n",
      "\n",
      "ğŸš€ Processing Crop File: tomato_2010_2024_master_weekly.csv\n",
      "Epoch 1/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 119ms/step - loss: 0.1461 - mae: 0.2632 - val_loss: 0.0336 - val_mae: 0.1245 - learning_rate: 0.0010\n",
      "Epoch 2/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - loss: 0.0153 - mae: 0.0948 - val_loss: 0.0419 - val_mae: 0.1219 - learning_rate: 0.0010\n",
      "Epoch 3/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - loss: 0.0103 - mae: 0.0752 - val_loss: 0.0434 - val_mae: 0.1245 - learning_rate: 0.0010\n",
      "Epoch 4/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - loss: 0.0089 - mae: 0.0682 - val_loss: 0.0421 - val_mae: 0.1227 - learning_rate: 0.0010\n",
      "Epoch 5/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - loss: 0.0084 - mae: 0.0661 - val_loss: 0.0366 - val_mae: 0.1290 - learning_rate: 0.0010\n",
      "Epoch 6/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - loss: 0.0079 - mae: 0.0644 - val_loss: 0.0382 - val_mae: 0.1273 - learning_rate: 5.0000e-04\n",
      "Epoch 7/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - loss: 0.0080 - mae: 0.0661 - val_loss: 0.0394 - val_mae: 0.1310 - learning_rate: 5.0000e-04\n",
      "Epoch 8/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 0.0076 - mae: 0.0629 - val_loss: 0.0380 - val_mae: 0.1330 - learning_rate: 5.0000e-04\n",
      "Epoch 9/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - loss: 0.0071 - mae: 0.0609 - val_loss: 0.0387 - val_mae: 0.1484 - learning_rate: 5.0000e-04\n",
      "\u001b[1m24/24\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 56ms/step  \n",
      "âœ… tomato_2010_2024_master_weekly.csv | MAE=748.49, RMSE=866.78, RÂ²=-0.253, MAPE=81.71%, Accuracy=18.29%\n",
      "\n",
      "ğŸš€ Processing Crop File: wheat_2010_2024_master_weekly.csv\n",
      "Epoch 1/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 183ms/step - loss: 0.2457 - mae: 0.3425 - val_loss: 0.0788 - val_mae: 0.2750 - learning_rate: 0.0010\n",
      "Epoch 2/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - loss: 0.0316 - mae: 0.1367 - val_loss: 0.0491 - val_mae: 0.2177 - learning_rate: 0.0010\n",
      "Epoch 3/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 0.0195 - mae: 0.1109 - val_loss: 0.0297 - val_mae: 0.1662 - learning_rate: 0.0010\n",
      "Epoch 4/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - loss: 0.0098 - mae: 0.0784 - val_loss: 0.0103 - val_mae: 0.0906 - learning_rate: 0.0010\n",
      "Epoch 5/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 76ms/step - loss: 0.0061 - mae: 0.0600 - val_loss: 0.0144 - val_mae: 0.1127 - learning_rate: 0.0010\n",
      "Epoch 6/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - loss: 0.0058 - mae: 0.0595 - val_loss: 0.0035 - val_mae: 0.0446 - learning_rate: 0.0010\n",
      "Epoch 7/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - loss: 0.0044 - mae: 0.0524 - val_loss: 0.0091 - val_mae: 0.0863 - learning_rate: 0.0010\n",
      "Epoch 8/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - loss: 0.0042 - mae: 0.0488 - val_loss: 0.0034 - val_mae: 0.0424 - learning_rate: 0.0010\n",
      "Epoch 9/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - loss: 0.0042 - mae: 0.0484 - val_loss: 0.0039 - val_mae: 0.0486 - learning_rate: 0.0010\n",
      "Epoch 10/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 0.0040 - mae: 0.0490 - val_loss: 0.0036 - val_mae: 0.0454 - learning_rate: 0.0010\n",
      "Epoch 11/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 0.0047 - mae: 0.0528 - val_loss: 0.0016 - val_mae: 0.0268 - learning_rate: 0.0010\n",
      "Epoch 12/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 0.0050 - mae: 0.0544 - val_loss: 0.0036 - val_mae: 0.0453 - learning_rate: 0.0010\n",
      "Epoch 13/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 0.0047 - mae: 0.0537 - val_loss: 0.0029 - val_mae: 0.0375 - learning_rate: 0.0010\n",
      "Epoch 14/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - loss: 0.0043 - mae: 0.0506 - val_loss: 0.0117 - val_mae: 0.0994 - learning_rate: 0.0010\n",
      "Epoch 15/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 0.0044 - mae: 0.0503 - val_loss: 0.0055 - val_mae: 0.0619 - learning_rate: 0.0010\n",
      "Epoch 16/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 0.0040 - mae: 0.0479 - val_loss: 0.0047 - val_mae: 0.0558 - learning_rate: 5.0000e-04\n",
      "Epoch 17/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 0.0034 - mae: 0.0445 - val_loss: 0.0067 - val_mae: 0.0707 - learning_rate: 5.0000e-04\n",
      "Epoch 18/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 0.0036 - mae: 0.0462 - val_loss: 0.0103 - val_mae: 0.0909 - learning_rate: 5.0000e-04\n",
      "Epoch 19/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - loss: 0.0038 - mae: 0.0474 - val_loss: 0.0145 - val_mae: 0.1111 - learning_rate: 5.0000e-04\n",
      "\u001b[1m24/24\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step  \n",
      "âœ… wheat_2010_2024_master_weekly.csv | MAE=115.15, RMSE=130.96, RÂ²=0.9302, MAPE=5.6%, Accuracy=94.4%\n",
      "\n",
      "ğŸ“Š All crops processed â€” metrics saved to tat_mha_metrics_weekly.csv\n",
      "                               Crop     MAE    RMSE    R2  MAPE(%)  \\\n",
      "0  capsicum_2010_2024_master_weekly  140.27  192.06  0.93     5.59   \n",
      "1     onion_2010_2024_master_weekly   71.63  102.99  0.98     4.29   \n",
      "2    tomato_2010_2024_master_weekly  748.49  866.78 -0.25    81.71   \n",
      "3     wheat_2010_2024_master_weekly  115.15  130.96  0.93     5.60   \n",
      "\n",
      "   Accuracy(%)  \n",
      "0        94.41  \n",
      "1        95.71  \n",
      "2        18.29  \n",
      "3        94.40  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers, callbacks\n",
    "\n",
    "# -----------------------------\n",
    "# Paths / Output dirs\n",
    "# -----------------------------\n",
    "input_folder = \"dataset\"\n",
    "output_models = \"tat_mha_output_models_weekly\"\n",
    "output_csv = \"tat_mha_output_csv_weekly\"\n",
    "output_graphs = \"tat_mha_output_graphs_weekly\"\n",
    "metrics_file = \"tat_mha_metrics_weekly.csv\"\n",
    "\n",
    "os.makedirs(output_models, exist_ok=True)\n",
    "os.makedirs(output_csv, exist_ok=True)\n",
    "os.makedirs(output_graphs, exist_ok=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Config - change if needed\n",
    "# -----------------------------\n",
    "look_back = 30         # number of weeks used as input sequence\n",
    "batch_size = 32\n",
    "epochs = 60\n",
    "val_split = 0.15\n",
    "d_model = 64\n",
    "num_heads = 4\n",
    "ff_dim = 128\n",
    "dropout_rate = 0.15\n",
    "seed = 42\n",
    "\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "def parse_dates_safe(s):\n",
    "    try:\n",
    "        return pd.to_datetime(s, dayfirst=True)\n",
    "    except:\n",
    "        return pd.to_datetime(s, dayfirst=False)\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    mask = y_true != 0\n",
    "    if mask.sum() == 0:\n",
    "        return np.nan\n",
    "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
    "\n",
    "def create_multivariate_dataset(y_scaled, exog_scaled, look_back):\n",
    "    \"\"\"\n",
    "    y_scaled: (n,1)\n",
    "    exog_scaled: (n, k)\n",
    "    returns X shape (n-look_back, look_back, 1+k), y: (n-look_back,)\n",
    "    \"\"\"\n",
    "    n = len(y_scaled)\n",
    "    k = exog_scaled.shape[1] if exog_scaled is not None else 0\n",
    "    X, Y = [], []\n",
    "    for i in range(n - look_back):\n",
    "        seq_y = y_scaled[i:i+look_back, 0]\n",
    "        if k > 0:\n",
    "            seq_exog = exog_scaled[i:i+look_back, :]\n",
    "            seq = np.concatenate([seq_y.reshape(-1,1), seq_exog], axis=1)\n",
    "        else:\n",
    "            seq = seq_y.reshape(-1,1)\n",
    "        X.append(seq)\n",
    "        Y.append(y_scaled[i+look_back, 0])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "def positional_encoding(seq_len, d_model):\n",
    "    \"\"\"Sinusoidal positional encoding (seq_len, d_model)\"\"\"\n",
    "    pos = np.arange(seq_len)[:, np.newaxis]\n",
    "    i = np.arange(d_model)[np.newaxis, :]\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    angle_rads = pos * angle_rates\n",
    "    # apply sin to even indices in the array; cos to odd indices\n",
    "    s = np.zeros_like(angle_rads)\n",
    "    s[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    s[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    return tf.cast(s, dtype=tf.float32)\n",
    "\n",
    "# -----------------------------\n",
    "# Build TAT+MHA model\n",
    "# -----------------------------\n",
    "def build_tat_mha_model(input_shape, d_model=64, num_heads=4, ff_dim=128, dropout_rate=0.15):\n",
    "    \"\"\"\n",
    "    input_shape: (seq_len, num_features)\n",
    "    returns compiled Keras model\n",
    "    \"\"\"\n",
    "    seq_len, num_feat = input_shape\n",
    "    inp = layers.Input(shape=(seq_len, num_feat))  # (B, S, F)\n",
    "\n",
    "    # project features to d_model\n",
    "    x = layers.Dense(d_model)(inp)  # (B, S, d_model)\n",
    "\n",
    "    # add positional encoding\n",
    "    pos_enc = positional_encoding(seq_len, d_model)\n",
    "    x = x + pos_enc  # broadcast ok\n",
    "\n",
    "    # Transformer Block(s) - we use one or two blocks\n",
    "    # Multi-head attention (temporal self-attention)\n",
    "    attn_out = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)(x, x)\n",
    "    attn_out = layers.Dropout(dropout_rate)(attn_out)\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x + attn_out)\n",
    "\n",
    "    # Feed-forward\n",
    "    ff = layers.Dense(ff_dim, activation='relu')(x)\n",
    "    ff = layers.Dense(d_model)(ff)\n",
    "    ff = layers.Dropout(dropout_rate)(ff)\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x + ff)\n",
    "\n",
    "    # (Optional) additional MHA block\n",
    "    attn_out2 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)(x, x)\n",
    "    attn_out2 = layers.Dropout(dropout_rate)(attn_out2)\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x + attn_out2)\n",
    "    ff2 = layers.Dense(ff_dim, activation='relu')(x)\n",
    "    ff2 = layers.Dense(d_model)(ff2)\n",
    "    ff2 = layers.Dropout(dropout_rate)(ff2)\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x + ff2)\n",
    "\n",
    "    # Pool and output\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dense(ff_dim//2, activation='relu')(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    out = layers.Dense(1)(x)\n",
    "\n",
    "    model = models.Model(inputs=inp, outputs=out)\n",
    "    model.compile(optimizer=optimizers.Adam(1e-3), loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# -----------------------------\n",
    "# Main loop: process each weekly crop CSV\n",
    "# -----------------------------\n",
    "metrics_list = []\n",
    "\n",
    "for file in sorted(os.listdir(input_folder)):\n",
    "    if not file.endswith(\".csv\"):\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nğŸš€ Processing Crop File: {file}\")\n",
    "    file_path = os.path.join(input_folder, file)\n",
    "\n",
    "    # -------------------------\n",
    "    # Load & parse\n",
    "    # -------------------------\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['Date'] = parse_dates_safe(df['Date'])\n",
    "    df = df.sort_values('Date')\n",
    "    # aggregate duplicate dates (take mean of numeric columns)\n",
    "    df = df.groupby('Date', as_index=False).mean(numeric_only=True)\n",
    "    df.set_index('Date', inplace=True)\n",
    "\n",
    "    # set weekly frequency (align to week)\n",
    "    df = df.asfreq('W')\n",
    "    # ensure Average Price exists\n",
    "    if 'Average Price' not in df.columns:\n",
    "        print(f\"âš  'Average Price' not found in {file}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    # fill missing price values\n",
    "    df['Average Price'] = df['Average Price'].ffill().bfill().fillna(df['Average Price'].mean())\n",
    "\n",
    "    # -------------------------\n",
    "    # Feature engineering (weekly)\n",
    "    # -------------------------\n",
    "    df['Lag_1'] = df['Average Price'].shift(1)\n",
    "    df['Lag_4'] = df['Average Price'].shift(4)\n",
    "    df['MA_4'] = df['Average Price'].rolling(window=4).mean()\n",
    "    df['MA_12'] = df['Average Price'].rolling(window=12).mean()\n",
    "\n",
    "    # backfill/forward fill any remaining NaNs in features\n",
    "    df[['Lag_1','Lag_4','MA_4','MA_12']] = df[['Lag_1','Lag_4','MA_4','MA_12']].bfill().ffill()\n",
    "\n",
    "    # -------------------------\n",
    "    # Prepare arrays & scalers\n",
    "    # -------------------------\n",
    "    # We'll use target scaler to inverse predictions later\n",
    "    y = df[['Average Price']].values.astype('float32')\n",
    "    exog_cols = ['Lag_1','Lag_4','MA_4','MA_12']\n",
    "    exog = df[exog_cols].values.astype('float32')\n",
    "\n",
    "    # scale\n",
    "    y_scaler = MinMaxScaler()\n",
    "    exog_scaler = MinMaxScaler()\n",
    "    y_scaled = y_scaler.fit_transform(y)\n",
    "    exog_scaled = exog_scaler.fit_transform(exog)\n",
    "\n",
    "    # sequences\n",
    "    X_all, y_all = create_multivariate_dataset(y_scaled, exog_scaled, look_back)\n",
    "    if len(X_all) == 0:\n",
    "        print(f\"âš  Not enough data for look_back={look_back} in {file}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # train/val split (chronological)\n",
    "    val_size = int(len(X_all) * val_split)\n",
    "    train_size = len(X_all) - val_size\n",
    "    X_train = X_all[:train_size]\n",
    "    y_train = y_all[:train_size]\n",
    "    X_val = X_all[train_size:]\n",
    "    y_val = y_all[train_size:]\n",
    "\n",
    "    # -------------------------\n",
    "    # Build model & train\n",
    "    # -------------------------\n",
    "    input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "    model = build_tat_mha_model(input_shape, d_model=d_model, num_heads=num_heads, ff_dim=ff_dim, dropout_rate=dropout_rate)\n",
    "\n",
    "    es = callbacks.EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True)\n",
    "    rl = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, min_lr=1e-6)\n",
    "\n",
    "    history = model.fit(X_train, y_train,\n",
    "                        validation_data=(X_val, y_val),\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        callbacks=[es, rl],\n",
    "                        verbose=1)\n",
    "\n",
    "    # -------------------------\n",
    "    # Predict on all sequences and align with original index\n",
    "    # -------------------------\n",
    "    preds_scaled = model.predict(X_all, batch_size=batch_size)\n",
    "    preds = y_scaler.inverse_transform(preds_scaled).flatten()\n",
    "\n",
    "    # create Predicted column with NaNs for first look_back rows\n",
    "    df['Predicted'] = np.nan\n",
    "    start_idx = look_back\n",
    "    df.iloc[start_idx : start_idx + len(preds), df.columns.get_loc('Predicted')] = preds\n",
    "\n",
    "    # round to 2 decimals\n",
    "    for col in ['Average Price','Predicted','Lag_1','Lag_4','MA_4','MA_12']:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].round(2)\n",
    "\n",
    "    # -------------------------\n",
    "    # Metrics (on rows where prediction exists)\n",
    "    # -------------------------\n",
    "    mask = ~np.isnan(df['Predicted'].values)\n",
    "    y_true = df.loc[mask, 'Average Price'].values\n",
    "    y_pred = df.loc[mask, 'Predicted'].values\n",
    "\n",
    "    mae = round(mean_absolute_error(y_true, y_pred), 2)\n",
    "    rmse = round(np.sqrt(mean_squared_error(y_true, y_pred)), 2)\n",
    "    r2 = round(r2_score(y_true, y_pred), 4)\n",
    "    mape = round(mean_absolute_percentage_error(y_true, y_pred), 2)\n",
    "    accuracy = round(100 - mape, 2)\n",
    "\n",
    "    metrics_list.append({\n",
    "        'Crop': file.replace('.csv',''),\n",
    "        'MAE': mae,\n",
    "        'RMSE': rmse,\n",
    "        'R2': r2,\n",
    "        'MAPE(%)': mape,\n",
    "        'Accuracy(%)': accuracy\n",
    "    })\n",
    "\n",
    "    print(f\"âœ… {file} | MAE={mae}, RMSE={rmse}, RÂ²={r2}, MAPE={mape}%, Accuracy={accuracy}%\")\n",
    "\n",
    "    # -------------------------\n",
    "    # Save model, CSV, graph\n",
    "    # -------------------------\n",
    "    model_save_path = os.path.join(output_models, file.replace('.csv','_tat_mha_weekly.keras'))\n",
    "    model.save(model_save_path)\n",
    "\n",
    "    out_csv = os.path.join(output_csv, file.replace('.csv','_tat_mha_weekly_updated.csv'))\n",
    "    df_reset = df.reset_index()\n",
    "    df_reset.to_csv(out_csv, index=False, float_format='%.2f')\n",
    "\n",
    "    # graph\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.plot(df_reset['Date'], df_reset['Average Price'], label='Actual', color='blue')\n",
    "    plt.plot(df_reset['Date'], df_reset['Predicted'], label='Predicted (TAT+MHA)', color='red', linestyle='dashed')\n",
    "    plt.plot(df_reset['Date'], df_reset['MA_4'], label='MA_4', color='green', linestyle='--')\n",
    "    plt.plot(df_reset['Date'], df_reset['MA_12'], label='MA_12', color='orange', linestyle='--')\n",
    "    plt.title(f\"TAT+MHA Weekly Forecast - {file}\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Average Price\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_graphs, file.replace('.csv','_tat_mha_weekly_graph.png')))\n",
    "    plt.close()\n",
    "\n",
    "    # cleanup\n",
    "    del model, X_all, X_train, X_val, y_all, y_train, y_val, preds_scaled, preds\n",
    "    gc.collect()\n",
    "\n",
    "# -----------------------------\n",
    "# Save metrics summary (all crops)\n",
    "# -----------------------------\n",
    "metrics_df = pd.DataFrame(metrics_list).round(2)\n",
    "metrics_df.to_csv(metrics_file, index=False)\n",
    "print(f\"\\nğŸ“Š All crops processed â€” metrics saved to {metrics_file}\")\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678969d2-c9aa-4e6f-88a0-309aee6e3ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TAT+MQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d52a592-38b6-42da-9bd3-d2058199f221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ Processing Crop File: capsicum_2010_2024_master_weekly.csv\n",
      "WARNING:tensorflow:From C:\\Users\\ravik\\AppData\\Roaming\\Python\\Python313\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:232: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "Epoch 1/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 124ms/step - loss: 0.5550 - mae: 0.5596 - val_loss: 0.2944 - val_mae: 0.5278 - learning_rate: 0.0010\n",
      "Epoch 2/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - loss: 0.0782 - mae: 0.2173 - val_loss: 0.0169 - val_mae: 0.0997 - learning_rate: 0.0010\n",
      "Epoch 3/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - loss: 0.0387 - mae: 0.1548 - val_loss: 0.0218 - val_mae: 0.1101 - learning_rate: 0.0010\n",
      "Epoch 4/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 0.0264 - mae: 0.1258 - val_loss: 0.0190 - val_mae: 0.1036 - learning_rate: 0.0010\n",
      "Epoch 5/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - loss: 0.0235 - mae: 0.1220 - val_loss: 0.0349 - val_mae: 0.1493 - learning_rate: 0.0010\n",
      "Epoch 6/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 0.0215 - mae: 0.1165 - val_loss: 0.0266 - val_mae: 0.1265 - learning_rate: 0.0010\n",
      "Epoch 7/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - loss: 0.0191 - mae: 0.1088 - val_loss: 0.0222 - val_mae: 0.1143 - learning_rate: 5.0000e-04\n",
      "Epoch 8/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - loss: 0.0190 - mae: 0.1073 - val_loss: 0.0229 - val_mae: 0.1172 - learning_rate: 5.0000e-04\n",
      "Epoch 9/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step - loss: 0.0175 - mae: 0.1013 - val_loss: 0.0175 - val_mae: 0.1018 - learning_rate: 5.0000e-04\n",
      "Epoch 10/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 0.0167 - mae: 0.1029 - val_loss: 0.0155 - val_mae: 0.0957 - learning_rate: 5.0000e-04\n",
      "Epoch 11/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 0.0145 - mae: 0.0934 - val_loss: 0.0131 - val_mae: 0.0869 - learning_rate: 5.0000e-04\n",
      "Epoch 12/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - loss: 0.0139 - mae: 0.0932 - val_loss: 0.0123 - val_mae: 0.0840 - learning_rate: 5.0000e-04\n",
      "Epoch 13/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 64ms/step - loss: 0.0102 - mae: 0.0793 - val_loss: 0.0114 - val_mae: 0.0790 - learning_rate: 5.0000e-04\n",
      "Epoch 14/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 63ms/step - loss: 0.0110 - mae: 0.0821 - val_loss: 0.0111 - val_mae: 0.0789 - learning_rate: 5.0000e-04\n",
      "Epoch 15/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - loss: 0.0093 - mae: 0.0740 - val_loss: 0.0106 - val_mae: 0.0775 - learning_rate: 5.0000e-04\n",
      "Epoch 16/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 0.0084 - mae: 0.0701 - val_loss: 0.0165 - val_mae: 0.1022 - learning_rate: 5.0000e-04\n",
      "Epoch 17/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - loss: 0.0101 - mae: 0.0786 - val_loss: 0.0174 - val_mae: 0.1064 - learning_rate: 5.0000e-04\n",
      "Epoch 18/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - loss: 0.0108 - mae: 0.0818 - val_loss: 0.0163 - val_mae: 0.1028 - learning_rate: 5.0000e-04\n",
      "Epoch 19/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - loss: 0.0107 - mae: 0.0819 - val_loss: 0.0208 - val_mae: 0.1209 - learning_rate: 5.0000e-04\n",
      "Epoch 20/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 0.0090 - mae: 0.0731 - val_loss: 0.0111 - val_mae: 0.0825 - learning_rate: 2.5000e-04\n",
      "Epoch 21/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - loss: 0.0066 - mae: 0.0626 - val_loss: 0.0110 - val_mae: 0.0822 - learning_rate: 2.5000e-04\n",
      "Epoch 22/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - loss: 0.0079 - mae: 0.0677 - val_loss: 0.0120 - val_mae: 0.0867 - learning_rate: 2.5000e-04\n",
      "Epoch 23/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - loss: 0.0067 - mae: 0.0640 - val_loss: 0.0108 - val_mae: 0.0806 - learning_rate: 2.5000e-04\n",
      "\u001b[1m24/24\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 56ms/step\n",
      "âœ… capsicum_2010_2024_master_weekly.csv | MAE=225.6, RMSE=300.93, RÂ²=0.8244, MAPE=9.21%, Accuracy=90.79%\n",
      "\n",
      "ğŸš€ Processing Crop File: onion_2010_2024_master_weekly.csv\n",
      "Epoch 1/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 66ms/step - loss: 0.2474 - mae: 0.3532 - val_loss: 0.0869 - val_mae: 0.2353 - learning_rate: 0.0010\n",
      "Epoch 2/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 0.0433 - mae: 0.1608 - val_loss: 0.0445 - val_mae: 0.1550 - learning_rate: 0.0010\n",
      "Epoch 3/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.0241 - mae: 0.1206 - val_loss: 0.0747 - val_mae: 0.2234 - learning_rate: 0.0010\n",
      "Epoch 4/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0222 - mae: 0.1127 - val_loss: 0.0312 - val_mae: 0.1382 - learning_rate: 0.0010\n",
      "Epoch 5/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 0.0182 - mae: 0.1019 - val_loss: 0.0447 - val_mae: 0.1577 - learning_rate: 0.0010\n",
      "Epoch 6/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 0.0181 - mae: 0.0972 - val_loss: 0.0316 - val_mae: 0.1368 - learning_rate: 0.0010\n",
      "Epoch 7/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 0.0164 - mae: 0.0965 - val_loss: 0.0359 - val_mae: 0.1439 - learning_rate: 0.0010\n",
      "Epoch 8/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 0.0172 - mae: 0.0955 - val_loss: 0.0243 - val_mae: 0.1205 - learning_rate: 0.0010\n",
      "Epoch 9/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.0128 - mae: 0.0837 - val_loss: 0.0251 - val_mae: 0.1214 - learning_rate: 0.0010\n",
      "Epoch 10/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 0.0108 - mae: 0.0745 - val_loss: 0.0108 - val_mae: 0.0830 - learning_rate: 0.0010\n",
      "Epoch 11/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 0.0095 - mae: 0.0723 - val_loss: 0.0120 - val_mae: 0.0866 - learning_rate: 0.0010\n",
      "Epoch 12/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0082 - mae: 0.0658 - val_loss: 0.0184 - val_mae: 0.1108 - learning_rate: 0.0010\n",
      "Epoch 13/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.0073 - mae: 0.0606 - val_loss: 0.0059 - val_mae: 0.0576 - learning_rate: 0.0010\n",
      "Epoch 14/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.0061 - mae: 0.0533 - val_loss: 0.0057 - val_mae: 0.0554 - learning_rate: 0.0010\n",
      "Epoch 15/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 0.0052 - mae: 0.0535 - val_loss: 0.0054 - val_mae: 0.0599 - learning_rate: 0.0010\n",
      "Epoch 16/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - loss: 0.0049 - mae: 0.0495 - val_loss: 0.0053 - val_mae: 0.0532 - learning_rate: 0.0010\n",
      "Epoch 17/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - loss: 0.0053 - mae: 0.0514 - val_loss: 0.0045 - val_mae: 0.0487 - learning_rate: 0.0010\n",
      "Epoch 18/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - loss: 0.0048 - mae: 0.0487 - val_loss: 0.0047 - val_mae: 0.0482 - learning_rate: 0.0010\n",
      "Epoch 19/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - loss: 0.0038 - mae: 0.0447 - val_loss: 0.0046 - val_mae: 0.0530 - learning_rate: 0.0010\n",
      "Epoch 20/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - loss: 0.0044 - mae: 0.0474 - val_loss: 0.0045 - val_mae: 0.0562 - learning_rate: 0.0010\n",
      "Epoch 21/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 0.0038 - mae: 0.0451 - val_loss: 0.0058 - val_mae: 0.0557 - learning_rate: 0.0010\n",
      "Epoch 22/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 0.0039 - mae: 0.0444 - val_loss: 0.0047 - val_mae: 0.0594 - learning_rate: 5.0000e-04\n",
      "Epoch 23/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 0.0034 - mae: 0.0419 - val_loss: 0.0036 - val_mae: 0.0512 - learning_rate: 5.0000e-04\n",
      "Epoch 24/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - loss: 0.0034 - mae: 0.0403 - val_loss: 0.0030 - val_mae: 0.0450 - learning_rate: 5.0000e-04\n",
      "Epoch 25/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 0.0033 - mae: 0.0397 - val_loss: 0.0043 - val_mae: 0.0570 - learning_rate: 5.0000e-04\n",
      "Epoch 26/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - loss: 0.0029 - mae: 0.0384 - val_loss: 0.0044 - val_mae: 0.0571 - learning_rate: 5.0000e-04\n",
      "Epoch 27/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - loss: 0.0032 - mae: 0.0401 - val_loss: 0.0034 - val_mae: 0.0508 - learning_rate: 5.0000e-04\n",
      "Epoch 28/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - loss: 0.0028 - mae: 0.0375 - val_loss: 0.0036 - val_mae: 0.0483 - learning_rate: 5.0000e-04\n",
      "Epoch 29/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - loss: 0.0032 - mae: 0.0397 - val_loss: 0.0027 - val_mae: 0.0408 - learning_rate: 2.5000e-04\n",
      "Epoch 30/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - loss: 0.0026 - mae: 0.0353 - val_loss: 0.0037 - val_mae: 0.0506 - learning_rate: 2.5000e-04\n",
      "Epoch 31/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - loss: 0.0028 - mae: 0.0358 - val_loss: 0.0027 - val_mae: 0.0438 - learning_rate: 2.5000e-04\n",
      "Epoch 32/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - loss: 0.0024 - mae: 0.0333 - val_loss: 0.0038 - val_mae: 0.0525 - learning_rate: 2.5000e-04\n",
      "Epoch 33/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 0.0027 - mae: 0.0362 - val_loss: 0.0040 - val_mae: 0.0557 - learning_rate: 2.5000e-04\n",
      "Epoch 34/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.0027 - mae: 0.0358 - val_loss: 0.0036 - val_mae: 0.0528 - learning_rate: 1.2500e-04\n",
      "Epoch 35/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.0023 - mae: 0.0360 - val_loss: 0.0028 - val_mae: 0.0462 - learning_rate: 1.2500e-04\n",
      "Epoch 36/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 0.0022 - mae: 0.0332 - val_loss: 0.0028 - val_mae: 0.0460 - learning_rate: 1.2500e-04\n",
      "Epoch 37/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 0.0027 - mae: 0.0359 - val_loss: 0.0036 - val_mae: 0.0522 - learning_rate: 1.2500e-04\n",
      "\u001b[1m24/24\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step \n",
      "âœ… onion_2010_2024_master_weekly.csv | MAE=90.38, RMSE=132.24, RÂ²=0.9604, MAPE=5.11%, Accuracy=94.89%\n",
      "\n",
      "ğŸš€ Processing Crop File: tomato_2010_2024_master_weekly.csv\n",
      "Epoch 1/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 153ms/step - loss: 0.5072 - mae: 0.5573 - val_loss: 0.1067 - val_mae: 0.2671 - learning_rate: 0.0010\n",
      "Epoch 2/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 0.0960 - mae: 0.2384 - val_loss: 0.0338 - val_mae: 0.1284 - learning_rate: 0.0010\n",
      "Epoch 3/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 0.0301 - mae: 0.1387 - val_loss: 0.0754 - val_mae: 0.2067 - learning_rate: 0.0010\n",
      "Epoch 4/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - loss: 0.0143 - mae: 0.0890 - val_loss: 0.0482 - val_mae: 0.1368 - learning_rate: 0.0010\n",
      "Epoch 5/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 0.0110 - mae: 0.0795 - val_loss: 0.0476 - val_mae: 0.1354 - learning_rate: 0.0010\n",
      "Epoch 6/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - loss: 0.0100 - mae: 0.0719 - val_loss: 0.0483 - val_mae: 0.1375 - learning_rate: 0.0010\n",
      "Epoch 7/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - loss: 0.0096 - mae: 0.0707 - val_loss: 0.0457 - val_mae: 0.1316 - learning_rate: 5.0000e-04\n",
      "Epoch 8/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - loss: 0.0092 - mae: 0.0685 - val_loss: 0.0433 - val_mae: 0.1269 - learning_rate: 5.0000e-04\n",
      "Epoch 9/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - loss: 0.0091 - mae: 0.0691 - val_loss: 0.0410 - val_mae: 0.1239 - learning_rate: 5.0000e-04\n",
      "Epoch 10/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - loss: 0.0087 - mae: 0.0669 - val_loss: 0.0419 - val_mae: 0.1257 - learning_rate: 5.0000e-04\n",
      "\u001b[1m24/24\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step\n",
      "âœ… tomato_2010_2024_master_weekly.csv | MAE=854.12, RMSE=966.54, RÂ²=-0.558, MAPE=94.64%, Accuracy=5.36%\n",
      "\n",
      "ğŸš€ Processing Crop File: wheat_2010_2024_master_weekly.csv\n",
      "Epoch 1/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 149ms/step - loss: 0.4564 - mae: 0.4687 - val_loss: 0.0586 - val_mae: 0.2374 - learning_rate: 0.0010\n",
      "Epoch 2/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - loss: 0.0504 - mae: 0.1722 - val_loss: 0.0425 - val_mae: 0.1911 - learning_rate: 0.0010\n",
      "Epoch 3/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - loss: 0.0133 - mae: 0.0913 - val_loss: 0.0150 - val_mae: 0.1034 - learning_rate: 0.0010\n",
      "Epoch 4/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - loss: 0.0086 - mae: 0.0746 - val_loss: 0.0387 - val_mae: 0.1889 - learning_rate: 0.0010\n",
      "Epoch 5/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 0.0113 - mae: 0.0847 - val_loss: 0.0182 - val_mae: 0.1256 - learning_rate: 0.0010\n",
      "Epoch 6/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 0.0072 - mae: 0.0669 - val_loss: 0.0116 - val_mae: 0.0972 - learning_rate: 0.0010\n",
      "Epoch 7/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 0.0046 - mae: 0.0533 - val_loss: 0.0045 - val_mae: 0.0500 - learning_rate: 0.0010\n",
      "Epoch 8/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - loss: 0.0047 - mae: 0.0531 - val_loss: 0.0061 - val_mae: 0.0639 - learning_rate: 0.0010\n",
      "Epoch 9/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 0.0047 - mae: 0.0540 - val_loss: 0.0077 - val_mae: 0.0749 - learning_rate: 0.0010\n",
      "Epoch 10/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - loss: 0.0036 - mae: 0.0476 - val_loss: 0.0042 - val_mae: 0.0472 - learning_rate: 0.0010\n",
      "Epoch 11/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 82ms/step - loss: 0.0040 - mae: 0.0486 - val_loss: 0.0034 - val_mae: 0.0399 - learning_rate: 0.0010\n",
      "Epoch 12/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - loss: 0.0039 - mae: 0.0487 - val_loss: 0.0026 - val_mae: 0.0336 - learning_rate: 0.0010\n",
      "Epoch 13/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 0.0039 - mae: 0.0485 - val_loss: 0.0043 - val_mae: 0.0502 - learning_rate: 0.0010\n",
      "Epoch 14/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - loss: 0.0038 - mae: 0.0461 - val_loss: 0.0059 - val_mae: 0.0648 - learning_rate: 0.0010\n",
      "Epoch 15/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - loss: 0.0041 - mae: 0.0493 - val_loss: 0.0018 - val_mae: 0.0252 - learning_rate: 0.0010\n",
      "Epoch 16/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - loss: 0.0047 - mae: 0.0535 - val_loss: 0.0036 - val_mae: 0.0452 - learning_rate: 0.0010\n",
      "Epoch 17/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 0.0035 - mae: 0.0463 - val_loss: 0.0060 - val_mae: 0.0651 - learning_rate: 0.0010\n",
      "Epoch 18/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 0.0039 - mae: 0.0473 - val_loss: 0.0037 - val_mae: 0.0446 - learning_rate: 0.0010\n",
      "Epoch 19/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - loss: 0.0037 - mae: 0.0471 - val_loss: 0.0069 - val_mae: 0.0709 - learning_rate: 0.0010\n",
      "Epoch 20/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - loss: 0.0037 - mae: 0.0459 - val_loss: 0.0027 - val_mae: 0.0346 - learning_rate: 5.0000e-04\n",
      "Epoch 21/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - loss: 0.0033 - mae: 0.0433 - val_loss: 0.0052 - val_mae: 0.0580 - learning_rate: 5.0000e-04\n",
      "Epoch 22/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 0.0033 - mae: 0.0447 - val_loss: 0.0059 - val_mae: 0.0630 - learning_rate: 5.0000e-04\n",
      "Epoch 23/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 0.0038 - mae: 0.0469 - val_loss: 0.0019 - val_mae: 0.0284 - learning_rate: 5.0000e-04\n",
      "\u001b[1m24/24\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step  \n",
      "âœ… wheat_2010_2024_master_weekly.csv | MAE=91.51, RMSE=113.65, RÂ²=0.9474, MAPE=4.22%, Accuracy=95.78%\n",
      "\n",
      "ğŸ“Š All crops processed â€” metrics saved to tat_mqa_metrics_weekly.csv\n",
      "                               Crop     MAE    RMSE    R2  MAPE(%)  \\\n",
      "0  capsicum_2010_2024_master_weekly  225.60  300.93  0.82     9.21   \n",
      "1     onion_2010_2024_master_weekly   90.38  132.24  0.96     5.11   \n",
      "2    tomato_2010_2024_master_weekly  854.12  966.54 -0.56    94.64   \n",
      "3     wheat_2010_2024_master_weekly   91.51  113.65  0.95     4.22   \n",
      "\n",
      "   Accuracy(%)  \n",
      "0        90.79  \n",
      "1        94.89  \n",
      "2         5.36  \n",
      "3        95.78  \n"
     ]
    }
   ],
   "source": [
    "import os, gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers, callbacks\n",
    "\n",
    "# -----------------------------\n",
    "# Paths / outputs\n",
    "# -----------------------------\n",
    "input_folder = \"dataset\"\n",
    "output_models = \"tat_mqa_output_models_weekly\"\n",
    "output_csv = \"tat_mqa_output_csv_weekly\"\n",
    "output_graphs = \"tat_mqa_output_graphs_weekly\"\n",
    "metrics_file = \"tat_mqa_metrics_weekly.csv\"\n",
    "\n",
    "os.makedirs(output_models, exist_ok=True)\n",
    "os.makedirs(output_csv, exist_ok=True)\n",
    "os.makedirs(output_graphs, exist_ok=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Config (tweak if needed)\n",
    "# -----------------------------\n",
    "look_back = 30       # weeks of history used as input\n",
    "batch_size = 32\n",
    "epochs = 60\n",
    "val_split = 0.15\n",
    "d_model = 64\n",
    "num_heads = 4        # used for key/value dim split if needed, MQA uses multiple Q projections\n",
    "num_q_heads = 4      # how many different Q projections (multi-query)\n",
    "ff_dim = 128\n",
    "dropout_rate = 0.15\n",
    "seed = 42\n",
    "\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "def parse_dates_safe(s):\n",
    "    try:\n",
    "        return pd.to_datetime(s, dayfirst=True)\n",
    "    except:\n",
    "        return pd.to_datetime(s, dayfirst=False)\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    mask = y_true != 0\n",
    "    if mask.sum() == 0:\n",
    "        return np.nan\n",
    "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
    "\n",
    "def create_multivariate_dataset(y_scaled, exog_scaled, look_back):\n",
    "    n = len(y_scaled)\n",
    "    k = exog_scaled.shape[1] if exog_scaled is not None else 0\n",
    "    X, Y = [], []\n",
    "    for i in range(n - look_back):\n",
    "        seq_y = y_scaled[i:i+look_back, 0]\n",
    "        if k > 0:\n",
    "            seq_exog = exog_scaled[i:i+look_back, :]\n",
    "            seq = np.concatenate([seq_y.reshape(-1,1), seq_exog], axis=1)\n",
    "        else:\n",
    "            seq = seq_y.reshape(-1,1)\n",
    "        X.append(seq)\n",
    "        Y.append(y_scaled[i+look_back, 0])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "def positional_encoding(seq_len, d_model):\n",
    "    pos = np.arange(seq_len)[:, np.newaxis]\n",
    "    i = np.arange(d_model)[np.newaxis, :]\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    angle_rads = pos * angle_rates\n",
    "    pe = np.zeros_like(angle_rads)\n",
    "    pe[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    pe[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    return tf.cast(pe, tf.float32)\n",
    "\n",
    "# -----------------------------\n",
    "# Multi-Query Attention (MQA) Layer\n",
    "# -----------------------------\n",
    "class MultiQueryAttention(layers.Layer):\n",
    "    def __init__(self, num_q_heads=4, key_dim=64, dropout_rate=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_q_heads = num_q_heads\n",
    "        self.key_dim = key_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "        # we'll create Q dense layers list, and shared K,V\n",
    "        self.q_layers = [layers.Dense(key_dim) for _ in range(num_q_heads)]\n",
    "        self.k_dense = layers.Dense(key_dim)\n",
    "        self.v_dense = layers.Dense(key_dim)\n",
    "        self.out_dense = layers.Dense(key_dim)\n",
    "        self.dropout = layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        # x: (batch, seq_len, d_model)\n",
    "        K = self.k_dense(x)  # (B, S, key_dim)\n",
    "        V = self.v_dense(x)  # (B, S, key_dim)\n",
    "        head_outputs = []\n",
    "        for q_layer in self.q_layers:\n",
    "            Q = q_layer(x)  # (B, S, key_dim)\n",
    "            # scaled dot-product: Q @ K^T\n",
    "            scores = tf.matmul(Q, K, transpose_b=True) / tf.math.sqrt(tf.cast(self.key_dim, tf.float32))\n",
    "            attn = tf.nn.softmax(scores, axis=-1)  # (B, S, S)\n",
    "            out = tf.matmul(attn, V)  # (B, S, key_dim)\n",
    "            head_outputs.append(out)\n",
    "        concat = tf.concat(head_outputs, axis=-1)  # (B, S, key_dim * num_q_heads)\n",
    "        proj = self.out_dense(concat)               # project back to key_dim (or d_model)\n",
    "        proj = self.dropout(proj, training=training)\n",
    "        return proj  # (B, S, key_dim)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"num_q_heads\": self.num_q_heads,\n",
    "            \"key_dim\": self.key_dim,\n",
    "            \"dropout_rate\": self.dropout_rate\n",
    "        })\n",
    "        return config\n",
    "\n",
    "# -----------------------------\n",
    "# Build TAT + MQA model\n",
    "# -----------------------------\n",
    "def build_tat_mqa_model(input_shape, d_model=64, num_q_heads=4, ff_dim=128, dropout_rate=0.15):\n",
    "    seq_len, num_feat = input_shape\n",
    "    inp = layers.Input(shape=(seq_len, num_feat))  # (B,S,F)\n",
    "    # project input features to d_model for residual compatibility\n",
    "    x = layers.Dense(d_model)(inp)                # (B,S,d_model)\n",
    "    # add positional enc\n",
    "    pe = positional_encoding(seq_len, d_model)\n",
    "    x = x + pe\n",
    "\n",
    "    # Multi-Query Attention block\n",
    "    mqa = MultiQueryAttention(num_q_heads=num_q_heads, key_dim=d_model, dropout_rate=dropout_rate)\n",
    "    attn_out = mqa(x)                              # (B,S,d_model)\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x + attn_out)\n",
    "\n",
    "    # Feed-forward\n",
    "    ff = layers.Dense(ff_dim, activation='relu')(x)\n",
    "    ff = layers.Dense(d_model)(ff)\n",
    "    ff = layers.Dropout(dropout_rate)(ff)\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x + ff)\n",
    "\n",
    "    # Optional second MHA block (temporal)\n",
    "    attn2 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)(x, x)\n",
    "    attn2 = layers.Dropout(dropout_rate)(attn2)\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x + attn2)\n",
    "    ff2 = layers.Dense(ff_dim, activation='relu')(x)\n",
    "    ff2 = layers.Dense(d_model)(ff2)\n",
    "    ff2 = layers.Dropout(dropout_rate)(ff2)\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x + ff2)\n",
    "\n",
    "    # Pool and output\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dense(ff_dim//2, activation='relu')(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    out = layers.Dense(1)(x)\n",
    "\n",
    "    model = models.Model(inputs=inp, outputs=out)\n",
    "    model.compile(optimizer=optimizers.Adam(1e-3), loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# -----------------------------\n",
    "# Main loop â€” process weekly crop files\n",
    "# -----------------------------\n",
    "metrics_list = []\n",
    "\n",
    "for file in sorted(os.listdir(input_folder)):\n",
    "    if not file.endswith(\".csv\"):\n",
    "        continue\n",
    "    print(f\"\\nğŸš€ Processing Crop File: {file}\")\n",
    "    file_path = os.path.join(input_folder, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # parse dates and sort\n",
    "    df['Date'] = parse_dates_safe(df['Date'])\n",
    "    df = df.sort_values('Date')\n",
    "    df = df.groupby('Date', as_index=False).mean(numeric_only=True)\n",
    "    df.set_index('Date', inplace=True)\n",
    "\n",
    "    # weekly frequency\n",
    "    df = df.asfreq('W')\n",
    "\n",
    "    # ensure Average Price exists and fill\n",
    "    if 'Average Price' not in df.columns:\n",
    "        print(f\"âš  'Average Price' not found in {file}. Skipping.\")\n",
    "        continue\n",
    "    df['Average Price'] = df['Average Price'].ffill().bfill().fillna(df['Average Price'].mean())\n",
    "\n",
    "    # feature engineering (weekly)\n",
    "    df['Lag_1'] = df['Average Price'].shift(1)\n",
    "    df['Lag_4'] = df['Average Price'].shift(4)\n",
    "    df['MA_4'] = df['Average Price'].rolling(window=4).mean()\n",
    "    df['MA_12'] = df['Average Price'].rolling(window=12).mean()\n",
    "    df[['Lag_1','Lag_4','MA_4','MA_12']] = df[['Lag_1','Lag_4','MA_4','MA_12']].bfill().ffill()\n",
    "\n",
    "    # prepare arrays & scalers\n",
    "    y = df[['Average Price']].values.astype('float32')\n",
    "    exog_cols = ['Lag_1','Lag_4','MA_4','MA_12']\n",
    "    exog = df[exog_cols].values.astype('float32')\n",
    "\n",
    "    y_scaler = MinMaxScaler()\n",
    "    exog_scaler = MinMaxScaler()\n",
    "    y_scaled = y_scaler.fit_transform(y)\n",
    "    exog_scaled = exog_scaler.fit_transform(exog)\n",
    "\n",
    "    # create sequences\n",
    "    X_all, y_all = create_multivariate_dataset(y_scaled, exog_scaled, look_back)\n",
    "    if len(X_all) == 0:\n",
    "        print(f\"âš  Not enough data after look_back={look_back} in {file}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # train/val split\n",
    "    val_size = int(len(X_all) * val_split)\n",
    "    train_size = len(X_all) - val_size\n",
    "    X_train, y_train = X_all[:train_size], y_all[:train_size]\n",
    "    X_val, y_val = X_all[train_size:], y_all[train_size:]\n",
    "\n",
    "    # build & train model\n",
    "    input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "    model = build_tat_mqa_model(input_shape, d_model=d_model, num_q_heads=num_q_heads, ff_dim=ff_dim, dropout_rate=dropout_rate)\n",
    "\n",
    "    es = callbacks.EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True)\n",
    "    rl = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, min_lr=1e-6)\n",
    "\n",
    "    history = model.fit(X_train, y_train,\n",
    "                        validation_data=(X_val, y_val),\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        callbacks=[es, rl],\n",
    "                        verbose=1)\n",
    "\n",
    "    # predict on all sequences and align\n",
    "    preds_scaled = model.predict(X_all, batch_size=batch_size)\n",
    "    preds = y_scaler.inverse_transform(preds_scaled).flatten()\n",
    "\n",
    "    df['Predicted'] = np.nan\n",
    "    start_idx = look_back\n",
    "    df.iloc[start_idx:start_idx + len(preds), df.columns.get_loc('Predicted')] = preds\n",
    "\n",
    "    # round numeric columns to 2 decimals\n",
    "    for col in ['Average Price','Predicted','Lag_1','Lag_4','MA_4','MA_12']:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].round(2)\n",
    "\n",
    "    # compute metrics on rows with predictions\n",
    "    mask = ~np.isnan(df['Predicted'].values)\n",
    "    y_true = df.loc[mask, 'Average Price'].values\n",
    "    y_pred = df.loc[mask, 'Predicted'].values\n",
    "\n",
    "    mae = round(mean_absolute_error(y_true, y_pred), 2)\n",
    "    rmse = round(np.sqrt(mean_squared_error(y_true, y_pred)), 2)\n",
    "    r2 = round(r2_score(y_true, y_pred), 4)\n",
    "    mape = round(mean_absolute_percentage_error(y_true, y_pred), 2)\n",
    "    accuracy = round(100 - mape, 2)\n",
    "\n",
    "    metrics_list.append({\n",
    "        'Crop': file.replace('.csv',''),\n",
    "        'MAE': mae,\n",
    "        'RMSE': rmse,\n",
    "        'R2': r2,\n",
    "        'MAPE(%)': mape,\n",
    "        'Accuracy(%)': accuracy\n",
    "    })\n",
    "\n",
    "    print(f\"âœ… {file} | MAE={mae}, RMSE={rmse}, RÂ²={r2}, MAPE={mape}%, Accuracy={accuracy}%\")\n",
    "\n",
    "    # save model, CSV, graph\n",
    "    model_save_path = os.path.join(output_models, file.replace('.csv','_tat_mqa_weekly.keras'))\n",
    "    model.save(model_save_path)\n",
    "\n",
    "    df_reset = df.reset_index()\n",
    "    df_reset.to_csv(os.path.join(output_csv, file.replace('.csv','_tat_mqa_weekly_updated.csv')), index=False, float_format='%.2f')\n",
    "\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.plot(df_reset['Date'], df_reset['Average Price'], label='Actual', color='blue')\n",
    "    plt.plot(df_reset['Date'], df_reset['Predicted'], label='Predicted (TAT+MQA)', color='red', linestyle='dashed')\n",
    "    plt.plot(df_reset['Date'], df_reset['MA_4'], label='MA_4', color='green', linestyle='--')\n",
    "    plt.plot(df_reset['Date'], df_reset['MA_12'], label='MA_12', color='orange', linestyle='--')\n",
    "    plt.title(f\"TAT+MQA Weekly Forecast - {file}\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Average Price\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_graphs, file.replace('.csv','_tat_mqa_weekly_graph.png')))\n",
    "    plt.close()\n",
    "\n",
    "    # cleanup\n",
    "    del model, X_all, X_train, X_val, y_all, y_train, y_val, preds_scaled, preds\n",
    "    gc.collect()\n",
    "\n",
    "# save consolidated metrics\n",
    "metrics_df = pd.DataFrame(metrics_list).round(2)\n",
    "metrics_df.to_csv(metrics_file, index=False)\n",
    "print(f\"\\nğŸ“Š All crops processed â€” metrics saved to {metrics_file}\")\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b9afe9-4251-434c-a45f-e00c07438e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TAT+GQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6e11f9e-0393-4af9-a014-868c05cc87ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ Processing Crop File: capsicum_2010_2024_master_weekly.csv\n",
      "Epoch 1/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 125ms/step - loss: 0.1826 - mae: 0.3188 - val_loss: 0.0651 - val_mae: 0.2220 - learning_rate: 0.0010\n",
      "Epoch 2/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 0.0387 - mae: 0.1596 - val_loss: 0.0909 - val_mae: 0.2738 - learning_rate: 0.0010\n",
      "Epoch 3/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 0.0260 - mae: 0.1288 - val_loss: 0.0559 - val_mae: 0.1998 - learning_rate: 0.0010\n",
      "Epoch 4/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - loss: 0.0222 - mae: 0.1205 - val_loss: 0.0310 - val_mae: 0.1369 - learning_rate: 0.0010\n",
      "Epoch 5/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 0.0194 - mae: 0.1098 - val_loss: 0.0165 - val_mae: 0.1011 - learning_rate: 0.0010\n",
      "Epoch 6/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - loss: 0.0201 - mae: 0.1132 - val_loss: 0.0160 - val_mae: 0.1020 - learning_rate: 0.0010\n",
      "Epoch 7/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - loss: 0.0213 - mae: 0.1154 - val_loss: 0.0290 - val_mae: 0.1348 - learning_rate: 0.0010\n",
      "Epoch 8/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 0.0165 - mae: 0.1006 - val_loss: 0.0244 - val_mae: 0.1248 - learning_rate: 0.0010\n",
      "Epoch 9/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 0.0141 - mae: 0.0922 - val_loss: 0.0147 - val_mae: 0.0891 - learning_rate: 0.0010\n",
      "Epoch 10/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 0.0112 - mae: 0.0820 - val_loss: 0.0147 - val_mae: 0.0920 - learning_rate: 0.0010\n",
      "Epoch 11/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 0.0091 - mae: 0.0728 - val_loss: 0.0178 - val_mae: 0.1052 - learning_rate: 0.0010\n",
      "Epoch 12/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 0.0071 - mae: 0.0662 - val_loss: 0.0102 - val_mae: 0.0762 - learning_rate: 0.0010\n",
      "Epoch 13/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 0.0075 - mae: 0.0657 - val_loss: 0.0093 - val_mae: 0.0737 - learning_rate: 0.0010\n",
      "Epoch 14/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 0.0065 - mae: 0.0616 - val_loss: 0.0137 - val_mae: 0.0937 - learning_rate: 0.0010\n",
      "Epoch 15/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 0.0069 - mae: 0.0630 - val_loss: 0.0090 - val_mae: 0.0743 - learning_rate: 0.0010\n",
      "Epoch 16/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 0.0067 - mae: 0.0624 - val_loss: 0.0081 - val_mae: 0.0692 - learning_rate: 0.0010\n",
      "Epoch 17/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 0.0060 - mae: 0.0593 - val_loss: 0.0171 - val_mae: 0.1100 - learning_rate: 0.0010\n",
      "Epoch 18/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 0.0088 - mae: 0.0730 - val_loss: 0.0093 - val_mae: 0.0752 - learning_rate: 0.0010\n",
      "Epoch 19/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 0.0064 - mae: 0.0616 - val_loss: 0.0101 - val_mae: 0.0785 - learning_rate: 0.0010\n",
      "Epoch 20/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 0.0068 - mae: 0.0646 - val_loss: 0.0127 - val_mae: 0.0890 - learning_rate: 0.0010\n",
      "Epoch 21/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - loss: 0.0079 - mae: 0.0698 - val_loss: 0.0071 - val_mae: 0.0645 - learning_rate: 5.0000e-04\n",
      "Epoch 22/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - loss: 0.0053 - mae: 0.0566 - val_loss: 0.0065 - val_mae: 0.0614 - learning_rate: 5.0000e-04\n",
      "Epoch 23/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 0.0055 - mae: 0.0567 - val_loss: 0.0065 - val_mae: 0.0611 - learning_rate: 5.0000e-04\n",
      "Epoch 24/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 0.0052 - mae: 0.0544 - val_loss: 0.0062 - val_mae: 0.0602 - learning_rate: 5.0000e-04\n",
      "Epoch 25/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - loss: 0.0055 - mae: 0.0563 - val_loss: 0.0081 - val_mae: 0.0682 - learning_rate: 5.0000e-04\n",
      "Epoch 26/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 0.0055 - mae: 0.0566 - val_loss: 0.0095 - val_mae: 0.0756 - learning_rate: 5.0000e-04\n",
      "Epoch 27/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - loss: 0.0063 - mae: 0.0623 - val_loss: 0.0065 - val_mae: 0.0601 - learning_rate: 5.0000e-04\n",
      "Epoch 28/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 0.0053 - mae: 0.0560 - val_loss: 0.0104 - val_mae: 0.0799 - learning_rate: 5.0000e-04\n",
      "Epoch 29/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 0.0059 - mae: 0.0587 - val_loss: 0.0070 - val_mae: 0.0624 - learning_rate: 2.5000e-04\n",
      "Epoch 30/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 0.0047 - mae: 0.0531 - val_loss: 0.0062 - val_mae: 0.0619 - learning_rate: 2.5000e-04\n",
      "Epoch 31/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - loss: 0.0052 - mae: 0.0538 - val_loss: 0.0066 - val_mae: 0.0605 - learning_rate: 2.5000e-04\n",
      "Epoch 32/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 0.0044 - mae: 0.0518 - val_loss: 0.0060 - val_mae: 0.0570 - learning_rate: 2.5000e-04\n",
      "Epoch 33/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 0.0048 - mae: 0.0531 - val_loss: 0.0069 - val_mae: 0.0622 - learning_rate: 2.5000e-04\n",
      "Epoch 34/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - loss: 0.0055 - mae: 0.0563 - val_loss: 0.0058 - val_mae: 0.0560 - learning_rate: 2.5000e-04\n",
      "Epoch 35/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 0.0044 - mae: 0.0502 - val_loss: 0.0056 - val_mae: 0.0552 - learning_rate: 2.5000e-04\n",
      "Epoch 36/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - loss: 0.0048 - mae: 0.0540 - val_loss: 0.0056 - val_mae: 0.0551 - learning_rate: 2.5000e-04\n",
      "Epoch 37/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 0.0047 - mae: 0.0531 - val_loss: 0.0053 - val_mae: 0.0550 - learning_rate: 2.5000e-04\n",
      "Epoch 38/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - loss: 0.0044 - mae: 0.0505 - val_loss: 0.0060 - val_mae: 0.0568 - learning_rate: 2.5000e-04\n",
      "Epoch 39/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 0.0043 - mae: 0.0505 - val_loss: 0.0073 - val_mae: 0.0642 - learning_rate: 2.5000e-04\n",
      "Epoch 40/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - loss: 0.0051 - mae: 0.0535 - val_loss: 0.0057 - val_mae: 0.0556 - learning_rate: 2.5000e-04\n",
      "Epoch 41/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 0.0040 - mae: 0.0494 - val_loss: 0.0057 - val_mae: 0.0553 - learning_rate: 2.5000e-04\n",
      "Epoch 42/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 0.0050 - mae: 0.0511 - val_loss: 0.0056 - val_mae: 0.0550 - learning_rate: 1.2500e-04\n",
      "Epoch 43/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - loss: 0.0040 - mae: 0.0469 - val_loss: 0.0057 - val_mae: 0.0550 - learning_rate: 1.2500e-04\n",
      "Epoch 44/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - loss: 0.0039 - mae: 0.0465 - val_loss: 0.0052 - val_mae: 0.0540 - learning_rate: 1.2500e-04\n",
      "Epoch 45/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 0.0039 - mae: 0.0486 - val_loss: 0.0059 - val_mae: 0.0561 - learning_rate: 1.2500e-04\n",
      "Epoch 46/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - loss: 0.0046 - mae: 0.0518 - val_loss: 0.0061 - val_mae: 0.0574 - learning_rate: 1.2500e-04\n",
      "Epoch 47/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - loss: 0.0039 - mae: 0.0475 - val_loss: 0.0056 - val_mae: 0.0543 - learning_rate: 1.2500e-04\n",
      "Epoch 48/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 0.0046 - mae: 0.0499 - val_loss: 0.0052 - val_mae: 0.0530 - learning_rate: 1.2500e-04\n",
      "Epoch 49/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - loss: 0.0037 - mae: 0.0472 - val_loss: 0.0052 - val_mae: 0.0531 - learning_rate: 6.2500e-05\n",
      "Epoch 50/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - loss: 0.0044 - mae: 0.0495 - val_loss: 0.0052 - val_mae: 0.0531 - learning_rate: 6.2500e-05\n",
      "Epoch 51/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - loss: 0.0040 - mae: 0.0484 - val_loss: 0.0054 - val_mae: 0.0535 - learning_rate: 6.2500e-05\n",
      "Epoch 52/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - loss: 0.0041 - mae: 0.0493 - val_loss: 0.0053 - val_mae: 0.0532 - learning_rate: 6.2500e-05\n",
      "\u001b[1m24/24\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step  \n",
      "âœ… capsicum_2010_2024_master_weekly.csv | MAE=145.11, RMSE=194.65, RÂ²=0.9265, MAPE=5.71%, Accuracy=94.29%\n",
      "\n",
      "ğŸš€ Processing Crop File: onion_2010_2024_master_weekly.csv\n",
      "Epoch 1/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 100ms/step - loss: 0.5322 - mae: 0.5541 - val_loss: 0.0552 - val_mae: 0.1728 - learning_rate: 0.0010\n",
      "Epoch 2/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 0.0582 - mae: 0.1893 - val_loss: 0.0480 - val_mae: 0.1595 - learning_rate: 0.0010\n",
      "Epoch 3/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 0.0358 - mae: 0.1469 - val_loss: 0.0567 - val_mae: 0.1756 - learning_rate: 0.0010\n",
      "Epoch 4/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 0.0246 - mae: 0.1178 - val_loss: 0.0475 - val_mae: 0.1600 - learning_rate: 0.0010\n",
      "Epoch 5/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - loss: 0.0221 - mae: 0.1108 - val_loss: 0.0410 - val_mae: 0.1501 - learning_rate: 0.0010\n",
      "Epoch 6/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - loss: 0.0196 - mae: 0.1032 - val_loss: 0.0338 - val_mae: 0.1394 - learning_rate: 0.0010\n",
      "Epoch 7/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 0.0172 - mae: 0.0953 - val_loss: 0.0249 - val_mae: 0.1295 - learning_rate: 0.0010\n",
      "Epoch 8/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 0.0142 - mae: 0.0867 - val_loss: 0.0235 - val_mae: 0.1221 - learning_rate: 0.0010\n",
      "Epoch 9/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 0.0144 - mae: 0.0873 - val_loss: 0.0160 - val_mae: 0.1061 - learning_rate: 0.0010\n",
      "Epoch 10/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 0.0105 - mae: 0.0729 - val_loss: 0.0143 - val_mae: 0.0947 - learning_rate: 0.0010\n",
      "Epoch 11/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - loss: 0.0081 - mae: 0.0646 - val_loss: 0.0085 - val_mae: 0.0703 - learning_rate: 0.0010\n",
      "Epoch 12/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 0.0079 - mae: 0.0662 - val_loss: 0.0066 - val_mae: 0.0592 - learning_rate: 0.0010\n",
      "Epoch 13/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - loss: 0.0058 - mae: 0.0548 - val_loss: 0.0054 - val_mae: 0.0540 - learning_rate: 0.0010\n",
      "Epoch 14/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 0.0057 - mae: 0.0585 - val_loss: 0.0047 - val_mae: 0.0519 - learning_rate: 0.0010\n",
      "Epoch 15/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 0.0049 - mae: 0.0495 - val_loss: 0.0041 - val_mae: 0.0491 - learning_rate: 0.0010\n",
      "Epoch 16/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 0.0054 - mae: 0.0559 - val_loss: 0.0044 - val_mae: 0.0486 - learning_rate: 0.0010\n",
      "Epoch 17/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 0.0045 - mae: 0.0490 - val_loss: 0.0032 - val_mae: 0.0438 - learning_rate: 0.0010\n",
      "Epoch 18/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 0.0047 - mae: 0.0511 - val_loss: 0.0032 - val_mae: 0.0462 - learning_rate: 0.0010\n",
      "Epoch 19/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 0.0042 - mae: 0.0473 - val_loss: 0.0042 - val_mae: 0.0518 - learning_rate: 0.0010\n",
      "Epoch 20/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 0.0042 - mae: 0.0468 - val_loss: 0.0029 - val_mae: 0.0450 - learning_rate: 0.0010\n",
      "Epoch 21/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 0.0039 - mae: 0.0477 - val_loss: 0.0024 - val_mae: 0.0384 - learning_rate: 0.0010\n",
      "Epoch 22/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 0.0040 - mae: 0.0457 - val_loss: 0.0022 - val_mae: 0.0365 - learning_rate: 0.0010\n",
      "Epoch 23/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 0.0036 - mae: 0.0451 - val_loss: 0.0027 - val_mae: 0.0429 - learning_rate: 0.0010\n",
      "Epoch 24/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - loss: 0.0034 - mae: 0.0429 - val_loss: 0.0041 - val_mae: 0.0553 - learning_rate: 0.0010\n",
      "Epoch 25/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - loss: 0.0037 - mae: 0.0467 - val_loss: 0.0034 - val_mae: 0.0477 - learning_rate: 0.0010\n",
      "Epoch 26/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 0.0034 - mae: 0.0418 - val_loss: 0.0032 - val_mae: 0.0450 - learning_rate: 0.0010\n",
      "Epoch 27/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - loss: 0.0035 - mae: 0.0425 - val_loss: 0.0022 - val_mae: 0.0372 - learning_rate: 5.0000e-04\n",
      "Epoch 28/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 0.0030 - mae: 0.0398 - val_loss: 0.0030 - val_mae: 0.0449 - learning_rate: 5.0000e-04\n",
      "Epoch 29/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.0035 - mae: 0.0407 - val_loss: 0.0023 - val_mae: 0.0405 - learning_rate: 5.0000e-04\n",
      "Epoch 30/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.0028 - mae: 0.0399 - val_loss: 0.0030 - val_mae: 0.0431 - learning_rate: 5.0000e-04\n",
      "Epoch 31/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.0027 - mae: 0.0382 - val_loss: 0.0027 - val_mae: 0.0415 - learning_rate: 2.5000e-04\n",
      "Epoch 32/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0026 - mae: 0.0361 - val_loss: 0.0021 - val_mae: 0.0367 - learning_rate: 2.5000e-04\n",
      "Epoch 33/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.0024 - mae: 0.0364 - val_loss: 0.0021 - val_mae: 0.0369 - learning_rate: 2.5000e-04\n",
      "Epoch 34/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.0030 - mae: 0.0388 - val_loss: 0.0022 - val_mae: 0.0360 - learning_rate: 2.5000e-04\n",
      "Epoch 35/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 0.0026 - mae: 0.0371 - val_loss: 0.0025 - val_mae: 0.0399 - learning_rate: 2.5000e-04\n",
      "Epoch 36/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.0026 - mae: 0.0372 - val_loss: 0.0025 - val_mae: 0.0404 - learning_rate: 2.5000e-04\n",
      "Epoch 37/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.0029 - mae: 0.0385 - val_loss: 0.0021 - val_mae: 0.0363 - learning_rate: 1.2500e-04\n",
      "Epoch 38/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.0026 - mae: 0.0370 - val_loss: 0.0025 - val_mae: 0.0402 - learning_rate: 1.2500e-04\n",
      "Epoch 39/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 0.0028 - mae: 0.0383 - val_loss: 0.0025 - val_mae: 0.0395 - learning_rate: 1.2500e-04\n",
      "Epoch 40/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - loss: 0.0027 - mae: 0.0370 - val_loss: 0.0022 - val_mae: 0.0374 - learning_rate: 1.2500e-04\n",
      "Epoch 41/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 0.0024 - mae: 0.0347 - val_loss: 0.0022 - val_mae: 0.0373 - learning_rate: 6.2500e-05\n",
      "\u001b[1m24/24\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step  \n",
      "âœ… onion_2010_2024_master_weekly.csv | MAE=91.64, RMSE=125.7, RÂ²=0.9642, MAPE=5.54%, Accuracy=94.46%\n",
      "\n",
      "ğŸš€ Processing Crop File: tomato_2010_2024_master_weekly.csv\n",
      "Epoch 1/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 91ms/step - loss: 0.3109 - mae: 0.3983 - val_loss: 0.0951 - val_mae: 0.2499 - learning_rate: 0.0010\n",
      "Epoch 2/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 0.0349 - mae: 0.1442 - val_loss: 0.0448 - val_mae: 0.1288 - learning_rate: 0.0010\n",
      "Epoch 3/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - loss: 0.0173 - mae: 0.0990 - val_loss: 0.0400 - val_mae: 0.1161 - learning_rate: 0.0010\n",
      "Epoch 4/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 0.0165 - mae: 0.0986 - val_loss: 0.0661 - val_mae: 0.1852 - learning_rate: 0.0010\n",
      "Epoch 5/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 0.0106 - mae: 0.0761 - val_loss: 0.0499 - val_mae: 0.1424 - learning_rate: 0.0010\n",
      "Epoch 6/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 0.0100 - mae: 0.0736 - val_loss: 0.0464 - val_mae: 0.1329 - learning_rate: 0.0010\n",
      "Epoch 7/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 0.0095 - mae: 0.0714 - val_loss: 0.0508 - val_mae: 0.1446 - learning_rate: 0.0010\n",
      "Epoch 8/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - loss: 0.0088 - mae: 0.0677 - val_loss: 0.0436 - val_mae: 0.1274 - learning_rate: 5.0000e-04\n",
      "Epoch 9/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - loss: 0.0087 - mae: 0.0672 - val_loss: 0.0459 - val_mae: 0.1330 - learning_rate: 5.0000e-04\n",
      "Epoch 10/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 0.0082 - mae: 0.0658 - val_loss: 0.0415 - val_mae: 0.1263 - learning_rate: 5.0000e-04\n",
      "Epoch 11/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 0.0085 - mae: 0.0659 - val_loss: 0.0417 - val_mae: 0.1279 - learning_rate: 5.0000e-04\n",
      "\u001b[1m24/24\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step  \n",
      "âœ… tomato_2010_2024_master_weekly.csv | MAE=616.47, RMSE=793.6, RÂ²=-0.0504, MAPE=64.65%, Accuracy=35.35%\n",
      "\n",
      "ğŸš€ Processing Crop File: wheat_2010_2024_master_weekly.csv\n",
      "Epoch 1/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 109ms/step - loss: 0.4436 - mae: 0.5082 - val_loss: 0.1652 - val_mae: 0.3983 - learning_rate: 0.0010\n",
      "Epoch 2/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - loss: 0.0699 - mae: 0.2129 - val_loss: 0.0848 - val_mae: 0.2825 - learning_rate: 0.0010\n",
      "Epoch 3/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 0.0343 - mae: 0.1504 - val_loss: 0.0696 - val_mae: 0.2582 - learning_rate: 0.0010\n",
      "Epoch 4/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 0.0149 - mae: 0.0953 - val_loss: 0.0123 - val_mae: 0.1022 - learning_rate: 0.0010\n",
      "Epoch 5/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 0.0068 - mae: 0.0651 - val_loss: 0.0097 - val_mae: 0.0898 - learning_rate: 0.0010\n",
      "Epoch 6/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 0.0056 - mae: 0.0585 - val_loss: 0.0028 - val_mae: 0.0366 - learning_rate: 0.0010\n",
      "Epoch 7/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - loss: 0.0051 - mae: 0.0543 - val_loss: 0.0034 - val_mae: 0.0416 - learning_rate: 0.0010\n",
      "Epoch 8/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 0.0047 - mae: 0.0532 - val_loss: 0.0030 - val_mae: 0.0387 - learning_rate: 0.0010\n",
      "Epoch 9/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 0.0041 - mae: 0.0488 - val_loss: 0.0032 - val_mae: 0.0403 - learning_rate: 0.0010\n",
      "Epoch 10/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - loss: 0.0047 - mae: 0.0525 - val_loss: 0.0064 - val_mae: 0.0676 - learning_rate: 0.0010\n",
      "Epoch 11/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - loss: 0.0036 - mae: 0.0456 - val_loss: 0.0088 - val_mae: 0.0827 - learning_rate: 5.0000e-04\n",
      "Epoch 12/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 0.0038 - mae: 0.0467 - val_loss: 0.0042 - val_mae: 0.0476 - learning_rate: 5.0000e-04\n",
      "Epoch 13/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - loss: 0.0038 - mae: 0.0458 - val_loss: 0.0091 - val_mae: 0.0832 - learning_rate: 5.0000e-04\n",
      "Epoch 14/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - loss: 0.0031 - mae: 0.0438 - val_loss: 0.0072 - val_mae: 0.0726 - learning_rate: 5.0000e-04\n",
      "\u001b[1m24/24\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 47ms/step  \n",
      "âœ… wheat_2010_2024_master_weekly.csv | MAE=63.15, RMSE=84.32, RÂ²=0.9711, MAPE=2.89%, Accuracy=97.11%\n",
      "\n",
      "ğŸ“Š All crops processed â€” metrics saved to tat_gqa_metrics_weekly.csv\n",
      "                               Crop     MAE    RMSE    R2  MAPE(%)  \\\n",
      "0  capsicum_2010_2024_master_weekly  145.11  194.65  0.93     5.71   \n",
      "1     onion_2010_2024_master_weekly   91.64  125.70  0.96     5.54   \n",
      "2    tomato_2010_2024_master_weekly  616.47  793.60 -0.05    64.65   \n",
      "3     wheat_2010_2024_master_weekly   63.15   84.32  0.97     2.89   \n",
      "\n",
      "   Accuracy(%)  \n",
      "0        94.29  \n",
      "1        94.46  \n",
      "2        35.35  \n",
      "3        97.11  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers, callbacks\n",
    "\n",
    "# -----------------------------\n",
    "# Paths / outputs\n",
    "# -----------------------------\n",
    "input_folder = \"dataset\"   # folder with weekly CSVs\n",
    "output_models = \"tat_gqa_output_models_weekly\"\n",
    "output_csv = \"tat_gqa_output_csv_weekly\"\n",
    "output_graphs = \"tat_gqa_output_graphs_weekly\"\n",
    "metrics_file = \"tat_gqa_metrics_weekly.csv\"\n",
    "\n",
    "os.makedirs(output_models, exist_ok=True)\n",
    "os.makedirs(output_csv, exist_ok=True)\n",
    "os.makedirs(output_graphs, exist_ok=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Config (tweak if needed)\n",
    "# -----------------------------\n",
    "look_back = 30       # weeks of history used as input\n",
    "batch_size = 32\n",
    "epochs = 60\n",
    "val_split = 0.15\n",
    "d_model = 64\n",
    "num_groups = 2       # number of query groups in GQA\n",
    "ff_dim = 128\n",
    "dropout_rate = 0.15\n",
    "seed = 42\n",
    "\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "def parse_dates_safe(s):\n",
    "    try:\n",
    "        return pd.to_datetime(s, dayfirst=True)\n",
    "    except:\n",
    "        return pd.to_datetime(s, dayfirst=False)\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    mask = y_true != 0\n",
    "    if mask.sum() == 0:\n",
    "        return np.nan\n",
    "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
    "\n",
    "def create_multivariate_dataset(y_scaled, exog_scaled, look_back):\n",
    "    n = len(y_scaled)\n",
    "    k = exog_scaled.shape[1] if exog_scaled is not None else 0\n",
    "    X, Y = [], []\n",
    "    for i in range(n - look_back):\n",
    "        seq_y = y_scaled[i:i+look_back, 0]\n",
    "        if k > 0:\n",
    "            seq_exog = exog_scaled[i:i+look_back, :]\n",
    "            seq = np.concatenate([seq_y.reshape(-1,1), seq_exog], axis=1)\n",
    "        else:\n",
    "            seq = seq_y.reshape(-1,1)\n",
    "        X.append(seq)\n",
    "        Y.append(y_scaled[i+look_back, 0])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "def positional_encoding(seq_len, d_model):\n",
    "    pos = np.arange(seq_len)[:, np.newaxis]\n",
    "    i = np.arange(d_model)[np.newaxis, :]\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    angle_rads = pos * angle_rates\n",
    "    pe = np.zeros_like(angle_rads)\n",
    "    pe[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    pe[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    return tf.cast(pe, tf.float32)\n",
    "\n",
    "# -----------------------------\n",
    "# Grouped Query Attention Layer (safe for graph mode)\n",
    "# -----------------------------\n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "class GroupedQueryAttention(layers.Layer):\n",
    "    def __init__(self, num_groups=2, key_dim=64, dropout_rate=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_groups = int(num_groups)\n",
    "        self.key_dim = int(key_dim)\n",
    "        self.dropout_rate = dropout_rate\n",
    "        # Create layer objects (they will be built automatically)\n",
    "        self.k_dense = layers.Dense(self.key_dim, name=\"gqa_k_dense\")\n",
    "        self.v_dense = layers.Dense(self.key_dim, name=\"gqa_v_dense\")\n",
    "        self.q_dense_groups = [layers.Dense(self.key_dim, name=f\"gqa_q_dense_{i}\") for i in range(self.num_groups)]\n",
    "        self.output_dense = layers.Dense(self.key_dim, name=\"gqa_out_dense\")\n",
    "        self.dropout = layers.Dropout(dropout_rate)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # nothing else to build; children layers will handle their weights\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        # x: (batch, seq, d_model)\n",
    "        K = self.k_dense(x)              # (B, S, key_dim)\n",
    "        V = self.v_dense(x)              # (B, S, key_dim)\n",
    "        head_outputs = []\n",
    "        sqrt_k = tf.math.sqrt(tf.cast(self.key_dim, tf.float32))\n",
    "        for q_layer in self.q_dense_groups:\n",
    "            Q = q_layer(x)               # (B, S, key_dim)\n",
    "            # scores: (B, S, S)\n",
    "            scores = tf.matmul(Q, K, transpose_b=True) / sqrt_k\n",
    "            attn_weights = tf.nn.softmax(scores, axis=-1)\n",
    "            attn_out = tf.matmul(attn_weights, V)  # (B, S, key_dim)\n",
    "            head_outputs.append(attn_out)\n",
    "        # concat along last axis (B, S, key_dim * num_groups)\n",
    "        concat = tf.concat(head_outputs, axis=-1)\n",
    "        proj = self.output_dense(concat)   # project back to key_dim (or d_model)\n",
    "        proj = self.dropout(proj, training=training)\n",
    "        return proj  # (B, S, key_dim)\n",
    "\n",
    "    def get_config(self):\n",
    "        cfg = super().get_config()\n",
    "        cfg.update({\"num_groups\": self.num_groups, \"key_dim\": self.key_dim, \"dropout_rate\": self.dropout_rate})\n",
    "        return cfg\n",
    "\n",
    "# -----------------------------\n",
    "# Build TAT + GQA model\n",
    "# -----------------------------\n",
    "def build_tat_gqa_model(input_shape, d_model=64, num_groups=2, ff_dim=128, dropout_rate=0.15):\n",
    "    seq_len, num_feat = input_shape\n",
    "    inp = layers.Input(shape=(seq_len, num_feat), name=\"input_seq\")   # (B, S, F)\n",
    "    # project features -> d_model\n",
    "    x = layers.Dense(d_model, name=\"proj_in\")(inp)                   # (B, S, d_model)\n",
    "    # add positional encoding\n",
    "    pos_enc = positional_encoding(seq_len, d_model)\n",
    "    x = x + pos_enc\n",
    "\n",
    "    # Grouped Query Attention block\n",
    "    gqa = GroupedQueryAttention(num_groups=num_groups, key_dim=d_model, dropout_rate=dropout_rate)\n",
    "    attn_out = gqa(x)                  # (B, S, d_model)\n",
    "    x = layers.LayerNormalization(epsilon=1e-6, name=\"ln_gqa\")(x + attn_out)\n",
    "\n",
    "    # Feed-forward\n",
    "    ff = layers.Dense(ff_dim, activation='relu', name=\"ff1\")(x)\n",
    "    ff = layers.Dense(d_model, name=\"ff2\")(ff)\n",
    "    ff = layers.Dropout(dropout_rate)(ff)\n",
    "    x = layers.LayerNormalization(epsilon=1e-6, name=\"ln_ff\")(x + ff)\n",
    "\n",
    "    # Add a temporal MultiHeadAttention block for extra temporal mixing\n",
    "    attn2 = layers.MultiHeadAttention(num_heads=4, key_dim=d_model)(x, x)\n",
    "    attn2 = layers.Dropout(dropout_rate)(attn2)\n",
    "    x = layers.LayerNormalization(epsilon=1e-6, name=\"ln_mha\")(x + attn2)\n",
    "    ff2 = layers.Dense(ff_dim, activation='relu')(x)\n",
    "    ff2 = layers.Dense(d_model)(ff2)\n",
    "    ff2 = layers.Dropout(dropout_rate)(ff2)\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x + ff2)\n",
    "\n",
    "    # Pool & output\n",
    "    x = layers.GlobalAveragePooling1D(name=\"global_pool\")(x)\n",
    "    x = layers.Dense(ff_dim//2, activation='relu')(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    out = layers.Dense(1, name=\"out\")(x)\n",
    "\n",
    "    model = models.Model(inputs=inp, outputs=out, name=\"TAT_GQA_weekly\")\n",
    "    model.compile(optimizer=optimizers.Adam(1e-3), loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# -----------------------------\n",
    "# Main loop\n",
    "# -----------------------------\n",
    "metrics_list = []\n",
    "\n",
    "for file in sorted(os.listdir(input_folder)):\n",
    "    if not file.endswith(\".csv\"):\n",
    "        continue\n",
    "    print(f\"\\nğŸš€ Processing Crop File: {file}\")\n",
    "    file_path = os.path.join(input_folder, file)\n",
    "\n",
    "    # load\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['Date'] = parse_dates_safe(df['Date'])\n",
    "    df = df.sort_values('Date')\n",
    "    df = df.groupby('Date', as_index=False).mean(numeric_only=True)\n",
    "    df.set_index('Date', inplace=True)\n",
    "\n",
    "    # weekly frequency alignment\n",
    "    df = df.asfreq('W')\n",
    "    if 'Average Price' not in df.columns:\n",
    "        print(f\"âš  'Average Price' missing in {file}, skipping.\")\n",
    "        continue\n",
    "    df['Average Price'] = df['Average Price'].ffill().bfill().fillna(df['Average Price'].mean())\n",
    "\n",
    "    # features: weekly equivalents\n",
    "    df['Lag_1'] = df['Average Price'].shift(1)\n",
    "    df['Lag_4'] = df['Average Price'].shift(4)\n",
    "    df['MA_4']  = df['Average Price'].rolling(window=4).mean()\n",
    "    df['MA_12'] = df['Average Price'].rolling(window=12).mean()\n",
    "    # fill feature NaNs\n",
    "    df[['Lag_1','Lag_4','MA_4','MA_12']] = df[['Lag_1','Lag_4','MA_4','MA_12']].bfill().ffill()\n",
    "\n",
    "    # prepare arrays and scalers\n",
    "    y = df[['Average Price']].values.astype('float32')\n",
    "    exog_cols = ['Lag_1','Lag_4','MA_4','MA_12']\n",
    "    exog = df[exog_cols].values.astype('float32')\n",
    "\n",
    "    y_scaler = MinMaxScaler()\n",
    "    exog_scaler = MinMaxScaler()\n",
    "    y_scaled = y_scaler.fit_transform(y)\n",
    "    exog_scaled = exog_scaler.fit_transform(exog)\n",
    "\n",
    "    # sequences (multivariate)\n",
    "    X_all, y_all = create_multivariate_dataset(y_scaled, exog_scaled, look_back)\n",
    "    if len(X_all) == 0:\n",
    "        print(f\"âš  Not enough data after look_back={look_back} in {file}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # train/val split\n",
    "    val_size = int(len(X_all) * val_split)\n",
    "    train_size = len(X_all) - val_size\n",
    "    X_train, y_train = X_all[:train_size], y_all[:train_size]\n",
    "    X_val, y_val     = X_all[train_size:], y_all[train_size:]\n",
    "\n",
    "    # build model\n",
    "    input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "    model = build_tat_gqa_model(input_shape, d_model=d_model, num_groups=num_groups, ff_dim=ff_dim, dropout_rate=dropout_rate)\n",
    "\n",
    "    # callbacks\n",
    "    es = callbacks.EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True)\n",
    "    rl = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, min_lr=1e-6)\n",
    "\n",
    "    # train\n",
    "    history = model.fit(X_train, y_train,\n",
    "                        validation_data=(X_val, y_val),\n",
    "                        epochs=epochs, batch_size=batch_size,\n",
    "                        callbacks=[es, rl], verbose=1)\n",
    "\n",
    "    # predict on all sequences and align\n",
    "    preds_scaled = model.predict(X_all, batch_size=batch_size)\n",
    "    preds = y_scaler.inverse_transform(preds_scaled).flatten()\n",
    "\n",
    "    df['Predicted'] = np.nan\n",
    "    start_idx = look_back\n",
    "    df.iloc[start_idx:start_idx + len(preds), df.columns.get_loc('Predicted')] = preds\n",
    "\n",
    "    # round to 2 decimals\n",
    "    for col in ['Average Price','Predicted','Lag_1','Lag_4','MA_4','MA_12']:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].round(2)\n",
    "\n",
    "    # metrics where prediction exists\n",
    "    mask = ~np.isnan(df['Predicted'].values)\n",
    "    y_true = df.loc[mask, 'Average Price'].values\n",
    "    y_pred = df.loc[mask, 'Predicted'].values\n",
    "\n",
    "    mae = round(mean_absolute_error(y_true, y_pred), 2)\n",
    "    rmse = round(np.sqrt(mean_squared_error(y_true, y_pred)), 2)\n",
    "    r2 = round(r2_score(y_true, y_pred), 4)\n",
    "    mape = round(mean_absolute_percentage_error(y_true, y_pred), 2)\n",
    "    accuracy = round(100 - mape, 2)\n",
    "\n",
    "    metrics_list.append({\n",
    "        'Crop': file.replace('.csv',''),\n",
    "        'MAE': mae,\n",
    "        'RMSE': rmse,\n",
    "        'R2': r2,\n",
    "        'MAPE(%)': mape,\n",
    "        'Accuracy(%)': accuracy\n",
    "    })\n",
    "\n",
    "    print(f\"âœ… {file} | MAE={mae}, RMSE={rmse}, RÂ²={r2}, MAPE={mape}%, Accuracy={accuracy}%\")\n",
    "\n",
    "    # save model, csv, graph\n",
    "    model_save_path = os.path.join(output_models, file.replace('.csv','_tat_gqa_weekly.keras'))\n",
    "    model.save(model_save_path)\n",
    "\n",
    "    out_csv = os.path.join(output_csv, file.replace('.csv','_tat_gqa_weekly_updated.csv'))\n",
    "    df_reset = df.reset_index()\n",
    "    df_reset.to_csv(out_csv, index=False, float_format='%.2f')\n",
    "\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.plot(df_reset['Date'], df_reset['Average Price'], label='Actual', color='blue')\n",
    "    plt.plot(df_reset['Date'], df_reset['Predicted'], label='Predicted (TAT+GQA)', color='red', linestyle='dashed')\n",
    "    plt.plot(df_reset['Date'], df_reset['MA_4'], label='MA_4', color='green', linestyle='--')\n",
    "    plt.plot(df_reset['Date'], df_reset['MA_12'], label='MA_12', color='orange', linestyle='--')\n",
    "    plt.title(f\"TAT+GQA Weekly Forecast - {file}\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Average Price\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_graphs, file.replace('.csv','_tat_gqa_weekly_graph.png')))\n",
    "    plt.close()\n",
    "\n",
    "    # cleanup\n",
    "    del model, X_all, X_train, X_val, y_all, y_train, y_val, preds_scaled, preds\n",
    "    gc.collect()\n",
    "\n",
    "# save consolidated metrics\n",
    "metrics_df = pd.DataFrame(metrics_list).round(2)\n",
    "metrics_df.to_csv(metrics_file, index=False)\n",
    "print(f\"\\nğŸ“Š All crops processed â€” metrics saved to {metrics_file}\")\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9758df22-12bb-48f4-9905-4ad413cc6ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TAT+HA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c19f5d1d-a5bd-44f2-a6cf-6423506ad22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ Processing Crop File: capsicum_2010_2024_master_weekly.csv\n",
      "Epoch 1/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 132ms/step - loss: 0.4385 - mae: 0.4737 - val_loss: 0.0844 - val_mae: 0.2615 - learning_rate: 0.0010\n",
      "Epoch 2/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - loss: 0.0466 - mae: 0.1673 - val_loss: 0.0576 - val_mae: 0.2044 - learning_rate: 0.0010\n",
      "Epoch 3/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - loss: 0.0245 - mae: 0.1269 - val_loss: 0.0216 - val_mae: 0.1093 - learning_rate: 0.0010\n",
      "Epoch 4/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - loss: 0.0209 - mae: 0.1138 - val_loss: 0.0301 - val_mae: 0.1331 - learning_rate: 0.0010\n",
      "Epoch 5/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 0.0220 - mae: 0.1169 - val_loss: 0.0452 - val_mae: 0.1736 - learning_rate: 0.0010\n",
      "Epoch 6/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - loss: 0.0207 - mae: 0.1118 - val_loss: 0.0401 - val_mae: 0.1605 - learning_rate: 0.0010\n",
      "Epoch 7/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - loss: 0.0206 - mae: 0.1137 - val_loss: 0.0297 - val_mae: 0.1324 - learning_rate: 0.0010\n",
      "Epoch 8/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 0.0190 - mae: 0.1089 - val_loss: 0.0355 - val_mae: 0.1486 - learning_rate: 5.0000e-04\n",
      "Epoch 9/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - loss: 0.0185 - mae: 0.1077 - val_loss: 0.0252 - val_mae: 0.1197 - learning_rate: 5.0000e-04\n",
      "Epoch 10/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 0.0201 - mae: 0.1124 - val_loss: 0.0363 - val_mae: 0.1509 - learning_rate: 5.0000e-04\n",
      "Epoch 11/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - loss: 0.0188 - mae: 0.1075 - val_loss: 0.0263 - val_mae: 0.1231 - learning_rate: 5.0000e-04\n",
      "\u001b[1m24/24\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 57ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… capsicum_2010_2024_master_weekly.csv | MAE=474.51, RMSE=577.64, RÂ²=0.3529, MAPE=21.33%, Accuracy=78.67%\n",
      "\n",
      "ğŸš€ Processing Crop File: onion_2010_2024_master_weekly.csv\n",
      "Epoch 1/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 81ms/step - loss: 0.4091 - mae: 0.4575 - val_loss: 0.0544 - val_mae: 0.1698 - learning_rate: 0.0010\n",
      "Epoch 2/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 0.0694 - mae: 0.2076 - val_loss: 0.0762 - val_mae: 0.2143 - learning_rate: 0.0010\n",
      "Epoch 3/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - loss: 0.0319 - mae: 0.1347 - val_loss: 0.0444 - val_mae: 0.1532 - learning_rate: 0.0010\n",
      "Epoch 4/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - loss: 0.0238 - mae: 0.1138 - val_loss: 0.0429 - val_mae: 0.1518 - learning_rate: 0.0010\n",
      "Epoch 5/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - loss: 0.0192 - mae: 0.0991 - val_loss: 0.0392 - val_mae: 0.1481 - learning_rate: 0.0010\n",
      "Epoch 6/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - loss: 0.0180 - mae: 0.0966 - val_loss: 0.0394 - val_mae: 0.1482 - learning_rate: 0.0010\n",
      "Epoch 7/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - loss: 0.0175 - mae: 0.0960 - val_loss: 0.0389 - val_mae: 0.1475 - learning_rate: 0.0010\n",
      "Epoch 8/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - loss: 0.0178 - mae: 0.0949 - val_loss: 0.0428 - val_mae: 0.1536 - learning_rate: 0.0010\n",
      "Epoch 9/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - loss: 0.0175 - mae: 0.0952 - val_loss: 0.0355 - val_mae: 0.1439 - learning_rate: 0.0010\n",
      "Epoch 10/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - loss: 0.0170 - mae: 0.0934 - val_loss: 0.0364 - val_mae: 0.1442 - learning_rate: 0.0010\n",
      "Epoch 11/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 101ms/step - loss: 0.0168 - mae: 0.0920 - val_loss: 0.0352 - val_mae: 0.1425 - learning_rate: 0.0010\n",
      "Epoch 12/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - loss: 0.0165 - mae: 0.0915 - val_loss: 0.0320 - val_mae: 0.1387 - learning_rate: 0.0010\n",
      "Epoch 13/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 0.0165 - mae: 0.0908 - val_loss: 0.0341 - val_mae: 0.1410 - learning_rate: 0.0010\n",
      "Epoch 14/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - loss: 0.0161 - mae: 0.0918 - val_loss: 0.0332 - val_mae: 0.1397 - learning_rate: 0.0010\n",
      "Epoch 15/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - loss: 0.0164 - mae: 0.0915 - val_loss: 0.0374 - val_mae: 0.1449 - learning_rate: 0.0010\n",
      "Epoch 16/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 0.0166 - mae: 0.0921 - val_loss: 0.0458 - val_mae: 0.1571 - learning_rate: 0.0010\n",
      "Epoch 17/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - loss: 0.0167 - mae: 0.0921 - val_loss: 0.0417 - val_mae: 0.1511 - learning_rate: 5.0000e-04\n",
      "Epoch 18/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - loss: 0.0155 - mae: 0.0879 - val_loss: 0.0387 - val_mae: 0.1467 - learning_rate: 5.0000e-04\n",
      "Epoch 19/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 0.0148 - mae: 0.0872 - val_loss: 0.0383 - val_mae: 0.1457 - learning_rate: 5.0000e-04\n",
      "Epoch 20/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - loss: 0.0154 - mae: 0.0904 - val_loss: 0.0382 - val_mae: 0.1457 - learning_rate: 5.0000e-04\n",
      "\u001b[1m24/24\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… onion_2010_2024_master_weekly.csv | MAE=360.92, RMSE=475.08, RÂ²=0.4888, MAPE=22.79%, Accuracy=77.21%\n",
      "\n",
      "ğŸš€ Processing Crop File: tomato_2010_2024_master_weekly.csv\n",
      "Epoch 1/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 69ms/step - loss: 0.4626 - mae: 0.4257 - val_loss: 0.0447 - val_mae: 0.1284 - learning_rate: 0.0010\n",
      "Epoch 2/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 0.0271 - mae: 0.1262 - val_loss: 0.0472 - val_mae: 0.1356 - learning_rate: 0.0010\n",
      "Epoch 3/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 0.0125 - mae: 0.0825 - val_loss: 0.0426 - val_mae: 0.1219 - learning_rate: 0.0010\n",
      "Epoch 4/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 0.0088 - mae: 0.0684 - val_loss: 0.0379 - val_mae: 0.1123 - learning_rate: 0.0010\n",
      "Epoch 5/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 0.0076 - mae: 0.0644 - val_loss: 0.0405 - val_mae: 0.1159 - learning_rate: 0.0010\n",
      "Epoch 6/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 0.0078 - mae: 0.0639 - val_loss: 0.0360 - val_mae: 0.1115 - learning_rate: 0.0010\n",
      "Epoch 7/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 0.0087 - mae: 0.0688 - val_loss: 0.0398 - val_mae: 0.1159 - learning_rate: 0.0010\n",
      "Epoch 8/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 0.0072 - mae: 0.0611 - val_loss: 0.0393 - val_mae: 0.1162 - learning_rate: 0.0010\n",
      "Epoch 9/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - loss: 0.0078 - mae: 0.0642 - val_loss: 0.0351 - val_mae: 0.1176 - learning_rate: 0.0010\n",
      "Epoch 10/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - loss: 0.0075 - mae: 0.0631 - val_loss: 0.0378 - val_mae: 0.1168 - learning_rate: 0.0010\n",
      "Epoch 11/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - loss: 0.0078 - mae: 0.0644 - val_loss: 0.0400 - val_mae: 0.1200 - learning_rate: 0.0010\n",
      "Epoch 12/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - loss: 0.0074 - mae: 0.0618 - val_loss: 0.0369 - val_mae: 0.1204 - learning_rate: 0.0010\n",
      "Epoch 13/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 0.0071 - mae: 0.0612 - val_loss: 0.0382 - val_mae: 0.1208 - learning_rate: 0.0010\n",
      "Epoch 14/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 0.0072 - mae: 0.0603 - val_loss: 0.0387 - val_mae: 0.1206 - learning_rate: 5.0000e-04\n",
      "Epoch 15/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 0.0072 - mae: 0.0599 - val_loss: 0.0398 - val_mae: 0.1215 - learning_rate: 5.0000e-04\n",
      "Epoch 16/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 0.0072 - mae: 0.0611 - val_loss: 0.0376 - val_mae: 0.1274 - learning_rate: 5.0000e-04\n",
      "Epoch 17/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - loss: 0.0069 - mae: 0.0589 - val_loss: 0.0385 - val_mae: 0.1282 - learning_rate: 5.0000e-04\n",
      "\u001b[1m24/24\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… tomato_2010_2024_master_weekly.csv | MAE=412.91, RMSE=622.99, RÂ²=0.3527, MAPE=33.97%, Accuracy=66.03%\n",
      "\n",
      "ğŸš€ Processing Crop File: wheat_2010_2024_master_weekly.csv\n",
      "Epoch 1/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 126ms/step - loss: 0.7012 - mae: 0.5934 - val_loss: 0.4389 - val_mae: 0.6581 - learning_rate: 0.0010\n",
      "Epoch 2/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 0.0763 - mae: 0.2186 - val_loss: 0.1072 - val_mae: 0.3181 - learning_rate: 0.0010\n",
      "Epoch 3/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 0.0223 - mae: 0.1174 - val_loss: 0.0708 - val_mae: 0.2562 - learning_rate: 0.0010\n",
      "Epoch 4/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - loss: 0.0144 - mae: 0.0929 - val_loss: 0.0469 - val_mae: 0.2059 - learning_rate: 0.0010\n",
      "Epoch 5/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 0.0092 - mae: 0.0758 - val_loss: 0.0710 - val_mae: 0.2587 - learning_rate: 0.0010\n",
      "Epoch 6/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 0.0104 - mae: 0.0782 - val_loss: 0.0362 - val_mae: 0.1805 - learning_rate: 0.0010\n",
      "Epoch 7/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 0.0066 - mae: 0.0643 - val_loss: 0.0455 - val_mae: 0.2050 - learning_rate: 0.0010\n",
      "Epoch 8/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 0.0059 - mae: 0.0599 - val_loss: 0.0114 - val_mae: 0.0898 - learning_rate: 0.0010\n",
      "Epoch 9/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 0.0061 - mae: 0.0622 - val_loss: 0.0263 - val_mae: 0.1511 - learning_rate: 0.0010\n",
      "Epoch 10/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 0.0045 - mae: 0.0509 - val_loss: 0.0329 - val_mae: 0.1734 - learning_rate: 0.0010\n",
      "Epoch 11/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 0.0052 - mae: 0.0562 - val_loss: 0.0226 - val_mae: 0.1403 - learning_rate: 0.0010\n",
      "Epoch 12/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 0.0057 - mae: 0.0572 - val_loss: 0.0265 - val_mae: 0.1540 - learning_rate: 0.0010\n",
      "Epoch 13/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 0.0050 - mae: 0.0537 - val_loss: 0.0337 - val_mae: 0.1745 - learning_rate: 5.0000e-04\n",
      "Epoch 14/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 0.0040 - mae: 0.0475 - val_loss: 0.0243 - val_mae: 0.1454 - learning_rate: 5.0000e-04\n",
      "Epoch 15/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 0.0033 - mae: 0.0420 - val_loss: 0.0201 - val_mae: 0.1301 - learning_rate: 5.0000e-04\n",
      "Epoch 16/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 0.0037 - mae: 0.0466 - val_loss: 0.0266 - val_mae: 0.1534 - learning_rate: 5.0000e-04\n",
      "\u001b[1m24/24\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… wheat_2010_2024_master_weekly.csv | MAE=122.12, RMSE=149.43, RÂ²=0.9091, MAPE=5.41%, Accuracy=94.59%\n",
      "\n",
      "ğŸ“Š All crops processed â€” metrics saved to tat_ha_metrics_weekly.csv\n",
      "                               Crop     MAE    RMSE    R2  MAPE(%)  \\\n",
      "0  capsicum_2010_2024_master_weekly  474.51  577.64  0.35    21.33   \n",
      "1     onion_2010_2024_master_weekly  360.92  475.08  0.49    22.79   \n",
      "2    tomato_2010_2024_master_weekly  412.91  622.99  0.35    33.97   \n",
      "3     wheat_2010_2024_master_weekly  122.12  149.43  0.91     5.41   \n",
      "\n",
      "   Accuracy(%)  \n",
      "0        78.67  \n",
      "1        77.21  \n",
      "2        66.03  \n",
      "3        94.59  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers, callbacks\n",
    "\n",
    "# -----------------------------\n",
    "# Paths / outputs\n",
    "# -----------------------------\n",
    "input_folder = \"dataset\"   # folder with weekly CSVs\n",
    "output_models = \"tat_ha_output_models_weekly\"\n",
    "output_csv = \"tat_ha_output_csv_weekly\"\n",
    "output_graphs = \"tat_ha_output_graphs_weekly\"\n",
    "metrics_file = \"tat_ha_metrics_weekly.csv\"\n",
    "\n",
    "os.makedirs(output_models, exist_ok=True)\n",
    "os.makedirs(output_csv, exist_ok=True)\n",
    "os.makedirs(output_graphs, exist_ok=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Config (tweak if needed)\n",
    "# -----------------------------\n",
    "look_back = 30       # weeks of history used as input\n",
    "batch_size = 32\n",
    "epochs = 60\n",
    "val_split = 0.15\n",
    "d_model = 64\n",
    "num_heads = 4\n",
    "ff_dim = 128\n",
    "dropout_rate = 0.15\n",
    "seed = 42\n",
    "\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "def parse_dates_safe(s):\n",
    "    try:\n",
    "        return pd.to_datetime(s, dayfirst=True)\n",
    "    except:\n",
    "        return pd.to_datetime(s, dayfirst=False)\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    mask = y_true != 0\n",
    "    if mask.sum() == 0:\n",
    "        return np.nan\n",
    "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
    "\n",
    "def create_multivariate_dataset(y_scaled, exog_scaled, look_back):\n",
    "    n = len(y_scaled)\n",
    "    k = exog_scaled.shape[1] if exog_scaled is not None else 0\n",
    "    X, Y = [], []\n",
    "    for i in range(n - look_back):\n",
    "        seq_y = y_scaled[i:i+look_back, 0]\n",
    "        if k > 0:\n",
    "            seq_exog = exog_scaled[i:i+look_back, :]\n",
    "            seq = np.concatenate([seq_y.reshape(-1,1), seq_exog], axis=1)\n",
    "        else:\n",
    "            seq = seq_y.reshape(-1,1)\n",
    "        X.append(seq)\n",
    "        Y.append(y_scaled[i+look_back, 0])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "def positional_encoding(seq_len, d_model):\n",
    "    pos = np.arange(seq_len)[:, np.newaxis]\n",
    "    i = np.arange(d_model)[np.newaxis, :]\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    angle_rads = pos * angle_rates\n",
    "    pe = np.zeros_like(angle_rads)\n",
    "    pe[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    pe[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    return tf.cast(pe, tf.float32)\n",
    "\n",
    "# -----------------------------\n",
    "# Hierarchical Attention Layer (Graph-safe)\n",
    "# -----------------------------\n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "class HierarchicalAttention(layers.Layer):\n",
    "    def __init__(self, d_model=64, num_heads=4, ff_dim=128, dropout_rate=0.15, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.d_model = int(d_model)\n",
    "        self.num_heads = int(num_heads)\n",
    "        self.ff_dim = int(ff_dim)\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # Feature scoring: project last axis to same size (num_features) then softmax across features\n",
    "        self.feature_score_dense = None  # built lazily\n",
    "        # Project aggregated feature -> d_model\n",
    "        self.feature_proj = layers.Dense(self.d_model)\n",
    "        # Temporal MHA\n",
    "        self.temporal_mha = layers.MultiHeadAttention(num_heads=self.num_heads, key_dim=self.d_model)\n",
    "        # FFN\n",
    "        self.ffn1 = layers.Dense(self.ff_dim, activation='relu')\n",
    "        self.ffn2 = layers.Dense(self.d_model)\n",
    "        # Norms & dropout\n",
    "        self.ln1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.ln2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout = layers.Dropout(self.dropout_rate)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # input_shape = (batch, seq_len, num_features)\n",
    "        num_features = int(input_shape[-1])\n",
    "        # feature_score_dense projects each feature position to a scalar per feature (we produce num_features outputs)\n",
    "        self.feature_score_dense = layers.Dense(num_features)\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        # x: (batch, seq_len, num_features)\n",
    "        # 1) Feature-level scoring per timestep\n",
    "        # feature_scores: (batch, seq_len, num_features)\n",
    "        feature_scores = self.feature_score_dense(x)\n",
    "        # feature_weights across features\n",
    "        feature_weights = tf.nn.softmax(feature_scores, axis=-1)  # softmax over features\n",
    "        # weighted features\n",
    "        weighted = x * feature_weights  # broadcast (batch, seq_len, num_features)\n",
    "        # aggregate features into a single feature per timestep\n",
    "        feature_agg = tf.reduce_sum(weighted, axis=-1, keepdims=True)  # (batch, seq_len, 1)\n",
    "        # project to d_model for temporal attention\n",
    "        seq_repr = self.feature_proj(feature_agg)  # (batch, seq_len, d_model)\n",
    "\n",
    "        # 2) Temporal multi-head self-attention\n",
    "        attn_out = self.temporal_mha(seq_repr, seq_repr)  # (batch, seq_len, d_model)\n",
    "        attn_out = self.dropout(attn_out, training=training)\n",
    "        out1 = self.ln1(seq_repr + attn_out)\n",
    "\n",
    "        # 3) Feed-forward\n",
    "        ffn = self.ffn1(out1)\n",
    "        ffn = self.ffn2(ffn)\n",
    "        ffn = self.dropout(ffn, training=training)\n",
    "        out2 = self.ln2(out1 + ffn)\n",
    "\n",
    "        return out2  # (batch, seq_len, d_model)\n",
    "\n",
    "    def get_config(self):\n",
    "        cfg = super().get_config()\n",
    "        cfg.update({\"d_model\": self.d_model, \"num_heads\": self.num_heads, \"ff_dim\": self.ff_dim, \"dropout_rate\": self.dropout_rate})\n",
    "        return cfg\n",
    "\n",
    "# -----------------------------\n",
    "# Build TAT+HA model\n",
    "# -----------------------------\n",
    "def build_tat_ha_model(input_shape, d_model=64, num_heads=4, ff_dim=128, dropout_rate=0.15):\n",
    "    seq_len, num_feat = input_shape\n",
    "    inp = layers.Input(shape=(seq_len, num_feat))\n",
    "    # project input features to d_model\n",
    "    x = layers.Dense(d_model)(inp)  # (B,S,d_model)\n",
    "    # add positional encoding\n",
    "    pe = positional_encoding(seq_len, d_model)\n",
    "    x = x + pe\n",
    "    # hierarchical attention block\n",
    "    ha_out = HierarchicalAttention(d_model=d_model, num_heads=num_heads, ff_dim=ff_dim, dropout_rate=dropout_rate)(inp)  # note: pass original inp for feature-level scoring\n",
    "    # align residuals: project inp to d_model\n",
    "    inp_proj = layers.Dense(d_model)(inp)\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(ha_out + inp_proj)\n",
    "    # optional additional transformer layers (temporal mixing)\n",
    "    attn2 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)(x, x)\n",
    "    attn2 = layers.Dropout(dropout_rate)(attn2)\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x + attn2)\n",
    "    ff = layers.Dense(ff_dim, activation='relu')(x)\n",
    "    ff = layers.Dense(d_model)(ff)\n",
    "    ff = layers.Dropout(dropout_rate)(ff)\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x + ff)\n",
    "    # pooling + output\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dense(ff_dim//2, activation='relu')(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    out = layers.Dense(1)(x)\n",
    "\n",
    "    model = models.Model(inputs=inp, outputs=out)\n",
    "    model.compile(optimizer=optimizers.Adam(1e-3), loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# -----------------------------\n",
    "# Main loop: process weekly crop files\n",
    "# -----------------------------\n",
    "metrics_list = []\n",
    "\n",
    "for file in sorted(os.listdir(input_folder)):\n",
    "    if not file.endswith(\".csv\"):\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nğŸš€ Processing Crop File: {file}\")\n",
    "    file_path = os.path.join(input_folder, file)\n",
    "\n",
    "    # -------------------------\n",
    "    # Load & parse\n",
    "    # -------------------------\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['Date'] = parse_dates_safe(df['Date'])\n",
    "    df = df.sort_values('Date')\n",
    "    # aggregate duplicates (mean)\n",
    "    df = df.groupby('Date', as_index=False).mean(numeric_only=True)\n",
    "    df.set_index('Date', inplace=True)\n",
    "\n",
    "    # weekly frequency\n",
    "    df = df.asfreq('W')\n",
    "\n",
    "    # check column\n",
    "    if 'Average Price' not in df.columns:\n",
    "        print(f\"âš  'Average Price' not found in {file}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    # fill missing price values\n",
    "    df['Average Price'] = df['Average Price'].ffill().bfill().fillna(df['Average Price'].mean())\n",
    "\n",
    "    # -------------------------\n",
    "    # Feature engineering (weekly)\n",
    "    # -------------------------\n",
    "    df['Lag_1'] = df['Average Price'].shift(1)\n",
    "    df['Lag_4'] = df['Average Price'].shift(4)\n",
    "    df['MA_4'] = df['Average Price'].rolling(window=4).mean()\n",
    "    df['MA_12'] = df['Average Price'].rolling(window=12).mean()\n",
    "\n",
    "    # fill features\n",
    "    df[['Lag_1','Lag_4','MA_4','MA_12']] = df[['Lag_1','Lag_4','MA_4','MA_12']].bfill().ffill()\n",
    "\n",
    "    # -------------------------\n",
    "    # Prepare arrays & scalers\n",
    "    # -------------------------\n",
    "    y = df[['Average Price']].values.astype('float32')\n",
    "    exog_cols = ['Lag_1','Lag_4','MA_4','MA_12']\n",
    "    exog = df[exog_cols].values.astype('float32')\n",
    "\n",
    "    # scale\n",
    "    y_scaler = MinMaxScaler()\n",
    "    exog_scaler = MinMaxScaler()\n",
    "    y_scaled = y_scaler.fit_transform(y)\n",
    "    exog_scaled = exog_scaler.fit_transform(exog)\n",
    "\n",
    "    # create sequences\n",
    "    X_all, y_all = create_multivariate_dataset(y_scaled, exog_scaled, look_back)\n",
    "    if len(X_all) == 0:\n",
    "        print(f\"âš  Not enough data for look_back={look_back} in {file}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # train/val split (chronological)\n",
    "    val_size = int(len(X_all) * val_split)\n",
    "    train_size = len(X_all) - val_size\n",
    "    X_train, y_train = X_all[:train_size], y_all[:train_size]\n",
    "    X_val, y_val     = X_all[train_size:], y_all[train_size:]\n",
    "\n",
    "    # -------------------------\n",
    "    # Build model & train\n",
    "    # -------------------------\n",
    "    input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "    model = build_tat_ha_model(input_shape, d_model=d_model, num_heads=num_heads, ff_dim=ff_dim, dropout_rate=dropout_rate)\n",
    "\n",
    "    es = callbacks.EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True)\n",
    "    rl = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, min_lr=1e-6)\n",
    "\n",
    "    history = model.fit(X_train, y_train,\n",
    "                        validation_data=(X_val, y_val),\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        callbacks=[es, rl],\n",
    "                        verbose=1)\n",
    "\n",
    "    # -------------------------\n",
    "    # Predict on all sequences and align with original index\n",
    "    # -------------------------\n",
    "    preds_scaled = model.predict(X_all, batch_size=batch_size)\n",
    "    preds = y_scaler.inverse_transform(preds_scaled).flatten()\n",
    "\n",
    "    # create Predicted column with NaNs for first look_back rows\n",
    "    df['Predicted'] = np.nan\n",
    "    start_idx = look_back\n",
    "    df.iloc[start_idx : start_idx + len(preds), df.columns.get_loc('Predicted')] = preds\n",
    "\n",
    "    # round to 2 decimals\n",
    "    for col in ['Average Price','Predicted','Lag_1','Lag_4','MA_4','MA_12']:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].round(2)\n",
    "\n",
    "    # -------------------------\n",
    "    # Metrics (on rows where prediction exists)\n",
    "    # -------------------------\n",
    "    mask = ~np.isnan(df['Predicted'].values)\n",
    "    y_true = df.loc[mask, 'Average Price'].values\n",
    "    y_pred = df.loc[mask, 'Predicted'].values\n",
    "\n",
    "    mae = round(mean_absolute_error(y_true, y_pred), 2)\n",
    "    rmse = round(np.sqrt(mean_squared_error(y_true, y_pred)), 2)\n",
    "    r2 = round(r2_score(y_true, y_pred), 4)\n",
    "    mape = round(mean_absolute_percentage_error(y_true, y_pred), 2)\n",
    "    accuracy = round(100 - mape, 2)\n",
    "\n",
    "    metrics_list.append({\n",
    "        'Crop': file.replace('.csv',''),\n",
    "        'MAE': mae,\n",
    "        'RMSE': rmse,\n",
    "        'R2': r2,\n",
    "        'MAPE(%)': mape,\n",
    "        'Accuracy(%)': accuracy\n",
    "    })\n",
    "\n",
    "    print(f\"âœ… {file} | MAE={mae}, RMSE={rmse}, RÂ²={r2}, MAPE={mape}%, Accuracy={accuracy}%\")\n",
    "\n",
    "    # -------------------------\n",
    "    # Save model, CSV, graph\n",
    "    # -------------------------\n",
    "    model_save_path = os.path.join(output_models, file.replace('.csv','_tat_ha_weekly.h5'))\n",
    "    model.save(model_save_path)\n",
    "\n",
    "    out_csv = os.path.join(output_csv, file.replace('.csv','_tat_ha_weekly_updated.csv'))\n",
    "    df_reset = df.reset_index()\n",
    "    df_reset.to_csv(out_csv, index=False, float_format='%.2f')\n",
    "\n",
    "    # graph\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.plot(df_reset['Date'], df_reset['Average Price'], label='Actual', color='blue')\n",
    "    plt.plot(df_reset['Date'], df_reset['Predicted'], label='Predicted (TAT+HA)', color='red', linestyle='dashed')\n",
    "    plt.plot(df_reset['Date'], df_reset['MA_4'], label='MA_4', color='green', linestyle='--')\n",
    "    plt.plot(df_reset['Date'], df_reset['MA_12'], label='MA_12', color='orange', linestyle='--')\n",
    "    plt.title(f\"TAT+HA Weekly Forecast - {file}\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Average Price\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_graphs, file.replace('.csv','_tat_ha_weekly_graph.png')))\n",
    "    plt.close()\n",
    "\n",
    "    # cleanup\n",
    "    del model, X_all, X_train, X_val, y_all, y_train, y_val, preds_scaled, preds\n",
    "    gc.collect()\n",
    "\n",
    "# -----------------------------\n",
    "# Save metrics summary (all crops)\n",
    "# -----------------------------\n",
    "metrics_df = pd.DataFrame(metrics_list).round(2)\n",
    "metrics_df.to_csv(metrics_file, index=False)\n",
    "print(f\"\\nğŸ“Š All crops processed â€” metrics saved to {metrics_file}\")\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a08bdd-5ec6-4a6e-9e84-b287b5d3c439",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01f99b67-00cb-4b3e-8673-d5c0cf5deac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ Processing Crop File: capsicum_2010_2024_master_weekly.csv\n",
      "Epoch 1/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 50ms/step - loss: 0.0352 - mae: 0.1416 - val_loss: 0.0221 - val_mae: 0.1149 - learning_rate: 0.0010\n",
      "Epoch 2/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0142 - mae: 0.0932 - val_loss: 0.0302 - val_mae: 0.1398 - learning_rate: 0.0010\n",
      "Epoch 3/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0125 - mae: 0.0861 - val_loss: 0.0281 - val_mae: 0.1353 - learning_rate: 0.0010\n",
      "Epoch 4/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 0.0112 - mae: 0.0820 - val_loss: 0.0273 - val_mae: 0.1336 - learning_rate: 0.0010\n",
      "Epoch 5/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0100 - mae: 0.0769 - val_loss: 0.0257 - val_mae: 0.1287 - learning_rate: 0.0010\n",
      "Epoch 6/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0090 - mae: 0.0728 - val_loss: 0.0233 - val_mae: 0.1212 - learning_rate: 5.0000e-04\n",
      "Epoch 7/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0086 - mae: 0.0709 - val_loss: 0.0238 - val_mae: 0.1236 - learning_rate: 5.0000e-04\n",
      "Epoch 8/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0080 - mae: 0.0696 - val_loss: 0.0216 - val_mae: 0.1156 - learning_rate: 5.0000e-04\n",
      "Epoch 9/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0078 - mae: 0.0677 - val_loss: 0.0235 - val_mae: 0.1223 - learning_rate: 5.0000e-04\n",
      "Epoch 10/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0083 - mae: 0.0692 - val_loss: 0.0200 - val_mae: 0.1111 - learning_rate: 5.0000e-04\n",
      "Epoch 11/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0067 - mae: 0.0634 - val_loss: 0.0230 - val_mae: 0.1216 - learning_rate: 5.0000e-04\n",
      "Epoch 12/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0072 - mae: 0.0640 - val_loss: 0.0212 - val_mae: 0.1157 - learning_rate: 5.0000e-04\n",
      "Epoch 13/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0067 - mae: 0.0626 - val_loss: 0.0217 - val_mae: 0.1173 - learning_rate: 5.0000e-04\n",
      "Epoch 14/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 0.0067 - mae: 0.0616 - val_loss: 0.0219 - val_mae: 0.1189 - learning_rate: 5.0000e-04\n",
      "Epoch 15/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0068 - mae: 0.0620 - val_loss: 0.0229 - val_mae: 0.1228 - learning_rate: 2.5000e-04\n",
      "Epoch 16/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.0066 - mae: 0.0626 - val_loss: 0.0250 - val_mae: 0.1293 - learning_rate: 2.5000e-04\n",
      "Epoch 17/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.0065 - mae: 0.0624 - val_loss: 0.0230 - val_mae: 0.1231 - learning_rate: 2.5000e-04\n",
      "Epoch 18/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0066 - mae: 0.0610 - val_loss: 0.0200 - val_mae: 0.1126 - learning_rate: 2.5000e-04\n",
      "Epoch 19/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.0055 - mae: 0.0568 - val_loss: 0.0185 - val_mae: 0.1075 - learning_rate: 1.2500e-04\n",
      "Epoch 20/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0057 - mae: 0.0581 - val_loss: 0.0192 - val_mae: 0.1103 - learning_rate: 1.2500e-04\n",
      "Epoch 21/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0061 - mae: 0.0598 - val_loss: 0.0182 - val_mae: 0.1068 - learning_rate: 1.2500e-04\n",
      "Epoch 22/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0060 - mae: 0.0593 - val_loss: 0.0173 - val_mae: 0.1033 - learning_rate: 1.2500e-04\n",
      "Epoch 23/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0060 - mae: 0.0585 - val_loss: 0.0163 - val_mae: 0.1002 - learning_rate: 1.2500e-04\n",
      "Epoch 24/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 0.0056 - mae: 0.0583 - val_loss: 0.0170 - val_mae: 0.1026 - learning_rate: 1.2500e-04\n",
      "Epoch 25/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0057 - mae: 0.0577 - val_loss: 0.0174 - val_mae: 0.1040 - learning_rate: 1.2500e-04\n",
      "Epoch 26/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.0062 - mae: 0.0604 - val_loss: 0.0179 - val_mae: 0.1061 - learning_rate: 1.2500e-04\n",
      "Epoch 27/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 0.0064 - mae: 0.0610 - val_loss: 0.0172 - val_mae: 0.1035 - learning_rate: 1.2500e-04\n",
      "Epoch 28/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 0.0057 - mae: 0.0568 - val_loss: 0.0181 - val_mae: 0.1067 - learning_rate: 6.2500e-05\n",
      "Epoch 29/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 0.0055 - mae: 0.0575 - val_loss: 0.0171 - val_mae: 0.1034 - learning_rate: 6.2500e-05\n",
      "Epoch 30/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 0.0053 - mae: 0.0562 - val_loss: 0.0179 - val_mae: 0.1060 - learning_rate: 6.2500e-05\n",
      "Epoch 31/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0062 - mae: 0.0588 - val_loss: 0.0166 - val_mae: 0.1015 - learning_rate: 6.2500e-05\n",
      "\u001b[1m24/24\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 47ms/step  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… capsicum_2010_2024_master_weekly.csv | MAE=245.6, RMSE=333.03, RÂ²=0.7849, MAPE=9.5%, Accuracy=90.5%\n",
      "\n",
      "ğŸš€ Processing Crop File: onion_2010_2024_master_weekly.csv\n",
      "Epoch 1/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 103ms/step - loss: 0.0296 - mae: 0.1224 - val_loss: 0.0487 - val_mae: 0.1675 - learning_rate: 0.0010\n",
      "Epoch 2/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.0131 - mae: 0.0787 - val_loss: 0.0307 - val_mae: 0.1320 - learning_rate: 0.0010\n",
      "Epoch 3/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.0102 - mae: 0.0683 - val_loss: 0.0178 - val_mae: 0.1051 - learning_rate: 0.0010\n",
      "Epoch 4/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.0088 - mae: 0.0619 - val_loss: 0.0141 - val_mae: 0.0933 - learning_rate: 0.0010\n",
      "Epoch 5/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 0.0072 - mae: 0.0558 - val_loss: 0.0099 - val_mae: 0.0773 - learning_rate: 0.0010\n",
      "Epoch 6/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0073 - mae: 0.0571 - val_loss: 0.0132 - val_mae: 0.0874 - learning_rate: 0.0010\n",
      "Epoch 7/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0062 - mae: 0.0515 - val_loss: 0.0082 - val_mae: 0.0680 - learning_rate: 0.0010\n",
      "Epoch 8/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0063 - mae: 0.0500 - val_loss: 0.0103 - val_mae: 0.0743 - learning_rate: 0.0010\n",
      "Epoch 9/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.0059 - mae: 0.0473 - val_loss: 0.0091 - val_mae: 0.0702 - learning_rate: 0.0010\n",
      "Epoch 10/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 0.0057 - mae: 0.0462 - val_loss: 0.0084 - val_mae: 0.0650 - learning_rate: 0.0010\n",
      "Epoch 11/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0052 - mae: 0.0449 - val_loss: 0.0083 - val_mae: 0.0655 - learning_rate: 0.0010\n",
      "Epoch 12/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.0050 - mae: 0.0458 - val_loss: 0.0090 - val_mae: 0.0664 - learning_rate: 5.0000e-04\n",
      "Epoch 13/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.0044 - mae: 0.0421 - val_loss: 0.0081 - val_mae: 0.0622 - learning_rate: 5.0000e-04\n",
      "Epoch 14/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 0.0044 - mae: 0.0419 - val_loss: 0.0076 - val_mae: 0.0593 - learning_rate: 5.0000e-04\n",
      "Epoch 15/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0045 - mae: 0.0417 - val_loss: 0.0072 - val_mae: 0.0574 - learning_rate: 5.0000e-04\n",
      "Epoch 16/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0040 - mae: 0.0400 - val_loss: 0.0065 - val_mae: 0.0547 - learning_rate: 5.0000e-04\n",
      "Epoch 17/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.0038 - mae: 0.0392 - val_loss: 0.0064 - val_mae: 0.0539 - learning_rate: 5.0000e-04\n",
      "Epoch 18/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0038 - mae: 0.0388 - val_loss: 0.0066 - val_mae: 0.0549 - learning_rate: 5.0000e-04\n",
      "Epoch 19/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 0.0035 - mae: 0.0382 - val_loss: 0.0062 - val_mae: 0.0526 - learning_rate: 5.0000e-04\n",
      "Epoch 20/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0037 - mae: 0.0377 - val_loss: 0.0067 - val_mae: 0.0542 - learning_rate: 5.0000e-04\n",
      "Epoch 21/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0034 - mae: 0.0366 - val_loss: 0.0061 - val_mae: 0.0513 - learning_rate: 5.0000e-04\n",
      "Epoch 22/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0040 - mae: 0.0383 - val_loss: 0.0065 - val_mae: 0.0529 - learning_rate: 5.0000e-04\n",
      "Epoch 23/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.0036 - mae: 0.0363 - val_loss: 0.0065 - val_mae: 0.0533 - learning_rate: 5.0000e-04\n",
      "Epoch 24/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0033 - mae: 0.0354 - val_loss: 0.0073 - val_mae: 0.0577 - learning_rate: 2.5000e-04\n",
      "Epoch 25/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0036 - mae: 0.0371 - val_loss: 0.0077 - val_mae: 0.0596 - learning_rate: 2.5000e-04\n",
      "Epoch 26/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0031 - mae: 0.0335 - val_loss: 0.0062 - val_mae: 0.0520 - learning_rate: 2.5000e-04\n",
      "Epoch 27/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0032 - mae: 0.0341 - val_loss: 0.0062 - val_mae: 0.0520 - learning_rate: 2.5000e-04\n",
      "Epoch 28/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0030 - mae: 0.0340 - val_loss: 0.0090 - val_mae: 0.0675 - learning_rate: 1.2500e-04\n",
      "Epoch 29/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.0029 - mae: 0.0324 - val_loss: 0.0075 - val_mae: 0.0590 - learning_rate: 1.2500e-04\n",
      "\u001b[1m24/24\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 52ms/step  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… onion_2010_2024_master_weekly.csv | MAE=125.15, RMSE=201.3, RÂ²=0.9082, MAPE=7.06%, Accuracy=92.94%\n",
      "\n",
      "ğŸš€ Processing Crop File: tomato_2010_2024_master_weekly.csv\n",
      "Epoch 1/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 84ms/step - loss: 0.0093 - mae: 0.0643 - val_loss: 0.0361 - val_mae: 0.1170 - learning_rate: 0.0010\n",
      "Epoch 2/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.0062 - mae: 0.0539 - val_loss: 0.0328 - val_mae: 0.1122 - learning_rate: 0.0010\n",
      "Epoch 3/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0055 - mae: 0.0518 - val_loss: 0.0301 - val_mae: 0.1092 - learning_rate: 0.0010\n",
      "Epoch 4/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 0.0049 - mae: 0.0486 - val_loss: 0.0270 - val_mae: 0.1063 - learning_rate: 0.0010\n",
      "Epoch 5/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0042 - mae: 0.0449 - val_loss: 0.0238 - val_mae: 0.1044 - learning_rate: 0.0010\n",
      "Epoch 6/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0038 - mae: 0.0436 - val_loss: 0.0202 - val_mae: 0.0939 - learning_rate: 0.0010\n",
      "Epoch 7/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0031 - mae: 0.0392 - val_loss: 0.0188 - val_mae: 0.0960 - learning_rate: 0.0010\n",
      "Epoch 8/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.0028 - mae: 0.0369 - val_loss: 0.0164 - val_mae: 0.0879 - learning_rate: 0.0010\n",
      "Epoch 9/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 0.0028 - mae: 0.0361 - val_loss: 0.0151 - val_mae: 0.0850 - learning_rate: 0.0010\n",
      "Epoch 10/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0024 - mae: 0.0345 - val_loss: 0.0135 - val_mae: 0.0803 - learning_rate: 0.0010\n",
      "Epoch 11/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0024 - mae: 0.0340 - val_loss: 0.0121 - val_mae: 0.0748 - learning_rate: 0.0010\n",
      "Epoch 12/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0022 - mae: 0.0329 - val_loss: 0.0115 - val_mae: 0.0736 - learning_rate: 0.0010\n",
      "Epoch 13/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.0019 - mae: 0.0309 - val_loss: 0.0107 - val_mae: 0.0705 - learning_rate: 0.0010\n",
      "Epoch 14/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.0019 - mae: 0.0302 - val_loss: 0.0100 - val_mae: 0.0682 - learning_rate: 0.0010\n",
      "Epoch 15/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 0.0018 - mae: 0.0303 - val_loss: 0.0092 - val_mae: 0.0658 - learning_rate: 0.0010\n",
      "Epoch 16/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0017 - mae: 0.0298 - val_loss: 0.0090 - val_mae: 0.0653 - learning_rate: 0.0010\n",
      "Epoch 17/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0017 - mae: 0.0284 - val_loss: 0.0081 - val_mae: 0.0613 - learning_rate: 0.0010\n",
      "Epoch 18/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.0017 - mae: 0.0290 - val_loss: 0.0078 - val_mae: 0.0608 - learning_rate: 0.0010\n",
      "Epoch 19/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.0017 - mae: 0.0274 - val_loss: 0.0072 - val_mae: 0.0577 - learning_rate: 0.0010\n",
      "Epoch 20/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0016 - mae: 0.0282 - val_loss: 0.0069 - val_mae: 0.0562 - learning_rate: 0.0010\n",
      "Epoch 21/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0015 - mae: 0.0270 - val_loss: 0.0066 - val_mae: 0.0554 - learning_rate: 0.0010\n",
      "Epoch 22/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0015 - mae: 0.0272 - val_loss: 0.0064 - val_mae: 0.0556 - learning_rate: 0.0010\n",
      "Epoch 23/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0015 - mae: 0.0269 - val_loss: 0.0066 - val_mae: 0.0563 - learning_rate: 0.0010\n",
      "Epoch 24/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.0013 - mae: 0.0251 - val_loss: 0.0058 - val_mae: 0.0527 - learning_rate: 0.0010\n",
      "Epoch 25/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.0014 - mae: 0.0254 - val_loss: 0.0059 - val_mae: 0.0536 - learning_rate: 0.0010\n",
      "Epoch 26/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 0.0014 - mae: 0.0250 - val_loss: 0.0058 - val_mae: 0.0522 - learning_rate: 0.0010\n",
      "Epoch 27/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.0012 - mae: 0.0241 - val_loss: 0.0053 - val_mae: 0.0497 - learning_rate: 0.0010\n",
      "Epoch 28/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.0014 - mae: 0.0258 - val_loss: 0.0054 - val_mae: 0.0499 - learning_rate: 0.0010\n",
      "Epoch 29/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0013 - mae: 0.0254 - val_loss: 0.0057 - val_mae: 0.0517 - learning_rate: 0.0010\n",
      "Epoch 30/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 0.0011 - mae: 0.0231 - val_loss: 0.0053 - val_mae: 0.0498 - learning_rate: 0.0010\n",
      "Epoch 31/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.0012 - mae: 0.0230 - val_loss: 0.0051 - val_mae: 0.0509 - learning_rate: 0.0010\n",
      "Epoch 32/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.0013 - mae: 0.0253 - val_loss: 0.0056 - val_mae: 0.0517 - learning_rate: 0.0010\n",
      "Epoch 33/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0011 - mae: 0.0224 - val_loss: 0.0048 - val_mae: 0.0493 - learning_rate: 0.0010\n",
      "Epoch 34/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 0.0011 - mae: 0.0235 - val_loss: 0.0048 - val_mae: 0.0481 - learning_rate: 0.0010\n",
      "Epoch 35/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 9.9181e-04 - mae: 0.0220 - val_loss: 0.0044 - val_mae: 0.0462 - learning_rate: 0.0010\n",
      "Epoch 36/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0011 - mae: 0.0224 - val_loss: 0.0046 - val_mae: 0.0464 - learning_rate: 0.0010\n",
      "Epoch 37/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0010 - mae: 0.0224 - val_loss: 0.0043 - val_mae: 0.0455 - learning_rate: 0.0010\n",
      "Epoch 38/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0010 - mae: 0.0220 - val_loss: 0.0043 - val_mae: 0.0460 - learning_rate: 0.0010\n",
      "Epoch 39/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0011 - mae: 0.0235 - val_loss: 0.0048 - val_mae: 0.0507 - learning_rate: 0.0010\n",
      "Epoch 40/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0011 - mae: 0.0230 - val_loss: 0.0044 - val_mae: 0.0457 - learning_rate: 0.0010\n",
      "Epoch 41/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.0010 - mae: 0.0226 - val_loss: 0.0041 - val_mae: 0.0445 - learning_rate: 0.0010\n",
      "Epoch 42/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 9.6882e-04 - mae: 0.0215 - val_loss: 0.0042 - val_mae: 0.0451 - learning_rate: 0.0010\n",
      "Epoch 43/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0011 - mae: 0.0227 - val_loss: 0.0042 - val_mae: 0.0435 - learning_rate: 0.0010\n",
      "Epoch 44/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 8.9087e-04 - mae: 0.0206 - val_loss: 0.0043 - val_mae: 0.0444 - learning_rate: 0.0010\n",
      "Epoch 45/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 8.3327e-04 - mae: 0.0204 - val_loss: 0.0041 - val_mae: 0.0451 - learning_rate: 0.0010\n",
      "Epoch 46/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 9.9488e-04 - mae: 0.0214 - val_loss: 0.0039 - val_mae: 0.0433 - learning_rate: 5.0000e-04\n",
      "Epoch 47/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 8.8406e-04 - mae: 0.0203 - val_loss: 0.0038 - val_mae: 0.0435 - learning_rate: 5.0000e-04\n",
      "Epoch 48/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 9.6051e-04 - mae: 0.0210 - val_loss: 0.0039 - val_mae: 0.0442 - learning_rate: 5.0000e-04\n",
      "Epoch 49/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0010 - mae: 0.0222 - val_loss: 0.0039 - val_mae: 0.0442 - learning_rate: 5.0000e-04\n",
      "Epoch 50/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0011 - mae: 0.0219 - val_loss: 0.0037 - val_mae: 0.0422 - learning_rate: 5.0000e-04\n",
      "Epoch 51/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 8.4999e-04 - mae: 0.0206 - val_loss: 0.0038 - val_mae: 0.0426 - learning_rate: 5.0000e-04\n",
      "Epoch 52/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 8.5461e-04 - mae: 0.0203 - val_loss: 0.0037 - val_mae: 0.0424 - learning_rate: 5.0000e-04\n",
      "Epoch 53/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 8.5888e-04 - mae: 0.0207 - val_loss: 0.0038 - val_mae: 0.0427 - learning_rate: 5.0000e-04\n",
      "Epoch 54/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 9.2012e-04 - mae: 0.0207 - val_loss: 0.0037 - val_mae: 0.0421 - learning_rate: 5.0000e-04\n",
      "Epoch 55/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 9.4364e-04 - mae: 0.0201 - val_loss: 0.0037 - val_mae: 0.0412 - learning_rate: 2.5000e-04\n",
      "Epoch 56/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 7.6249e-04 - mae: 0.0195 - val_loss: 0.0040 - val_mae: 0.0416 - learning_rate: 2.5000e-04\n",
      "Epoch 57/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 8.9661e-04 - mae: 0.0200 - val_loss: 0.0041 - val_mae: 0.0420 - learning_rate: 2.5000e-04\n",
      "Epoch 58/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 7.6243e-04 - mae: 0.0192 - val_loss: 0.0037 - val_mae: 0.0411 - learning_rate: 2.5000e-04\n",
      "Epoch 59/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 9.1622e-04 - mae: 0.0201 - val_loss: 0.0039 - val_mae: 0.0416 - learning_rate: 1.2500e-04\n",
      "Epoch 60/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 7.7074e-04 - mae: 0.0188 - val_loss: 0.0037 - val_mae: 0.0407 - learning_rate: 1.2500e-04\n",
      "\u001b[1m24/24\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… tomato_2010_2024_master_weekly.csv | MAE=126.31, RMSE=200.42, RÂ²=0.933, MAPE=9.42%, Accuracy=90.58%\n",
      "\n",
      "ğŸš€ Processing Crop File: wheat_2010_2024_master_weekly.csv\n",
      "Epoch 1/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 76ms/step - loss: 0.0222 - mae: 0.1000 - val_loss: 0.0254 - val_mae: 0.1514 - learning_rate: 0.0010\n",
      "Epoch 2/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 0.0028 - mae: 0.0401 - val_loss: 0.0142 - val_mae: 0.1111 - learning_rate: 0.0010\n",
      "Epoch 3/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.0023 - mae: 0.0346 - val_loss: 0.0179 - val_mae: 0.1267 - learning_rate: 0.0010\n",
      "Epoch 4/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.0021 - mae: 0.0337 - val_loss: 0.0202 - val_mae: 0.1353 - learning_rate: 0.0010\n",
      "Epoch 5/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0024 - mae: 0.0350 - val_loss: 0.0145 - val_mae: 0.1121 - learning_rate: 0.0010\n",
      "Epoch 6/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.0020 - mae: 0.0322 - val_loss: 0.0229 - val_mae: 0.1445 - learning_rate: 0.0010\n",
      "Epoch 7/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0018 - mae: 0.0309 - val_loss: 0.0246 - val_mae: 0.1497 - learning_rate: 5.0000e-04\n",
      "Epoch 8/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 0.0017 - mae: 0.0296 - val_loss: 0.0197 - val_mae: 0.1327 - learning_rate: 5.0000e-04\n",
      "Epoch 9/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 0.0015 - mae: 0.0280 - val_loss: 0.0194 - val_mae: 0.1318 - learning_rate: 5.0000e-04\n",
      "Epoch 10/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0018 - mae: 0.0299 - val_loss: 0.0225 - val_mae: 0.1428 - learning_rate: 5.0000e-04\n",
      "\u001b[1m24/24\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… wheat_2010_2024_master_weekly.csv | MAE=84.73, RMSE=119.03, RÂ²=0.9423, MAPE=3.5%, Accuracy=96.5%\n",
      "\n",
      "ğŸ“Š All crops processed â€” metrics saved to lstm_metrics_weekly.csv\n",
      "                               Crop     MAE    RMSE    R2  MAPE(%)  \\\n",
      "0  capsicum_2010_2024_master_weekly  245.60  333.03  0.78     9.50   \n",
      "1     onion_2010_2024_master_weekly  125.15  201.30  0.91     7.06   \n",
      "2    tomato_2010_2024_master_weekly  126.31  200.42  0.93     9.42   \n",
      "3     wheat_2010_2024_master_weekly   84.73  119.03  0.94     3.50   \n",
      "\n",
      "   Accuracy(%)  \n",
      "0        90.50  \n",
      "1        92.94  \n",
      "2        90.58  \n",
      "3        96.50  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers, callbacks\n",
    "\n",
    "# -----------------------------\n",
    "# Paths / outputs\n",
    "# -----------------------------\n",
    "input_folder = \"dataset\"   # weekly CSVs\n",
    "output_models = \"lstm_output_models_weekly\"\n",
    "output_csv = \"lstm_output_csv_weekly\"\n",
    "output_graphs = \"lstm_output_graphs_weekly\"\n",
    "metrics_file = \"lstm_metrics_weekly.csv\"\n",
    "\n",
    "os.makedirs(output_models, exist_ok=True)\n",
    "os.makedirs(output_csv, exist_ok=True)\n",
    "os.makedirs(output_graphs, exist_ok=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Config (tweak if needed)\n",
    "# -----------------------------\n",
    "look_back = 30       # weeks of history used as input\n",
    "batch_size = 32\n",
    "epochs = 60\n",
    "val_split = 0.15\n",
    "lstm_units = 64\n",
    "dropout_rate = 0.2\n",
    "seed = 42\n",
    "\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "def parse_dates_safe(s):\n",
    "    try:\n",
    "        return pd.to_datetime(s, dayfirst=True)\n",
    "    except:\n",
    "        return pd.to_datetime(s, dayfirst=False)\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    mask = y_true != 0\n",
    "    if mask.sum() == 0:\n",
    "        return np.nan\n",
    "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
    "\n",
    "def create_multivariate_dataset(y_scaled, exog_scaled, look_back):\n",
    "    \"\"\"\n",
    "    Build multivariate sequences:\n",
    "      y_scaled: (n,1), exog_scaled: (n,k)\n",
    "    returns:\n",
    "      X shape (n-look_back, look_back, 1+k), y shape (n-look_back,)\n",
    "    \"\"\"\n",
    "    n = len(y_scaled)\n",
    "    k = exog_scaled.shape[1] if exog_scaled is not None else 0\n",
    "    X, Y = [], []\n",
    "    for i in range(n - look_back):\n",
    "        seq_y = y_scaled[i:i+look_back, 0]\n",
    "        if k > 0:\n",
    "            seq_exog = exog_scaled[i:i+look_back, :]\n",
    "            seq = np.concatenate([seq_y.reshape(-1,1), seq_exog], axis=1)\n",
    "        else:\n",
    "            seq = seq_y.reshape(-1,1)\n",
    "        X.append(seq)\n",
    "        Y.append(y_scaled[i+look_back, 0])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "# -----------------------------\n",
    "# Build LSTM Model\n",
    "# -----------------------------\n",
    "def build_lstm_model(input_shape, units=64, dropout_rate=0.2, lr=0.001):\n",
    "    \"\"\"\n",
    "    input_shape: (seq_len, num_features)\n",
    "    returns compiled Keras model\n",
    "    \"\"\"\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Input(shape=input_shape))\n",
    "    model.add(layers.LSTM(units, return_sequences=True))\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    model.add(layers.LSTM(units//2, return_sequences=False))\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    model.add(layers.Dense(units//2, activation='relu'))\n",
    "    model.add(layers.Dense(1))\n",
    "    model.compile(optimizer=optimizers.Adam(learning_rate=lr), loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# -----------------------------\n",
    "# Main loop â€” process weekly crop files\n",
    "# -----------------------------\n",
    "metrics_list = []\n",
    "\n",
    "for file in sorted(os.listdir(input_folder)):\n",
    "    if not file.endswith(\".csv\"):\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nğŸš€ Processing Crop File: {file}\")\n",
    "    file_path = os.path.join(input_folder, file)\n",
    "\n",
    "    # -------------------------\n",
    "    # Load & parse\n",
    "    # -------------------------\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['Date'] = parse_dates_safe(df['Date'])\n",
    "    df = df.sort_values('Date')\n",
    "\n",
    "    # aggregate duplicate dates numeric mean\n",
    "    df = df.groupby('Date', as_index=False).mean(numeric_only=True)\n",
    "    df.set_index('Date', inplace=True)\n",
    "\n",
    "    # weekly frequency alignment\n",
    "    df = df.asfreq('W')\n",
    "\n",
    "    # check column\n",
    "    if 'Average Price' not in df.columns:\n",
    "        print(f\"âš  'Average Price' not found in {file}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    # fill missing price values\n",
    "    df['Average Price'] = df['Average Price'].ffill().bfill().fillna(df['Average Price'].mean())\n",
    "\n",
    "    # -------------------------\n",
    "    # Feature engineering (weekly)\n",
    "    # -------------------------\n",
    "    df['Lag_1'] = df['Average Price'].shift(1)\n",
    "    df['Lag_4'] = df['Average Price'].shift(4)\n",
    "    df['MA_4'] = df['Average Price'].rolling(window=4).mean()\n",
    "    df['MA_12'] = df['Average Price'].rolling(window=12).mean()\n",
    "    # fill feature NaNs\n",
    "    df[['Lag_1','Lag_4','MA_4','MA_12']] = df[['Lag_1','Lag_4','MA_4','MA_12']].bfill().ffill()\n",
    "\n",
    "    # -------------------------\n",
    "    # Prepare arrays & scalers\n",
    "    # -------------------------\n",
    "    y = df[['Average Price']].values.astype('float32')\n",
    "    exog_cols = ['Lag_1','Lag_4','MA_4','MA_12']\n",
    "    exog = df[exog_cols].values.astype('float32')\n",
    "\n",
    "    # scale\n",
    "    y_scaler = MinMaxScaler()\n",
    "    exog_scaler = MinMaxScaler()\n",
    "    y_scaled = y_scaler.fit_transform(y)\n",
    "    exog_scaled = exog_scaler.fit_transform(exog)\n",
    "\n",
    "    # sequences\n",
    "    X_all, y_all = create_multivariate_dataset(y_scaled, exog_scaled, look_back)\n",
    "    if len(X_all) == 0:\n",
    "        print(f\"âš  Not enough data after look_back={look_back} in {file}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # train/val split (chronological)\n",
    "    val_size = int(len(X_all) * val_split)\n",
    "    train_size = len(X_all) - val_size\n",
    "    X_train, y_train = X_all[:train_size], y_all[:train_size]\n",
    "    X_val, y_val = X_all[train_size:], y_all[train_size:]\n",
    "\n",
    "    # -------------------------\n",
    "    # Build model & train\n",
    "    # -------------------------\n",
    "    input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "    model = build_lstm_model(input_shape, units=lstm_units, dropout_rate=dropout_rate)\n",
    "\n",
    "    es = callbacks.EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True)\n",
    "    rl = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, min_lr=1e-6)\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=[es, rl],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # Predict on all sequences and align with original index\n",
    "    # -------------------------\n",
    "    preds_scaled = model.predict(X_all, batch_size=batch_size)\n",
    "    preds = y_scaler.inverse_transform(preds_scaled).flatten()\n",
    "\n",
    "    # insert Predicted column with NaNs for first look_back rows\n",
    "    df['Predicted'] = np.nan\n",
    "    start_idx = look_back\n",
    "    df.iloc[start_idx : start_idx + len(preds), df.columns.get_loc('Predicted')] = preds\n",
    "\n",
    "    # round numeric columns to 2 decimals\n",
    "    for col in ['Average Price','Predicted','Lag_1','Lag_4','MA_4','MA_12']:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].round(2)\n",
    "\n",
    "    # -------------------------\n",
    "    # Metrics (on rows with predictions)\n",
    "    # -------------------------\n",
    "    mask = ~np.isnan(df['Predicted'].values)\n",
    "    y_true = df.loc[mask, 'Average Price'].values\n",
    "    y_pred = df.loc[mask, 'Predicted'].values\n",
    "\n",
    "    mae = round(mean_absolute_error(y_true, y_pred), 2)\n",
    "    rmse = round(np.sqrt(mean_squared_error(y_true, y_pred)), 2)\n",
    "    r2 = round(r2_score(y_true, y_pred), 4)\n",
    "    mape = round(mean_absolute_percentage_error(y_true, y_pred), 2)\n",
    "    accuracy = round(100 - mape, 2)\n",
    "\n",
    "    metrics_list.append({\n",
    "        'Crop': file.replace('.csv',''),\n",
    "        'MAE': mae,\n",
    "        'RMSE': rmse,\n",
    "        'R2': r2,\n",
    "        'MAPE(%)': mape,\n",
    "        'Accuracy(%)': accuracy\n",
    "    })\n",
    "\n",
    "    print(f\"âœ… {file} | MAE={mae}, RMSE={rmse}, RÂ²={r2}, MAPE={mape}%, Accuracy={accuracy}%\")\n",
    "\n",
    "    # -------------------------\n",
    "    # Save model, CSV, graph\n",
    "    # -------------------------\n",
    "    model_save_path = os.path.join(output_models, file.replace('.csv','_lstm_weekly.h5'))\n",
    "    model.save(model_save_path)\n",
    "\n",
    "    out_csv = os.path.join(output_csv, file.replace('.csv','_lstm_weekly_updated.csv'))\n",
    "    df_reset = df.reset_index()\n",
    "    df_reset.to_csv(out_csv, index=False, float_format='%.2f')\n",
    "\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.plot(df_reset['Date'], df_reset['Average Price'], label='Actual', color='blue')\n",
    "    plt.plot(df_reset['Date'], df_reset['Predicted'], label='Predicted (LSTM)', color='red', linestyle='dashed')\n",
    "    plt.plot(df_reset['Date'], df_reset['MA_4'], label='MA_4', color='green', linestyle='--')\n",
    "    plt.plot(df_reset['Date'], df_reset['MA_12'], label='MA_12', color='orange', linestyle='--')\n",
    "    plt.title(f\"LSTM Weekly Forecast - {file}\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Average Price\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_graphs, file.replace('.csv','_lstm_weekly_graph.png')))\n",
    "    plt.close()\n",
    "\n",
    "    # cleanup\n",
    "    del model, X_all, X_train, X_val, y_all, y_train, y_val, preds_scaled, preds\n",
    "    gc.collect()\n",
    "\n",
    "# -----------------------------\n",
    "# Save metrics summary (all crops)\n",
    "# -----------------------------\n",
    "metrics_df = pd.DataFrame(metrics_list).round(2)\n",
    "metrics_df.to_csv(metrics_file, index=False)\n",
    "print(f\"\\nğŸ“Š All crops processed â€” metrics saved to {metrics_file}\")\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d657938-3b12-41cc-96a5-3cf7d7a60f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ee6efc1-2de1-4d62-b22e-9d4f2299f5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ Processing Crop File: capsicum_2010_2024_master_weekly.csv\n",
      "Epoch 1/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 173ms/step - loss: 0.2820 - mae: 0.3763 - val_loss: 0.0511 - val_mae: 0.1876 - learning_rate: 0.0010\n",
      "Epoch 2/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - loss: 0.0654 - mae: 0.2075 - val_loss: 0.0329 - val_mae: 0.1389 - learning_rate: 0.0010\n",
      "Epoch 3/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 0.0422 - mae: 0.1625 - val_loss: 0.0397 - val_mae: 0.1578 - learning_rate: 0.0010\n",
      "Epoch 4/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 46ms/step - loss: 0.0287 - mae: 0.1354 - val_loss: 0.0626 - val_mae: 0.2160 - learning_rate: 0.0010\n",
      "Epoch 5/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - loss: 0.0272 - mae: 0.1319 - val_loss: 0.0541 - val_mae: 0.1958 - learning_rate: 0.0010\n",
      "Epoch 6/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 66ms/step - loss: 0.0242 - mae: 0.1235 - val_loss: 0.0442 - val_mae: 0.1709 - learning_rate: 0.0010\n",
      "Epoch 7/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - loss: 0.0217 - mae: 0.1180 - val_loss: 0.0484 - val_mae: 0.1820 - learning_rate: 5.0000e-04\n",
      "Epoch 8/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - loss: 0.0209 - mae: 0.1149 - val_loss: 0.0502 - val_mae: 0.1866 - learning_rate: 5.0000e-04\n",
      "Epoch 9/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - loss: 0.0215 - mae: 0.1164 - val_loss: 0.0467 - val_mae: 0.1780 - learning_rate: 5.0000e-04\n",
      "Epoch 10/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - loss: 0.0201 - mae: 0.1126 - val_loss: 0.0470 - val_mae: 0.1787 - learning_rate: 5.0000e-04\n",
      "\u001b[1m24/24\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 81ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… capsicum_2010_2024_master_weekly.csv | MAE=545.96, RMSE=655.26, RÂ²=0.1673, MAPE=25.26%, Accuracy=74.74%\n",
      "\n",
      "ğŸš€ Processing Crop File: onion_2010_2024_master_weekly.csv\n",
      "Epoch 1/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 183ms/step - loss: 0.1048 - mae: 0.2503 - val_loss: 0.0409 - val_mae: 0.1515 - learning_rate: 0.0010\n",
      "Epoch 2/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - loss: 0.0298 - mae: 0.1289 - val_loss: 0.0629 - val_mae: 0.1878 - learning_rate: 0.0010\n",
      "Epoch 3/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 0.0201 - mae: 0.1057 - val_loss: 0.0575 - val_mae: 0.1782 - learning_rate: 0.0010\n",
      "Epoch 4/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - loss: 0.0183 - mae: 0.0992 - val_loss: 0.0440 - val_mae: 0.1535 - learning_rate: 0.0010\n",
      "Epoch 5/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 0.0171 - mae: 0.0959 - val_loss: 0.0491 - val_mae: 0.1632 - learning_rate: 0.0010\n",
      "Epoch 6/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - loss: 0.0156 - mae: 0.0894 - val_loss: 0.0403 - val_mae: 0.1477 - learning_rate: 5.0000e-04\n",
      "Epoch 7/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 0.0157 - mae: 0.0900 - val_loss: 0.0411 - val_mae: 0.1491 - learning_rate: 5.0000e-04\n",
      "Epoch 8/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 0.0150 - mae: 0.0886 - val_loss: 0.0399 - val_mae: 0.1477 - learning_rate: 5.0000e-04\n",
      "Epoch 9/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 0.0154 - mae: 0.0882 - val_loss: 0.0410 - val_mae: 0.1491 - learning_rate: 5.0000e-04\n",
      "Epoch 10/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - loss: 0.0162 - mae: 0.0907 - val_loss: 0.0344 - val_mae: 0.1401 - learning_rate: 5.0000e-04\n",
      "Epoch 11/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - loss: 0.0149 - mae: 0.0863 - val_loss: 0.0368 - val_mae: 0.1428 - learning_rate: 5.0000e-04\n",
      "Epoch 12/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - loss: 0.0143 - mae: 0.0852 - val_loss: 0.0369 - val_mae: 0.1429 - learning_rate: 5.0000e-04\n",
      "Epoch 13/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 0.0132 - mae: 0.0806 - val_loss: 0.0317 - val_mae: 0.1361 - learning_rate: 5.0000e-04\n",
      "Epoch 14/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - loss: 0.0134 - mae: 0.0795 - val_loss: 0.0348 - val_mae: 0.1398 - learning_rate: 5.0000e-04\n",
      "Epoch 15/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - loss: 0.0143 - mae: 0.0826 - val_loss: 0.0244 - val_mae: 0.1269 - learning_rate: 5.0000e-04\n",
      "Epoch 16/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 0.0130 - mae: 0.0793 - val_loss: 0.0256 - val_mae: 0.1271 - learning_rate: 5.0000e-04\n",
      "Epoch 17/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - loss: 0.0115 - mae: 0.0741 - val_loss: 0.0253 - val_mae: 0.1263 - learning_rate: 5.0000e-04\n",
      "Epoch 18/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - loss: 0.0132 - mae: 0.0789 - val_loss: 0.0237 - val_mae: 0.1229 - learning_rate: 5.0000e-04\n",
      "Epoch 19/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 0.0114 - mae: 0.0731 - val_loss: 0.0251 - val_mae: 0.1243 - learning_rate: 5.0000e-04\n",
      "Epoch 20/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0109 - mae: 0.0732 - val_loss: 0.0223 - val_mae: 0.1193 - learning_rate: 5.0000e-04\n",
      "Epoch 21/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 0.0111 - mae: 0.0734 - val_loss: 0.0221 - val_mae: 0.1183 - learning_rate: 5.0000e-04\n",
      "Epoch 22/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - loss: 0.0109 - mae: 0.0713 - val_loss: 0.0247 - val_mae: 0.1237 - learning_rate: 5.0000e-04\n",
      "Epoch 23/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 0.0099 - mae: 0.0680 - val_loss: 0.0232 - val_mae: 0.1208 - learning_rate: 5.0000e-04\n",
      "Epoch 24/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 0.0106 - mae: 0.0698 - val_loss: 0.0165 - val_mae: 0.1070 - learning_rate: 5.0000e-04\n",
      "Epoch 25/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 0.0109 - mae: 0.0730 - val_loss: 0.0166 - val_mae: 0.1053 - learning_rate: 5.0000e-04\n",
      "Epoch 26/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - loss: 0.0105 - mae: 0.0693 - val_loss: 0.0173 - val_mae: 0.1062 - learning_rate: 5.0000e-04\n",
      "Epoch 27/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - loss: 0.0093 - mae: 0.0665 - val_loss: 0.0183 - val_mae: 0.1081 - learning_rate: 5.0000e-04\n",
      "Epoch 28/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - loss: 0.0096 - mae: 0.0655 - val_loss: 0.0166 - val_mae: 0.1039 - learning_rate: 5.0000e-04\n",
      "Epoch 29/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - loss: 0.0082 - mae: 0.0615 - val_loss: 0.0181 - val_mae: 0.1068 - learning_rate: 2.5000e-04\n",
      "Epoch 30/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - loss: 0.0082 - mae: 0.0624 - val_loss: 0.0185 - val_mae: 0.1075 - learning_rate: 2.5000e-04\n",
      "Epoch 31/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 0.0080 - mae: 0.0625 - val_loss: 0.0178 - val_mae: 0.1060 - learning_rate: 2.5000e-04\n",
      "Epoch 32/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 0.0087 - mae: 0.0639 - val_loss: 0.0192 - val_mae: 0.1082 - learning_rate: 2.5000e-04\n",
      "\u001b[1m24/24\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 77ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… onion_2010_2024_master_weekly.csv | MAE=306.61, RMSE=380.94, RÂ²=0.6713, MAPE=19.93%, Accuracy=80.07%\n",
      "\n",
      "ğŸš€ Processing Crop File: tomato_2010_2024_master_weekly.csv\n",
      "Epoch 1/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 141ms/step - loss: 0.0908 - mae: 0.2169 - val_loss: 0.0359 - val_mae: 0.1140 - learning_rate: 0.0010\n",
      "Epoch 2/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - loss: 0.0138 - mae: 0.0893 - val_loss: 0.0431 - val_mae: 0.1251 - learning_rate: 0.0010\n",
      "Epoch 3/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - loss: 0.0105 - mae: 0.0769 - val_loss: 0.0414 - val_mae: 0.1195 - learning_rate: 0.0010\n",
      "Epoch 4/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 0.0087 - mae: 0.0681 - val_loss: 0.0400 - val_mae: 0.1171 - learning_rate: 0.0010\n",
      "Epoch 5/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 0.0087 - mae: 0.0688 - val_loss: 0.0346 - val_mae: 0.1091 - learning_rate: 0.0010\n",
      "Epoch 6/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 0.0085 - mae: 0.0682 - val_loss: 0.0370 - val_mae: 0.1115 - learning_rate: 0.0010\n",
      "Epoch 7/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 0.0073 - mae: 0.0620 - val_loss: 0.0354 - val_mae: 0.1107 - learning_rate: 0.0010\n",
      "Epoch 8/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 0.0067 - mae: 0.0602 - val_loss: 0.0377 - val_mae: 0.1131 - learning_rate: 0.0010\n",
      "Epoch 9/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0071 - mae: 0.0609 - val_loss: 0.0343 - val_mae: 0.1114 - learning_rate: 0.0010\n",
      "Epoch 10/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 0.0073 - mae: 0.0624 - val_loss: 0.0351 - val_mae: 0.1115 - learning_rate: 0.0010\n",
      "Epoch 11/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0068 - mae: 0.0583 - val_loss: 0.0356 - val_mae: 0.1120 - learning_rate: 0.0010\n",
      "Epoch 12/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 0.0071 - mae: 0.0603 - val_loss: 0.0324 - val_mae: 0.1119 - learning_rate: 0.0010\n",
      "Epoch 13/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.0067 - mae: 0.0587 - val_loss: 0.0354 - val_mae: 0.1121 - learning_rate: 0.0010\n",
      "Epoch 14/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 0.0066 - mae: 0.0585 - val_loss: 0.0337 - val_mae: 0.1114 - learning_rate: 0.0010\n",
      "Epoch 15/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 0.0064 - mae: 0.0577 - val_loss: 0.0346 - val_mae: 0.1112 - learning_rate: 0.0010\n",
      "Epoch 16/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - loss: 0.0067 - mae: 0.0589 - val_loss: 0.0316 - val_mae: 0.1116 - learning_rate: 0.0010\n",
      "Epoch 17/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - loss: 0.0062 - mae: 0.0555 - val_loss: 0.0360 - val_mae: 0.1126 - learning_rate: 0.0010\n",
      "Epoch 18/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 0.0063 - mae: 0.0567 - val_loss: 0.0335 - val_mae: 0.1108 - learning_rate: 0.0010\n",
      "Epoch 19/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - loss: 0.0065 - mae: 0.0582 - val_loss: 0.0351 - val_mae: 0.1132 - learning_rate: 0.0010\n",
      "Epoch 20/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 0.0062 - mae: 0.0541 - val_loss: 0.0323 - val_mae: 0.1096 - learning_rate: 0.0010\n",
      "Epoch 21/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - loss: 0.0059 - mae: 0.0546 - val_loss: 0.0298 - val_mae: 0.1104 - learning_rate: 5.0000e-04\n",
      "Epoch 22/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 0.0058 - mae: 0.0534 - val_loss: 0.0301 - val_mae: 0.1094 - learning_rate: 5.0000e-04\n",
      "Epoch 23/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 0.0057 - mae: 0.0537 - val_loss: 0.0300 - val_mae: 0.1094 - learning_rate: 5.0000e-04\n",
      "Epoch 24/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 0.0056 - mae: 0.0526 - val_loss: 0.0292 - val_mae: 0.1086 - learning_rate: 5.0000e-04\n",
      "Epoch 25/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 0.0056 - mae: 0.0516 - val_loss: 0.0297 - val_mae: 0.1066 - learning_rate: 5.0000e-04\n",
      "Epoch 26/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 0.0056 - mae: 0.0544 - val_loss: 0.0295 - val_mae: 0.1068 - learning_rate: 5.0000e-04\n",
      "Epoch 27/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 0.0057 - mae: 0.0538 - val_loss: 0.0287 - val_mae: 0.1063 - learning_rate: 5.0000e-04\n",
      "Epoch 28/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 0.0054 - mae: 0.0519 - val_loss: 0.0276 - val_mae: 0.1049 - learning_rate: 5.0000e-04\n",
      "Epoch 29/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - loss: 0.0052 - mae: 0.0523 - val_loss: 0.0269 - val_mae: 0.1046 - learning_rate: 5.0000e-04\n",
      "Epoch 30/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - loss: 0.0057 - mae: 0.0527 - val_loss: 0.0293 - val_mae: 0.1048 - learning_rate: 5.0000e-04\n",
      "Epoch 31/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 0.0050 - mae: 0.0503 - val_loss: 0.0262 - val_mae: 0.1018 - learning_rate: 5.0000e-04\n",
      "Epoch 32/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 0.0053 - mae: 0.0517 - val_loss: 0.0272 - val_mae: 0.1024 - learning_rate: 5.0000e-04\n",
      "Epoch 33/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 0.0047 - mae: 0.0498 - val_loss: 0.0255 - val_mae: 0.0994 - learning_rate: 5.0000e-04\n",
      "Epoch 34/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 0.0048 - mae: 0.0508 - val_loss: 0.0256 - val_mae: 0.0993 - learning_rate: 5.0000e-04\n",
      "Epoch 35/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 0.0048 - mae: 0.0469 - val_loss: 0.0256 - val_mae: 0.0982 - learning_rate: 5.0000e-04\n",
      "Epoch 36/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 0.0043 - mae: 0.0461 - val_loss: 0.0234 - val_mae: 0.0955 - learning_rate: 5.0000e-04\n",
      "Epoch 37/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - loss: 0.0048 - mae: 0.0502 - val_loss: 0.0254 - val_mae: 0.0969 - learning_rate: 5.0000e-04\n",
      "Epoch 38/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 0.0040 - mae: 0.0465 - val_loss: 0.0249 - val_mae: 0.0957 - learning_rate: 5.0000e-04\n",
      "Epoch 39/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 0.0038 - mae: 0.0442 - val_loss: 0.0258 - val_mae: 0.0984 - learning_rate: 5.0000e-04\n",
      "Epoch 40/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 0.0040 - mae: 0.0445 - val_loss: 0.0217 - val_mae: 0.0933 - learning_rate: 5.0000e-04\n",
      "Epoch 41/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - loss: 0.0043 - mae: 0.0468 - val_loss: 0.0248 - val_mae: 0.0966 - learning_rate: 5.0000e-04\n",
      "Epoch 42/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 0.0042 - mae: 0.0452 - val_loss: 0.0243 - val_mae: 0.0962 - learning_rate: 5.0000e-04\n",
      "Epoch 43/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - loss: 0.0040 - mae: 0.0446 - val_loss: 0.0223 - val_mae: 0.0935 - learning_rate: 5.0000e-04\n",
      "Epoch 44/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - loss: 0.0041 - mae: 0.0437 - val_loss: 0.0235 - val_mae: 0.0943 - learning_rate: 5.0000e-04\n",
      "Epoch 45/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 0.0035 - mae: 0.0438 - val_loss: 0.0212 - val_mae: 0.0890 - learning_rate: 2.5000e-04\n",
      "Epoch 46/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 0.0040 - mae: 0.0436 - val_loss: 0.0202 - val_mae: 0.0877 - learning_rate: 2.5000e-04\n",
      "Epoch 47/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 0.0032 - mae: 0.0412 - val_loss: 0.0204 - val_mae: 0.0876 - learning_rate: 2.5000e-04\n",
      "Epoch 48/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 0.0033 - mae: 0.0419 - val_loss: 0.0207 - val_mae: 0.0877 - learning_rate: 2.5000e-04\n",
      "Epoch 49/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 0.0032 - mae: 0.0400 - val_loss: 0.0200 - val_mae: 0.0869 - learning_rate: 2.5000e-04\n",
      "Epoch 50/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - loss: 0.0036 - mae: 0.0416 - val_loss: 0.0190 - val_mae: 0.0867 - learning_rate: 2.5000e-04\n",
      "Epoch 51/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - loss: 0.0033 - mae: 0.0414 - val_loss: 0.0200 - val_mae: 0.0856 - learning_rate: 2.5000e-04\n",
      "Epoch 52/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - loss: 0.0032 - mae: 0.0391 - val_loss: 0.0189 - val_mae: 0.0848 - learning_rate: 2.5000e-04\n",
      "Epoch 53/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 0.0036 - mae: 0.0411 - val_loss: 0.0191 - val_mae: 0.0845 - learning_rate: 2.5000e-04\n",
      "Epoch 54/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - loss: 0.0032 - mae: 0.0391 - val_loss: 0.0187 - val_mae: 0.0841 - learning_rate: 2.5000e-04\n",
      "Epoch 55/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 0.0037 - mae: 0.0427 - val_loss: 0.0215 - val_mae: 0.0893 - learning_rate: 2.5000e-04\n",
      "Epoch 56/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - loss: 0.0032 - mae: 0.0410 - val_loss: 0.0187 - val_mae: 0.0850 - learning_rate: 2.5000e-04\n",
      "Epoch 57/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - loss: 0.0032 - mae: 0.0402 - val_loss: 0.0186 - val_mae: 0.0838 - learning_rate: 2.5000e-04\n",
      "Epoch 58/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - loss: 0.0033 - mae: 0.0411 - val_loss: 0.0189 - val_mae: 0.0836 - learning_rate: 2.5000e-04\n",
      "Epoch 59/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - loss: 0.0036 - mae: 0.0408 - val_loss: 0.0194 - val_mae: 0.0849 - learning_rate: 1.2500e-04\n",
      "Epoch 60/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - loss: 0.0033 - mae: 0.0397 - val_loss: 0.0193 - val_mae: 0.0845 - learning_rate: 1.2500e-04\n",
      "\u001b[1m24/24\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 110ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… tomato_2010_2024_master_weekly.csv | MAE=245.38, RMSE=413.59, RÂ²=0.7147, MAPE=18.13%, Accuracy=81.87%\n",
      "\n",
      "ğŸš€ Processing Crop File: wheat_2010_2024_master_weekly.csv\n",
      "Epoch 1/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 137ms/step - loss: 0.1433 - mae: 0.2872 - val_loss: 0.0357 - val_mae: 0.1741 - learning_rate: 0.0010\n",
      "Epoch 2/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 0.0204 - mae: 0.1126 - val_loss: 0.0452 - val_mae: 0.1981 - learning_rate: 0.0010\n",
      "Epoch 3/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 0.0129 - mae: 0.0919 - val_loss: 0.0970 - val_mae: 0.3028 - learning_rate: 0.0010\n",
      "Epoch 4/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 0.0104 - mae: 0.0818 - val_loss: 0.0697 - val_mae: 0.2543 - learning_rate: 0.0010\n",
      "Epoch 5/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - loss: 0.0083 - mae: 0.0698 - val_loss: 0.0385 - val_mae: 0.1845 - learning_rate: 0.0010\n",
      "Epoch 6/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - loss: 0.0057 - mae: 0.0590 - val_loss: 0.0419 - val_mae: 0.1940 - learning_rate: 5.0000e-04\n",
      "Epoch 7/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - loss: 0.0058 - mae: 0.0583 - val_loss: 0.0352 - val_mae: 0.1761 - learning_rate: 5.0000e-04\n",
      "Epoch 8/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0044 - mae: 0.0527 - val_loss: 0.0289 - val_mae: 0.1575 - learning_rate: 5.0000e-04\n",
      "Epoch 9/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0046 - mae: 0.0536 - val_loss: 0.0337 - val_mae: 0.1722 - learning_rate: 5.0000e-04\n",
      "Epoch 10/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - loss: 0.0042 - mae: 0.0506 - val_loss: 0.0254 - val_mae: 0.1471 - learning_rate: 5.0000e-04\n",
      "Epoch 11/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - loss: 0.0040 - mae: 0.0489 - val_loss: 0.0163 - val_mae: 0.1123 - learning_rate: 5.0000e-04\n",
      "Epoch 12/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - loss: 0.0042 - mae: 0.0504 - val_loss: 0.0370 - val_mae: 0.1812 - learning_rate: 5.0000e-04\n",
      "Epoch 13/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - loss: 0.0040 - mae: 0.0483 - val_loss: 0.0269 - val_mae: 0.1526 - learning_rate: 5.0000e-04\n",
      "Epoch 14/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - loss: 0.0036 - mae: 0.0461 - val_loss: 0.0288 - val_mae: 0.1593 - learning_rate: 5.0000e-04\n",
      "Epoch 15/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - loss: 0.0035 - mae: 0.0448 - val_loss: 0.0377 - val_mae: 0.1841 - learning_rate: 5.0000e-04\n",
      "Epoch 16/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - loss: 0.0033 - mae: 0.0436 - val_loss: 0.0306 - val_mae: 0.1635 - learning_rate: 2.5000e-04\n",
      "Epoch 17/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - loss: 0.0034 - mae: 0.0442 - val_loss: 0.0311 - val_mae: 0.1654 - learning_rate: 2.5000e-04\n",
      "Epoch 18/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - loss: 0.0035 - mae: 0.0449 - val_loss: 0.0319 - val_mae: 0.1678 - learning_rate: 2.5000e-04\n",
      "Epoch 19/60\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 0.0035 - mae: 0.0450 - val_loss: 0.0293 - val_mae: 0.1597 - learning_rate: 2.5000e-04\n",
      "\u001b[1m24/24\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… wheat_2010_2024_master_weekly.csv | MAE=97.85, RMSE=134.53, RÂ²=0.9263, MAPE=4.19%, Accuracy=95.81%\n",
      "\n",
      "ğŸ“Š All crops processed â€” metrics saved to gru_transformer_metrics_weekly.csv\n",
      "                               Crop     MAE    RMSE    R2  MAPE(%)  \\\n",
      "0  capsicum_2010_2024_master_weekly  545.96  655.26  0.17    25.26   \n",
      "1     onion_2010_2024_master_weekly  306.61  380.94  0.67    19.93   \n",
      "2    tomato_2010_2024_master_weekly  245.38  413.59  0.71    18.13   \n",
      "3     wheat_2010_2024_master_weekly   97.85  134.53  0.93     4.19   \n",
      "\n",
      "   Accuracy(%)  \n",
      "0        74.74  \n",
      "1        80.07  \n",
      "2        81.87  \n",
      "3        95.81  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers, callbacks\n",
    "\n",
    "# -----------------------------\n",
    "# Paths / Outputs\n",
    "# -----------------------------\n",
    "input_folder = \"dataset\"  # folder containing weekly CSVs\n",
    "output_models = \"gru_transformer_output_models_weekly\"\n",
    "output_csv = \"gru_transformer_output_csv_weekly\"\n",
    "output_graphs = \"gru_transformer_output_graphs_weekly\"\n",
    "metrics_file = \"gru_transformer_metrics_weekly.csv\"\n",
    "\n",
    "os.makedirs(output_models, exist_ok=True)\n",
    "os.makedirs(output_csv, exist_ok=True)\n",
    "os.makedirs(output_graphs, exist_ok=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Config\n",
    "# -----------------------------\n",
    "look_back = 30       # number of weeks used for training window\n",
    "batch_size = 32\n",
    "epochs = 60\n",
    "val_split = 0.15\n",
    "gru_units = 64\n",
    "ff_dim = 128\n",
    "dropout_rate = 0.2\n",
    "learning_rate = 0.001\n",
    "seed = 42\n",
    "\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# -----------------------------\n",
    "# Helper Functions\n",
    "# -----------------------------\n",
    "def parse_dates_safe(series):\n",
    "    try:\n",
    "        return pd.to_datetime(series, dayfirst=True)\n",
    "    except:\n",
    "        return pd.to_datetime(series, dayfirst=False)\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    mask = y_true != 0\n",
    "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100 if np.any(mask) else np.nan\n",
    "\n",
    "def create_multivariate_dataset(y_scaled, exog_scaled, look_back):\n",
    "    n = len(y_scaled)\n",
    "    k = exog_scaled.shape[1] if exog_scaled is not None else 0\n",
    "    X, Y = [], []\n",
    "    for i in range(n - look_back):\n",
    "        seq_y = y_scaled[i:i+look_back, 0]\n",
    "        if k > 0:\n",
    "            seq_exog = exog_scaled[i:i+look_back, :]\n",
    "            seq = np.concatenate([seq_y.reshape(-1,1), seq_exog], axis=1)\n",
    "        else:\n",
    "            seq = seq_y.reshape(-1,1)\n",
    "        X.append(seq)\n",
    "        Y.append(y_scaled[i+look_back, 0])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "def positional_encoding(seq_len, d_model):\n",
    "    pos = np.arange(seq_len)[:, np.newaxis]\n",
    "    i = np.arange(d_model)[np.newaxis, :]\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    angle_rads = pos * angle_rates\n",
    "    pe = np.zeros_like(angle_rads)\n",
    "    pe[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    pe[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    return tf.cast(pe, tf.float32)\n",
    "\n",
    "# -----------------------------\n",
    "# Build GRU Transformer Model\n",
    "# -----------------------------\n",
    "def build_gru_transformer(input_shape, gru_units=64, ff_dim=128, dropout_rate=0.2, lr=0.001):\n",
    "    seq_len, num_feat = input_shape\n",
    "    inp = layers.Input(shape=(seq_len, num_feat))\n",
    "\n",
    "    # Step 1: GRU Layers\n",
    "    x = layers.GRU(gru_units, return_sequences=True)(inp)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    x = layers.GRU(gru_units//2, return_sequences=True)(x)\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "\n",
    "    # Step 2: Positional Encoding + Feedforward Transformer block\n",
    "    pe = positional_encoding(seq_len, gru_units//2)\n",
    "    x = x + pe\n",
    "\n",
    "    # Transformer-style FFN block\n",
    "    ff = layers.Dense(ff_dim, activation='relu')(x)\n",
    "    ff = layers.Dense(gru_units//2)(ff)\n",
    "    ff = layers.Dropout(dropout_rate)(ff)\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x + ff)\n",
    "\n",
    "    # Step 3: Global Pooling + Dense Output\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dense(ff_dim//2, activation='relu')(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    out = layers.Dense(1)(x)\n",
    "\n",
    "    model = models.Model(inputs=inp, outputs=out)\n",
    "    model.compile(optimizer=optimizers.Adam(learning_rate=lr), loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# -----------------------------\n",
    "# Main loop â€” process all crop CSVs\n",
    "# -----------------------------\n",
    "metrics_list = []\n",
    "\n",
    "for file in sorted(os.listdir(input_folder)):\n",
    "    if not file.endswith(\".csv\"):\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nğŸš€ Processing Crop File: {file}\")\n",
    "    file_path = os.path.join(input_folder, file)\n",
    "\n",
    "    # Load dataset\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['Date'] = parse_dates_safe(df['Date'])\n",
    "    df = df.sort_values('Date')\n",
    "\n",
    "    # Aggregate duplicate dates (mean)\n",
    "    df = df.groupby('Date', as_index=False).mean(numeric_only=True)\n",
    "    df.set_index('Date', inplace=True)\n",
    "\n",
    "    # Align weekly frequency\n",
    "    df = df.asfreq('W')\n",
    "\n",
    "    # Handle missing prices\n",
    "    if 'Average Price' not in df.columns:\n",
    "        print(f\"âš  Skipping {file}: No 'Average Price' column.\")\n",
    "        continue\n",
    "    df['Average Price'] = df['Average Price'].ffill().bfill().fillna(df['Average Price'].mean())\n",
    "\n",
    "    # Feature Engineering\n",
    "    df['Lag_1'] = df['Average Price'].shift(1)\n",
    "    df['Lag_4'] = df['Average Price'].shift(4)\n",
    "    df['MA_4'] = df['Average Price'].rolling(window=4).mean()\n",
    "    df['MA_12'] = df['Average Price'].rolling(window=12).mean()\n",
    "    df[['Lag_1','Lag_4','MA_4','MA_12']] = df[['Lag_1','Lag_4','MA_4','MA_12']].bfill().ffill()\n",
    "\n",
    "    # Prepare scaled data\n",
    "    y = df[['Average Price']].values.astype('float32')\n",
    "    exog_cols = ['Lag_1','Lag_4','MA_4','MA_12']\n",
    "    exog = df[exog_cols].values.astype('float32')\n",
    "\n",
    "    y_scaler = MinMaxScaler()\n",
    "    exog_scaler = MinMaxScaler()\n",
    "    y_scaled = y_scaler.fit_transform(y)\n",
    "    exog_scaled = exog_scaler.fit_transform(exog)\n",
    "\n",
    "    X_all, y_all = create_multivariate_dataset(y_scaled, exog_scaled, look_back)\n",
    "    if len(X_all) == 0:\n",
    "        print(f\"âš  Not enough data for look_back={look_back}. Skipping {file}.\")\n",
    "        continue\n",
    "\n",
    "    # Train/Val Split\n",
    "    val_size = int(len(X_all) * val_split)\n",
    "    train_size = len(X_all) - val_size\n",
    "    X_train, y_train = X_all[:train_size], y_all[:train_size]\n",
    "    X_val, y_val = X_all[train_size:], y_all[train_size:]\n",
    "\n",
    "    # Build and Train model\n",
    "    input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "    model = build_gru_transformer(input_shape, gru_units=gru_units, ff_dim=ff_dim, dropout_rate=dropout_rate)\n",
    "\n",
    "    es = callbacks.EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True)\n",
    "    rl = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, min_lr=1e-6)\n",
    "\n",
    "    history = model.fit(X_train, y_train,\n",
    "                        validation_data=(X_val, y_val),\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        callbacks=[es, rl],\n",
    "                        verbose=1)\n",
    "\n",
    "    # Predictions\n",
    "    preds_scaled = model.predict(X_all, batch_size=batch_size)\n",
    "    preds = y_scaler.inverse_transform(preds_scaled).flatten()\n",
    "\n",
    "    df['Predicted'] = np.nan\n",
    "    start_idx = look_back\n",
    "    df.iloc[start_idx:start_idx+len(preds), df.columns.get_loc('Predicted')] = preds\n",
    "\n",
    "    # Round to 2 decimals\n",
    "    for col in ['Average Price','Predicted','Lag_1','Lag_4','MA_4','MA_12']:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].round(2)\n",
    "\n",
    "    # Compute Metrics\n",
    "    mask = ~np.isnan(df['Predicted'].values)\n",
    "    y_true = df.loc[mask, 'Average Price'].values\n",
    "    y_pred = df.loc[mask, 'Predicted'].values\n",
    "\n",
    "    mae = round(mean_absolute_error(y_true, y_pred), 2)\n",
    "    rmse = round(np.sqrt(mean_squared_error(y_true, y_pred)), 2)\n",
    "    r2 = round(r2_score(y_true, y_pred), 4)\n",
    "    mape = round(mean_absolute_percentage_error(y_true, y_pred), 2)\n",
    "    accuracy = round(100 - mape, 2)\n",
    "\n",
    "    metrics_list.append({\n",
    "        'Crop': file.replace('.csv',''),\n",
    "        'MAE': mae,\n",
    "        'RMSE': rmse,\n",
    "        'R2': r2,\n",
    "        'MAPE(%)': mape,\n",
    "        'Accuracy(%)': accuracy\n",
    "    })\n",
    "\n",
    "    print(f\"âœ… {file} | MAE={mae}, RMSE={rmse}, RÂ²={r2}, MAPE={mape}%, Accuracy={accuracy}%\")\n",
    "\n",
    "    # Save model, CSV, and graph\n",
    "    model.save(os.path.join(output_models, file.replace('.csv','_gru_transformer_weekly.h5')))\n",
    "\n",
    "    out_csv = os.path.join(output_csv, file.replace('.csv','_gru_transformer_weekly_updated.csv'))\n",
    "    df_reset = df.reset_index()\n",
    "    df_reset.to_csv(out_csv, index=False, float_format='%.2f')\n",
    "\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.plot(df_reset['Date'], df_reset['Average Price'], label='Actual', color='blue')\n",
    "    plt.plot(df_reset['Date'], df_reset['Predicted'], label='Predicted (GRU Transformer)', color='red', linestyle='dashed')\n",
    "    plt.plot(df_reset['Date'], df_reset['MA_4'], label='MA_4', color='green', linestyle='--')\n",
    "    plt.plot(df_reset['Date'], df_reset['MA_12'], label='MA_12', color='orange', linestyle='--')\n",
    "    plt.title(f\"GRU Transformer Weekly Forecast - {file}\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Average Price\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_graphs, file.replace('.csv','_gru_transformer_weekly_graph.png')))\n",
    "    plt.close()\n",
    "\n",
    "    del model, X_all, X_train, X_val, y_all, y_train, y_val, preds_scaled, preds\n",
    "    gc.collect()\n",
    "\n",
    "# -----------------------------\n",
    "# Save combined metrics CSV\n",
    "# -----------------------------\n",
    "metrics_df = pd.DataFrame(metrics_list).round(2)\n",
    "metrics_df.to_csv(metrics_file, index=False)\n",
    "print(f\"\\nğŸ“Š All crops processed â€” metrics saved to {metrics_file}\")\n",
    "print(metrics_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
